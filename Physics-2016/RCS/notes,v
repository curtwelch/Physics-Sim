head	1.37;
access;
symbols;
locks; strict;
comment	@# @;


1.37
date	2016.10.13.15.16.49;	author curt;	state Exp;
branches;
next	1.36;

1.36
date	2016.10.12.16.44.52;	author curt;	state Exp;
branches;
next	1.35;

1.35
date	2016.10.11.18.21.41;	author curt;	state Exp;
branches;
next	1.34;

1.34
date	2016.10.10.12.14.41;	author curt;	state Exp;
branches;
next	1.33;

1.33
date	2016.10.09.18.43.08;	author curt;	state Exp;
branches;
next	1.32;

1.32
date	2016.10.08.19.52.43;	author curt;	state Exp;
branches;
next	1.31;

1.31
date	2016.10.07.21.36.24;	author curt;	state Exp;
branches;
next	1.30;

1.30
date	2016.10.07.18.45.44;	author curt;	state Exp;
branches;
next	1.29;

1.29
date	2016.10.06.17.10.13;	author curt;	state Exp;
branches;
next	1.28;

1.28
date	2016.10.05.22.31.11;	author curt;	state Exp;
branches;
next	1.27;

1.27
date	2016.10.04.13.46.31;	author curt;	state Exp;
branches;
next	1.26;

1.26
date	2016.10.03.14.37.28;	author curt;	state Exp;
branches;
next	1.25;

1.25
date	2016.10.02.13.51.12;	author curt;	state Exp;
branches;
next	1.24;

1.24
date	2016.10.01.12.35.00;	author curt;	state Exp;
branches;
next	1.23;

1.23
date	2016.09.30.13.55.15;	author curt;	state Exp;
branches;
next	1.22;

1.22
date	2016.09.29.20.13.58;	author curt;	state Exp;
branches;
next	1.21;

1.21
date	2016.09.28.14.52.33;	author curt;	state Exp;
branches;
next	1.20;

1.20
date	2016.09.27.14.02.57;	author curt;	state Exp;
branches;
next	1.19;

1.19
date	2016.09.26.15.06.05;	author curt;	state Exp;
branches;
next	1.18;

1.18
date	2016.09.25.12.25.05;	author curt;	state Exp;
branches;
next	1.17;

1.17
date	2016.09.24.12.43.50;	author curt;	state Exp;
branches;
next	1.16;

1.16
date	2016.09.23.17.16.08;	author curt;	state Exp;
branches;
next	1.15;

1.15
date	2016.09.22.14.26.30;	author curt;	state Exp;
branches;
next	1.14;

1.14
date	2016.09.21.20.51.10;	author curt;	state Exp;
branches;
next	1.13;

1.13
date	2016.09.20.14.47.25;	author curt;	state Exp;
branches;
next	1.12;

1.12
date	2016.09.19.17.00.58;	author curt;	state Exp;
branches;
next	1.11;

1.11
date	2016.09.19.16.46.46;	author curt;	state Exp;
branches;
next	1.10;

1.10
date	2016.09.05.17.43.18;	author curt;	state Exp;
branches;
next	1.9;

1.9
date	2016.09.02.14.59.54;	author curt;	state Exp;
branches;
next	1.8;

1.8
date	2016.08.30.17.07.59;	author curt;	state Exp;
branches;
next	1.7;

1.7
date	2016.08.28.14.10.43;	author curt;	state Exp;
branches;
next	1.6;

1.6
date	2016.08.27.13.12.05;	author curt;	state Exp;
branches;
next	1.5;

1.5
date	2016.08.25.15.59.54;	author curt;	state Exp;
branches;
next	1.4;

1.4
date	2016.08.24.14.43.34;	author curt;	state Exp;
branches;
next	1.3;

1.3
date	2016.08.18.21.40.23;	author curt;	state Exp;
branches;
next	1.2;

1.2
date	2016.08.18.21.39.34;	author curt;	state Exp;
branches;
next	1.1;

1.1
date	2016.08.17.13.18.01;	author curt;	state Exp;
branches;
next	;


desc
@notes -- taken from AI for Physics
@


1.37
log
@checkpoint by ciall
@
text
@notes -- Physics -- lab notes from work on Physics simulation

Lab notes and brainstorming and history of work on Physics.
Copied idea and file from AI work.
Started this file 2016-08-17 9:12 AM
Curt Welch -- curt@@kcwc.com

================================================================================

Atomic Simulation project Overview

2016-08-18

I've never liked the answers produced by quantum physics.  The idea that
a photon is a particle and the whole wave particle duality seems broken
to me (as it has to many people over the ages).

The notion that photons "exist" is created by the fact that low level
energy is only seen being trasfered from source to target in small
quantized units we know of as the photon.  But an alternate answer
could be that the atom is only able to accept and relase energy in
these quantilized units.  This I believe might be explained by some
complex orbital mechanics of the atomic particls that makeup the atom.
My idea is that the chaotic behavior of orbital mechanics could create
natural strong attractors that force the orbital dynamics of the atom
in to fixed stable energy configurations and that when energy flows in
to try and change the state -- it will shed the energy back out to other
atoms, or absorb it by flipping to a different state.


I certaonly don't know much about the complexites of particle physics,
but the little I do know makes me think the answer to why atoms are
energy stable at different levels could be found in the complex orbital
dynamics.

So much so, that I think these orbital dynamics might be fully explained
buy simple newtonian physics.

So my starting experment is to model electons and protrons with newtonian
physics and with the known electrostaic field and see what sort of
complex dynamics show up.  And if that doesn't duplicate known phsical
effects of atoms -- look for alternative models that do create the needed
dynamics so as to explain the complexities of QM and particel physics
all as emerget properties of a simple atom, vs fundamental properties.

It seems to me that these two particles and the one electrostaic field
might be enough to explain all of physics -- making this the new simpliest
stander model of physics.

The neutron might be explainable as nothing more than a electon proton pair
in a tight very high speed oribit for example.  In Beta decay, a neutron
turns into an electron and a proton, plus extra energy.  To get an orbiting
pair to split apart, it would have to absorb energy from the rest of
the environment to force the electron into escape velocity from the proton.
That extran energy absorbed from the enviornment causes the kentic enrgy
of the electron and protron increase and as they flys away it induces heat
kentic energy into the sourounding material which is measured as the
"extra energy" in the experiment.

In this simple model, there is potential energy created by the force
fields of the em field, and kentic energy in the motion of the two
particles.  As the system evolves over time, it's trading off potential
and kentic enrergy.  But in my simple model, there is no speed of light
delay in the potential energy fields.  So every particle in the univese
is in constant instentainous connection with ever other.  All the motion
of the particles in the univese creates a constant background noise that
is seen in the changing force vector each particle experiences.

But for something like beta decay, the background noise of the entire
univese could peak at some point creating enough energy to tear the
particles apart.  This might be energy that is absorned from trillions
of articles spread over the entire univese -- but it shows up as a local
teraing of the pair apart -- and that local kentic energy, then causes
local kentic spread of energy seen as the "extra energy" of Beta decay.

So energy is conserved -- this works automatically in the normal
newtownian f=ma motion equations and the 1/r^2 force field, but it changes
from potential to kentic, where potential enegy flow in the universe may
have no speed of light limits but kenetic energy flow is limited by the
actual motion of particles.  (the particles carries it's kentic energy
with it as it moves).

If there is kentic energy in one particle, the speed at which it can be
tranfered into kentic energy of another particle, is limited by the time
delay effect of f=ma.  Which can mean, that even though the potential
engy of the em field is instenious, the flow of kentic energy could be
speed limited as one article passes it to another and to another.

So I want to work out the math and experment at the movement of kentic
enough in a system of these particles as well.  It could turn out that
the speed of lighit is the limit of kentic energy even though potential
energy effects have no speed limites.  Wich means this simple newtonian
phislcs model might have the power to epxlain the speed of light limits
we see -- without odd ideas like "space bending".  So is the speed of
light also just an emergent property of a far simpler atomic model?
I want to answre that as well.

Likewise, strong and weak fields I don't understand.  I don't know what
experimental data gave rise to the idea these fields exist. But they
seem to again, be effects that only happen inside the atom -- so a faulty
understanding of what's happening inside the atom, might give rise
to the notion that these other fields are needed to explain how the atom
works, when instead, simple newtonian physics will explain it all -- but
with highly complex orbital dynmics when these particles interact.

Likewise, I might have an answer to gravity as well.  If neutrons are
really e p pairs in a tight orbit, I wonder if a field of millions of
these will tend to synchronise their orbital speed?  If the em field
is instantanious why wouldn't they tend to synchronise thoughout the
entire univese?  And if two of these syncchronise their oribits so they
are orbiting in lock step it seems to me they will naturally tend to
attract each other.  When in the "ep   ep"  state, the 4 particles will
be pulled together.  Ever particle is closest to it's oposit so the the
sum of all forces pull them all together.  when in this config, 90 deg
later in the orbit:

e   e
p   p

The e and p are pulled otgether, but the other forces are near neutral
and cancel each other out.  If we sum all the forces over the full orbit
I think the resulting averge will create a  weak attractive  force.

Just as if we had this:

ep       e

The distant e will be attractred to the ep pair, even though the close
together ep pair mostly cancles each other out it has a weak attracted
force in this configuration as the differnce between the e and p fields
since p is close to the distant e, it dominates.

The very weak force of gracity, cold maybe be exlained as this weak
attraction of syncronised neutrons.

If this is what gravity really is, then it doesn't actually work on all
matter based on mass.  it only works on neutons based on weight and the
extra free electons and protons, then just get pulled along.

Or maybe, the entire atomic orbital dynamics end up being syncchronised
across the entire unviese, which makes the entire atom all weakly attract
each other in this same sort of statistical effect to explain gravity
as a weak attraction created by the more fundamental electrostaic field.

The spooky action at a distance is explained by the fact that there is
no speed of light limits to the electrostic field in this model. So when
we create "entangled" photon pairs, the photons don't actually exist
which is why we can't explain QM in terms of a state encoded into the
photons the state is encoded into the environmental field of atoms and
their highly complex group orbital dynamics that the test is done in.

So the entangled pair is really large clounds of atoms pushed slightly
out of balance from their perfered stable states -- so a very tiny bit
of extra energy is stored in each atom. The wave colapse event, is
where all this extra energy in the field is transfered to one atom that
indicates it has "received a photon" instentiously because the em field
is an instentious field.  But the entangled paricle is really two
large cloulds of out of balance kentic fields moving away from each other
at the speed of light and when on colapses it instantly chagnes the
balance of the other cloud -- causing it to coplase in a correlted
way later when it is "measured".

Likewise, if photons are an illusion crated by the group dynamics of
of atoms, and QM and speed of light, are emergent behaviors of this
simple system at the large scale of interacting atoms, then I susupect
all the word done on particle physics to "break down" particles to
simpler sub-atomic paritlces, is likely in question as well.  I don't
undestand particle phsics enough to have off the cuff answers to what
that are looking at, but some of the descripions of how the field
advanced -- that 1000's of particles seem to show up in tests, which
was then simplified down to the current model, makes me think what
they are uncovering is not simpler particles, but rather, more complex
emerget behaviros of large fields of atoms. They aren't getting simpler
they are getting more complex is my gut feeling.  This is even more
consistent whe they tal kabout the newer particles having higher
and higher mass -- implying they are in fact studying a large collection
of small paricls, andnot a new massive smaller praticle.

So, if this approach holds any water, it means we have to understand
the highly complex orbital dynamics of how groups of two simple particles
tend to interact.  And that is beyond what we can easilly describe
with math.  We can describe the differential equations that create it
easilly -- simple f=ma with constant mass for the e and p particles,
and the one simple electostatic force field.  But the complex orbtial
mechans (N body orbital mechans) is way beond what we can describe.

In fact, just getting started in this I've been looking into the math of
two body orbital mechanics and I've learned that even two body math is far
more complex than I thought. Though two body orbits are a simple ellips
(when in orbit), if we know the position and velocity of the two objects,
there is no direct solution to calcate the elipictal orbit formula. It
must be done with trial and error search like newtons methond.  If I
understand it correctly.  So fitting the answer to the problem, is like
fidning zeros of a 3 order poloynomal -- you have to use newton's method
to find them by serach - they can't be calucated directly.  Which is
no doubt why no one has been able to figure this out yet.  They didn't
have the math to explain the oribital mechanis of atoms -- so we hit
a wall using the math we could, to talk about it as a probablity cloud
of electons.

Using small time step approximations is the normal compuational
way to simulate this. But each time step make false assumptions, and
introduces error -- whcih them accumults and builds over time -- making
the simluation pointless -- just like trying to predict weithat -- you
need massive compution to come close, and then the simuatlion only works
for a few days at best before the error accumulate.

So, can we even simulate orbital mechans well enough to accuratley
demonstate emergent properties that duplicate physics experment data
and prove the underling model is correct? Maybe we can't?  Maybe the
best we can do is show that it might be correct?

But none the less -- this project is an attempt to explore this
possiblit -- that a ver simple two particle and one field standard
model might have the ability to explain all of physics.  And if a model
that simple doesn't explain, maybe somthing just a little but more
complex will do the same?

Curt Welch
curt@@kcwc.com

================================================================================

[This section written to notes in AI then duplicated here as well]

2016-08-17 8:08 AM Wednesday

Been distracted by life for 5 months and have done no work on this.
Just lots of procrastination.  Did spring fling and Blacksmithdays May.
Julie came home from school and we went to the shore early in the summer.
Got hooked watching That 70's show on Netflix at the shore with Julie,
came how and watched the entire show and then got hooked on Orange is the
new Black and watched all the current seaons of that.  4 Seasons maybe?
Then I started on rewatching old shows -- Buffy, Angel, Person of
Interest, Blacklist (the first seaons and a half I have not seen).  Oh,
back at the beggining of the year I watched all the MythBusters shows --
recorded the marathon they ran between Christmas and NewYears and took
months to finish.

Haven't worked at Mount Versnon since December -- told them I was
working on AI.  Newsreader is shut down -- so my life is amazingly empty.
Running out of money and that has be scared and confused about what to do.
Desparately want to have my life back and be out from under the control
of the wife, but I'm not really sure how to do that, since I don't want
to deal with moving all my shit out of the house and don't know where
I'll end up becuase I need money which means a job which means deciding
just want I want to do -- move to London, SF?  Try a nationwide job hunt?

Endless hours wasted on facebook -- lots of debates about Basic Income,
and the like.  Got kicked out of one of my favorate groups which was on
technological unemployment beuase I was aggressively arguing against the
idea of a resouce based economy.  That left me feeling frustated angry,
said, and empty as well. But I got over that.  Then there's this endless
crap with Trump and the election comming up.  OMG.  That adds even more
stress to my life to have to see that.

Came to a new understannding at some point as well about arguing on
Facebook.  Many of the people I argue with about these high level ideas
really aren't able to understand what I'm talking about.   It's this
detail brain vs abstract big thinking brain thing that allows some of us
to see the big picture while others are stuck looking only at the details.
It's why people don't get that RL is the solution to AI. It's why all
these conservatives don't understand climate change.  It's why all my
big theoires that I find so intersting, like about conscioness, just
seems to go over people's heads.  I always felt that all I had to was
explain it well enough, and they would understand it. But I realie now,
some of them just can't understand it no matter how well you explain it.
They arelaly are trail and error learns and have such weak abstraction
skills they can't grasp the high level concepts I'm trying' to share.
So I need attention to stroke my ego, but I'm wasting my time trying to
get it from people that can't understand the ideas.

The deabtes on facebook often leave me feeling empty and worthless
and mad at the world.  A few times someone will really enjoy a post of
mine and that's what keeps me going -- but so much is just so wasted.
Seems to be a net loss -- but I'm so desparate for attention and have
nothing else in my life to fill the hole.

The past few weeks I've been debating with Tague on facebook about
Basic Income -- trying to share the "capital income is evil" idea but
he can't undestand it.  He only sees captial income as the driving
force of captialism and thinks GDP would suffer if we tried to reduce
or eliminate it.  The way I explain it is certaonly over the top, which
mislead him, but even when I backtrack and try to make clear the issue
I'm talking about, I don't get any traction with him.  Again, another
person too focused on details to see the bigger picture.

I've been really feeling paranoid and fearful and anxious which is part
of why I've not gotten back to AI.  I think a big part of this that I've
not realized was the damage Vecana did to me -- really left me scarred
with a capitaism sort of PTSD emoitnal damage that prevents me from
even thinking about sending out resemues and trying to get a job again.
I think that is more of what I'm afraid of than I'm able to admit.  If I
run out of money I have to face that fear again and I'll do anything to
escape that -- including making up stories about what else my problem
really is.

Which is why no doubt the idea of a Basic Income is so attractive to me
and why I push it so much -- I can unrstand the damage now of what it's
like to live beliving you can't get good work.

Oh, other news -- got the Liver Biosphy done -- they confirmed it was
NASH -- inflamation and scaring in the Liver from too much fat buildup.
My weight is around 230 now -- nothing but slowly getting fatter all since
last winter -- which I stopped riding my bike.  Was waiting for it to warm
up to ride, but it went from the 60, to the 90's almost overnight it felt,
and we have had endless fucking days if 100 degree weither and when not
that hot, in the 90's most the time.  Haven't been able to talk myself
into biking.  Oh, got the the LIPO packs, wired them up and tried them.
Seem to work, but the higher voltage cauess the stall on start which is
a bitch -- not having the power to get started from a stop.  Bought the
25A controller but haven't installed it. That might change the stall at
stop behavior.  Just haven't been motivated to get out -- but I really
need expercise and I'm not getting it.

Pogemon Go came out and I swtiched to it and stopped Ingress.  Up to
level 27 (of 40) so far.  Another great time waster.  My days start with
me waking up at 5 am type frame in a panic about life.  Forcing myself
to sleep and not getting up until 10 or later.  facebook in the morning.
Out to get coffee and play pokomon around none.  Back home for facebook.
Maybe pizza but I've not been doing that a lot.

But the progress on AI has been great when I was working on it before.
I've got a percpecption algorithm that should be able to do great things.
And an RL that seems cool -- but I need to figure out how to link them
together beter.  Or maybe do RL in a slightly different way to make
it compatible with the percpetion learning? Andit all works with this
cool time based activity trace system that seems like it might sovle the
temporal problem question and implement prcpeiton correctly!  But I need
to do lots of testing on that.

Oh, and Physics.  Which is why I came to this file today to start with.

Some facebook debates got me to dig into physics and QM deeper than
I have in the past that got me a better understanding of the actualy
tests that have been done for things like the double slit.  This got me
back to my old ideas that the physics has failed to understand the inner
workings of the atom, and that all the complexity they document in the
QM and other formulas really are emergent propeties of a far simpler
system they have missed.  That wave particle duality is an illuion
created by the lack of measurement tools -- we have reached the bottom
of the ladder in terms of measurement -- and a they try to understand
the measurements they are getting they are modeling the measument system
interaction with atoms instead of modling atoms.  And I think the atom
is far simpler than what they assume.

I think that simplicity can be found by modling the interaction of atomic
particles.  Pohtons are probabkly not real. The discret behavior of energy
transfer from source to receiver is due to us using atoms to measure the
recpetion of energy -- and the fact that it's quantized and belive to be
likel due to the a proprty of the atom as an energy receiver, not due
to a property of light itself.  I tihnk light is nothing more than the
em field and is not quntized, and is not limited by the speed of light.

If we build an accurate simualtion of atomic particles I think we will
find they form strong attractors that lock them into different but
complex orbital paterns that cause the quantization effect.  Energy is
only transfered from one atom to another when they get "pushed" enough
to escape the attractor.

And I think that the EM fields can be instentanious and not limited
by the speed of light, but that the flow of kentic energy that is
time delayed by the differential equation effect of force and motion,
is what has the speed of light effect -- alone with the quantum effect
of measurment that means the receiving atom can't chagne state untill
a threshold has been reached -- so as the speed at which the threashold
can be broken could be the speed of light.

So I need to experiment with models of the atom and see if I can find
a simple set of rules to explain it all.

And one option, is that the only real particles are electons and
protrons and the only real field is the EM field.  Neutrons might must
be a thighly coupled proton and electon in a tight high speed orbit.
Photons are just a measurement illusion.  Neutrons might tend to all self
syncronize so that they all end up in the same frquency orbit creating
a resonating vibration thoughout the entire univese of all the neutrons
being syncronized.

And, I have to wonder if synchronized neutrons might attract each other.
And be the answer to what graivty is -- not a serpate field, but just
the EM attrction of nutonrs.  Is it possible that this is gravity and
that it only actually attracts nturons?

The key here is to simulate the interction and see if atoms form the
same way they do in real life.

I've shared this idea with Tague and Chappell -- Tague acts like I'm a
idiot and he's just being nice to me.  Chappell was currious.

But I wrote a python program to do the basic math many months back. But
it was hard to visualize what was happening.

And the first thing I realiized was that I thought simlation would be
easy (just newtonian F-ma stuff), but it was not simple at all.  It blows
up with simulation errors very fast once an electon and protron get too
close together.  The closer together they are, the smaller the time step
you have to use to keep excess error from accumulating.  But if all you
have is one electon and one prograon, and the start with no velocity,
they fall straight together and colide and then pass though each other (my
model allows that) but as they pass, the force adn speed go asmtopic and
the simutiion looses all accracy and what should be a simple ossolator,
shoots off an electon into to space.

So, been looking at the math, but got discurated when I realied how
fucking hard obital mechanics acutaly is!  There isn't even a direct
solution to the simple two body problem -- it requires finding roots
of equations using newton's method!  And that's only for the two body
problem.  For the N body problem it's a diaster.

So the hope is that I don't have to get it correct, but only need to
get it close tnough to show the emergent behavior.

But yesterday I spent a few hours and got graphics working so I can do
I visual display of the particles and now I can see what is happening
much better!  But what happens is that the simaution acts sort of normal
for a bit, then an electon gets too close to a proton, the accuracy
goes to hell and the electon ends up being shot into space.  So I need
to find a way to do the math to stop that.

I have a Physics directory, but no notes file for it, so I came here to
the AI work to copy the format of the file I was using here.  But them
started to do all this writing about life.  So I'll copy this back to
the Physics directory, and start a file for the Phyusics work.

Julie is almost ready to go back to school. Wnats to go pick up a TV
today.  Wnats my help. She and Pig went to Cleavelend to see baseball
and rock and roll hall of fame.  I stayed home. But tyey want to go to
Baltimore today for ball -- I'm going but will play Pokemon in the inner
harbor instead.  Or something like that.

Wanted to a bit of work on Physics so I got up before 8AM to shower and
do Physics work.  First time I've been moti vated to get up in a long
time now. Feels sort of good.

[9:09 AM]

----

Now writting to Physics notes file.

Other life issue -- due to liver, I've decided to only get Starbucks on
M, W, F. S.  Cut back to 4 days a week and see if that helps weight and
cut back on fat?  But today is Wed so I get coffee today!

On physcis, wanted to check if momentum is conserved in my code.  Assued
it wasn't, but on second thought, I should be prefectly conserived if
I update all the forces in lock step.  But maybe I wasn't.

Now that graphics is working, I can attack this problem of finding out
how to code a simulation without losing all accurcy. The greaphcs code
is a hacked mess, and will need cleaning up, but first more hacking and
messing just to learn how to code it.

Had the idea to change mass of proton to be 2x electon to make eveything
move faster to get better read on whether simulation code is working.

[9:17 AM]

----

[9:24 AM]

Ok, checked code.  Changed mass of p to be 2x e.  Made ep pair orbit
each other.  Seems to work.

Checked code. It does update all the forces first, then the v and poistion
so montum should be prefectly perseved in the simaultion.  Need to write
code to calculate it and verify just to be sure. But first, starbucks run.

----

[11:33 AM]

Went to starbucks, got drink for jules and myself.  Ordered mine
incorrectly.  Forgot to add no-water.  Been playing on facebook like
normal.  Wrote suggestion to Starbucks about being able to pick from
past order menu when doing add item.  Hope they add this some day.
Julie is almost ready to go to Best Buy to hunt for TV.  I'll try to
add momentum check...

----

[12:06 PM]

Added momentum test -- current code does as expected matain perfect
conservation of momentum.  Added code to normalize to zero at start to
keep from drifting off screen.

----

[2:10 PM]

Back from Best Buy run with Julie to pick up new TV for school. Wrote
another facebook post about consciousness.  Back to code.


Need to do more testing with an electon that blows up -- what to verity
that momuntum is truely maintained.

----

Ok, confirmed that momentum stays near zero.  Rounding errors show up and
accumulate, but the number remains in the e-40 range even when electons
run into protons and fly away.

So as we are coding it, monumum is maintained, but energy is not!

So it seems we might need some sort of search with each update, that
adjusts for errors in energy, and and then adjust for errors in monumtum,
until a solution is converged on?

Seems hard and slow.  But I need to explore that.

But first, going to add some collsion detection with the wall to force
electons that have gone flying off to stop and stay on the screen.
Just for the fun of it and to allow the system to keep running.

----

[2:54 PM]

Got collision detection working.  I make it bounce and lose half it's
power.  Fun to watch the electons fly around now and try to get back
in orbit.  But every time it gets too close it will sling shot out to
the wall.

So I need to stop the sligshot error.

Heading to baltimore now

================================================================================

2016-08-18 11:37 AM Thursday

Yesterday

1) set up this Notes file from the AI file and wrote a lot of history

2) Checked conservation of momuntum and realised the step updates always
gurantted conservation already.  So that's not a new constraint to add.
But if we fudge velocity or position to try and maintain conservation
of energy it will break conservation of monuntum --- so these are two
things we can optimize the system against to try and create more accuracy.

3) Added code to make starting monumum of the entire system zero so the
particles do not drift off the screen no matter what starting values
they have for speed.

4) Added collision detection and bounce with 50% energy loss at edge of
screen to allow the system to keep running after an electon is thrown
out of the park due to the simulation errors when e and p become too
close together.

Today

1) Want to work on the simulation error that causes electons to fly off
the screen.  I think energy conservation by adjusting velociy to match
force might do it.  Don't know how much that might mess up other issues.

The other option is to take smaller and smaller time steps as the
distance is smaller and the force and speed becomes larger.  Maybe just
set dt each time after calclating forces based ont he force before doing
the savement update?  Or based on speed?  That would be easy to code.
And would be self adjusting in terms of allowing the simulation to take
larger steps and run faster when the ep were not too close together.

Maybe just distance vs speed so that he distance doesn't change by more
than 1% in any step?  Need speed and min distance for each particle to
do that?

2) Or, Use energy conservation to adjust velocity after each step.
I'm not even 100% that's valid in a system of lots of particles.  I have
to experment with that.  Could then re-normlaize all velocities to set
momentum back to zero?  Need to play with this and get abetter idea of
how conservation of energy works.

----

Did Baltimore last night. Got over 10K steps by iPhone count for the
first in a year.  Heavy rain after dark waiting for Baseball game to end
-- was rain delaied but they called it in the end.  Got two new pokemon
beucase of the work -- the upgraded flapping on side fish that upgraded
to a dragon!  400 candy needed.  But each fish caught comes with 3 candy
and returning it in gives 1 so only 100 fish needed to be caught total
to get to the 400 range.

And I caught the snake/dragon like guy in it's upgraded form, plus
got enough of the non evolved ones to evole a second.  Almost finished
level 27.

Ate too much waiting in the rain so I gained a pound even after all
that exercise.  But it was good to get some exercise again -- legs
hurt from the walking -- it was very hot and humid as well so I was
sweatting a lot.  Took my bike backpack to carry drink and rain gear.
But that did make me a bit hoter.

Thursday -- so first no starbucks day for me.  Just have to do normal
coffee. But now I don't have the excuse to go out and pokemon on the
way to starbucks...

Leaving to take Julie back to school in Greensboro tomorrow.  Pig and
Julie will round down in Julie's car, and I will follow with more stuff
in mine later in the day.  Pig and I will then ride back.  Stay down
two days.

Need to go get haircut today.  I guess pokemon when I do that?

----

Just wrote the overview at the top of the file.

----

[5:29 PM]

Spent lots of time writting facebook posts about consciousness.  Then
went off and got a hair cut and vitamen E per the liver doctor.  800 iu
per day -- so 2x 400iu pills a day.

Gave Milton $20 for first time.  Normally the cut is $12 and I give a
few dollars tops.  I think haircuts are $13 now, but I decided I really
need to share more money with him so I've upgraded to giving him $20
a cut.  Been using Miltion for many years now becuse he was cheap and
simple.  And becuase he wasn't an asian that was hard to understand. :)

Didn't play much pokemon.  Just grapped a few on the way there and back.

Now, fixing lasagne for dinner.  But while it's in the microwave, a few
thoughts...

On the energy conservation.  If the sun stands still, and the orbiting
object moves though the static field (or static frame of refence with
the sun at the center), then I think we have a normal trade off between
kentic and potential energy as I wrote in the formula in the code
that I got from somewhere.

Looking at code, I find the formula I'm using for kenetic energy is
just the same as the force forumula for two objects distance x apart.

It's no-directed, but it's signed, with postive for repelling forces
and negative for attractive forces.

Darn, I don't understand this.

Ok, right, force is ke/d^2, but potential energy is just ke/d.  Which is
sort of like mgh mass gravity height.

It wasn't the same!  I didn't look careful enough.

So, from the frame of referecne of one charge being the center or 0
point when the the other charrge moves it velocity will change.

But when both move (we are using a frame of reference not located at
ether charge), the total velocity change of both combined matches the
change in potential energy I think.

So I think this means, that when we look at the change of potential
energy of a give particle relative to all others, and the change in it's
kenetic energy, we won't find they match?  Beause the other particles
moved as well?

Yeah, I don't really understand the math here.  Need to study it until
I do/.  But first, just add some debug to the code and see if for a
simple two particle test, how the ke and pe of each particle changes to
understand what's happening.  Yeah, the two particles will obrit around
each other, and will both speed up as they get closer, and slow down
as they get further away.  So the ke has to increase in both but the
pe measured once between the two, offset both those changes I think.
Need to confirm this.  But lasagne is wating.  And TV.

[6:04 PM]

================================================================================

2016-08-19 9:20 AM Friday

Yesterday

1) Lots of notes and thinking about the simulation error problem. No
code chagnes.

Today

Have to drive to Greensboro soon.  But want to get a little work done
first.

----

Added normalizing of total PE so it starts at 1.0 so I can see how it
changes easier.

Tested two electons and two protons alone to check sign issues and the
total E worked for both -- stable.  It jumps suddenly when it bounces
on a wall.

Oh, if course, total e changes on a bounce becuase I cut the speed
in half!  Otherwise it would not change!

Checked code by turnning off loss of energy and then a bounce doesn't
mess up total energy.

Testing one e p pair with no starting velocity. Should fall together
and pass thorugh and oscillate.  But every time it passes though the
simulation messes up.  So lets try to fix this by changing velocity to
make total E constant.

But do it one step at a time.  That is, calculate total e for a single
particle move it and update velicty using total e for that particle
(wihtout moving others).

----

Much better behavied with new energy adjustment but still has clear
problems.  I coded it to record kenetic energ first, update velicty
using force, move particle then update veloicty using new poetential
energy to match what the erngy was at the start -- keeping direction of
velocty as calculated in the update.

But errors do still creep in.  Energy is 2.1 now after multiple wall
bounces that took energy out (instead of 1.0).

[10:16 AM] must stop playing and drive to Greensboro now.

On two particles with no velocity test, the electons fall though the
protron and seems to act correctl. But when it gets to the far side it
gets stuck.  Something with a zero velocoy not changing sign or something.
Need to debug when I get back.

It's odd that the system seems to work correctly in orbits, but in this
simple back and forth x only test it's going all crazy!

================================================================================

2016-08-22 1:03 PM Monday

Yesterday

1) Last worked Friday.  Tried first hack at improving code by doing single
particle updates with perservation of energy.  That seemed to help in
some cases like tight orbits but didn't make the single dimension two
particle problem oscillate correctly.

Today

Spent the weekend moving Julie back into her new place for school. She's
in a 4 bedroom apartment with 3 friends this year.  Drove down 29 for
the first time all the way there.

Feeling really tired lately.  Not sure why.  Not sleeping well.
Staying in bed too long as well.  Maybe just depression/fear crap.
Maybe I'm fighting a mild flu like thing or something.  Oh well.

Read a facebook post about Stephen Hawking declaring that black holes
don't exist -- that they don't have a boundry where nothing gets out.
That this old notion is incompatible with QM.  And that since we haven't
merged QM and gravity, we don't have an a full understanding of what
is going on.  So that's cool.  One of the problems I had with this e p
theory was that it was hard to explain black holes since the EM field
would not be limited by gravity.

Idea.  If the attraction we call gravity turns out to be an EM effect of
e and p in orbit syncchronizing with each other -- what happens when they
get highly dense in a black hole?  Coould it be that they pack tighter
together and increase in frequency and no long syncronize?  That would
seem to indicate the gravity would stop even if all the matter near the
core is syncchronizsed and attracting to each other?

No clue really.  I just need a good simlation working to get a better
understanding of the group behavior of these particles and see if we can
explain all of physics as complex group behavior of electons and protrons.

So, on to the simulation problem.

This is an odd total system problem I don't know how to address.

I can start with a given set of position and veloctiy vectors.  Then do a
dt update of the position assuming constant velocity.  Then check total
kenetic and potentital energy of the new configuartion and calcuate the
error. But how do I then distribute the error over all the particles?
Fixing error by adjusting velocity seems the easiest.  But does constant
momentum break whemn we do that?  I assume so.  I need to verify that.
And if the ending velocity is different from the beginning velocity, then
assuming constant velocityy over the dt peroid is not a very good model.
So should we them go back and do the dt postiion update again. assuming
velocity changes from the starting to the ending vector in a linear
fashion (chaning the step model to one that connects stratight lines?)?
But then the ending potential energy will be different, and we would
need to repeat.  If we keep repeting, will the system get closer and
closer to a more valid configuation?

I have no clue.  So I guess I just have to try coding something and
seeing how it works.  Or, attack from the theory direction of tryingt
to understand a theory.

My concern, is that most the error that creeps in, will likely come from
two particles that have gotten very close together, so most the error
correction should be applied to those two particles, instead of spreading
it over all particles.  I wonder if there's a straight forward way to
identify how much error comes from each particle and then distribute
the total error adjust based on that?  But if this was easy we could
just adjust each particle by it's error and be done with it.

Ah, if we look at the system from a new frame of reference, relative
to a single particle, might this help us determine something?  So the
velocity of each other particle is the sum of it's velocity and the
velocity of the reference particle.  And then we can check to see how
much the potential energy changed vs the velocity.  Ah, that makes it
simple if there are only two particles, but not with N beucase all the
other still interact with each other as well.  The entire system needs
to converge back to a new consistent solution after each dt step.

Ok, so lets just learn more about this by doing.  The trick I tried Friday
of doing it one particle at a time works for some cases, but the hard
case of two particles in a straight line doesn't work at all. Something
odd happaned when they were both movinbg in the same direction at the
same time.  Maybe that's related to the falure to maintain constant
momentum (they never should have been moving int the same direction at
the same time)!

But, lets just do this.  Calcuate total energy.  Ajust particles.
Recalucate -- find total e error.  Then fix total error by adjusting
all the velocities.  if the v adjust can be done in a monumum mainting

way that might be a good first step.

----

[3:16 PM]

Changed code to calculate total energy change after move, and then adjust
all the kenetic energy to force energy back to zero.  Found that the
sentKeneticEnergy() function I was using in the last test was broken! It
was using v^2 instead of v to ajust velocities with and that was adding
an error.  For a 2 particle test, this new code seems to keep energy
constant.  For 4 particle test, it was blowing up and crasing when
trying to do a sqrt of negative KE.  Don't know why yet.

Could go back and try single particle updates again to see what happens
when I use a working setKeneticEnergy function.  Maybe.

But lets dig deeper into what's happening here first.  I fear the problem
is that the PE and KE change is so great that it can't be fixed by
only changing KE.  So it's trying to make KE negative to adjust for
too great a PE -- which can't work.

----

When dt is too large the system still does a good job of mainting const
energy. But with a single pair, the long eleptical orbit will turn into
a random spirograph like effect due to accumuting error -- then it tends
to drift to a circular orbit -- more circular less error and the more
it stays in a circular orbit I think maybe.  Momentum does accumulate
error in this test but not a lot e-37 from e-40 starting error.

----

Yes, went back to 4 particles and that is what the problem was. The total
energy change was greater than what could be fixed by velocity alone. So
it was trying to set negative kenetic energy.  But why does that happen?
It means the position is an impossible one I guess.  Electon has moved
too far away from other other particles creating too great a potential
nergy -- so setting v to zero still leaves the system in a state that
is impossible to reach.  But, how to cope?  Reduce DT in half when this
happens and restart the last move over again?

let me just limit the adjust so that when when it needs to set ke to
less than zero, just change it to set it to small postive number and
whatch what happens?

----

[6:21 PM]

Been playing with the code for hours.  found and identified some bugs.
Added the excessEnergy system as a hack to cope with the constant energy
adjustment when KE is not large enough to adjust all the excess PE out.
So it tries to suck KE out over time.  But when an electon gets thrown
way off screen, the PE is just crazy and by constantly sucking the
KE of the electon that's standing still, it hardly moves and as such,
never takes the PE out...

The concept of total Enegy is sort of bogus with the fact that potential
enough is always negative between a E and a P.

Between an E and E or P and P the potential energy is like a wound up
spring that will convert it's stored energy into kenetic energy.  So the
postive number represents how much ke will be created by pushing the
particles apart to infinity.  In attraction mode, it reprsents how much
negative has been taking out from going to infinity to it's current orbit.

The PE function puts the energy at 0 at infinity.  But the energy becomes
infinity when the e and p cross.  Which is just a real computation
problem.

Wait, is the potential energy from a fixed distance to zero always
infinite for attraction?  I guess it is.  And for the reverse.  So the
sum from a fixed distance to infinity is always a constant, and the same
for both, but just a different sign. But when they get too close together,
it all gets to be a diaster.

I wonder if using log(e) might help somehow?

The the compuatation problem remains a problem when particles get too
close toether -- or worse, when they cross in the perfect straight
line exmaple.

Hum,  What if I limit the force to some max value that sets an inner
limit on force field? Ugh, I guess that breaks conservation of enargy
for one. Oh, but maybe it diesn't.  Maybe all that's needed is that the
force field be continous to maintain conservation???

It would mean max potential energy is not infinite at the center but some
fixed amount as well.  And I could then make the potentla energy zero for
two overlaping e and p particles making totale in the system inteligent.
The question of course is how much would it mess up the simulation from
a system that didn't have that limit?  If most particles were outside
of the distrotion space most the time, maybe not so much?

But it would limit how close and e and p could become in a tight orbit.

Maybe that's an approach.  However, the main problem is that the
simluation goes crazy when they get too close and I should be able to
fix this.  It's due to the dt time not plotting the new position and
v correctly.  The new position is totally out of line with the new v.
And I've tried just addjusting the v to fix, and that helps, but doesn't
address the problem of when the position is way out of line.

So I guess I need to adjust position as well as V when doing the error
adjustment. Or maybe adjust DT to fix position, and then just use V
adjstment to fix energy?  So maybe, look at how much we have to adjust
velocity (ke) to fix energy and if that's over some % amount, reduce dt
and try again? And if it's under a threshold, raise it and try again to
speed simulation back up?

That sounds like an approach I need to try.  Auto adjusting DT seems
like a viable approach -- as long as one e p pair doesn't get stuck
in such a tight loop that it needs a very very small dt which slows
the entire simulation down to a crawl.  But with a larger simauiton,
the total system noise might well prevent that form being possible.

Ok, that's the next thign to try then. But I need to figure out what I'm
doing for food now...

[6:50 PM]

----

Of course, kept playing.  Set up two pairs in sync orbit, and noticed
they started to move towards each other -- gravity!  But also noticed
that they don't tend in general to stay syncronized when playing with
three.  And my simulation blows up when they get too close and start
swapping electrons.  So I still need to fix the simulation before I
can get any decent idea of how these work in groups...

The dt auto-tune seems useful, but it strikes me as not being able
to fix this core problem of an e diving towards a lone p and having
a near collision followed by a simulation blow up.  The system needs
to catch the case of one e being too close to one P and not just
reducing DT, but adjusting the veloctiy/position so when it comes
out the other side it's got the right energy.

This might be an issue of adjusting v and p not just v to correct
for energy, and to use an algorithm of adjusting p that correctly
does most the work for the one that is the most broken to automatially
catch this.

So I think step one is to think about the process of calcuationg
new p(position) and v (velocity), then fixing the ending v to fix
energy. But then go back and redue to position calculation baesd
on the new average v and again, once again calculate the new
ending v to fix energy and see if this convergers.

----

[10:49 PM]

Was thinking about how I was coding the updates and realized I was
probably doing it wrong to start with.  If I know the current position,
velocity, and accelearation, and I want to update for a time period of
DT, what I'm doing now is updating the veloicty by the accelearation,
then assuming the new velocity stays the same for DT.  If we assume
constant acceleaqation (a simplification), then the velocity will
change form it's current value to the new value, over the full DT period.

So if it starts at 10 mph, and accelarates for an hour at 1 mph per hour,
then it will be 11 mph at the end of an hour.  But it's final position
will not be 10 miles down the road or 11, it will be something between
10 and 11.  Will it be 10.5?  Let me check the math. Yes, it will!

So I shouldn't have been using the starting or ending velocity, I should
have been using the average.

Oh, but I know the ending acceleration as well!  So I can do even better
by adjusting the ending position to account for both the changing velocity
and the changing acceleration.  The math gets tricker to do that.  But
I can assume a linear change from the starting to ending accelearation
as well.  And that requies itteration.

So to start, the first guess at how posoition will change, should be
pased on known position, velocity, and acceleration, with the assumption
acceleration will be constant over the range.

Then I can itterate, by finding the ending acceperation, and then 
doing it again, assuming acceleration will change over that period in
a linear way. And keep repeating until it converges maybe?  And if
it's not converging, dt may need to be smaller?

No clue. But let me code the first step and see what that does.

================================================================================

2016-08-23 11:24 AM Tuesday

Yesterday

1) Coded system to adjust velocity after update to fix conservation of
energy.  Found this couldn't always work if the position was too far off.
So did a hack that tracked total error fixed it later as the electron
came back to the protrons.  Stupid approach, but I was just playing.

2) found my setKenetic was broken as I coded it before and fixed
it. Played a lot to learn more about how it all works.

3) realized I really wasn't doing the basic update correctly.
The velocity should be understood as the current velocity and the
acceleration created by the force will slowly change it over the comming
DT period resulting in a different velocity at the next point.  But the
velocity will chagne from one to the next and the chagne in psotion will
be based on not the the starting or ending velociety, but the average
over that period. So I fixed the code to do it that way.

But the fix seems to work less well.  Odd.  Not sure why.  Eleptical
orbits were quickly turning into circular orbits instead of staying
eleptical as they should.

4) Changed bounce to reduce total ke by 1/4 instead of reducing only the
dimension that bounced by half.  This is a true half speed bounce now.
Wasn't before.

5) Changed "total energy" debug that was divided by starting energy to
normalize to make it subtract starting energy instead. But that doesn't
work either.  Made me realizse that the total energy concept is bougus,
because the total potential energy of an e p pair is always infinite --
total ke that will be generated as it falls together is infinite. So there
is no simple concept of total energy in a given configuaiton of the system
-- it's always infinite.  Energy in a ee pair or pp pair however is finite
beause the total KE generated as they fly apart is finite not infinite.

Today

Will keep working on the simulation algorithm to understand how to make
it it work better.

Ideas

1) Make sure the average velocity algoirthm is coded correctly and
compare to the old alogirhtm.

2) Maybe take more care in setting up a system to test how well different
algoirthms work -- like ploting a short path with high accuracy using
very short DT, then using long DT, see how well different algorithms track
the correct path.  The complex trade off is finding what algoirthm is the
most effecent for producing a given level of accuracy in the simulation.

3) Need the system to know when it's not working so it can either reduce
dt or just report the max error or something.

4) Updating velocity alone doesn't work when e and p get too close.
One close land next together causes the pair to shoot off at too high a
velocity making the next estimated location way beyond what can be fixed.
So I eithr need to take another itteration to adjust position, or detect
the problem, and reduce dt and repeat.

5) The way I spread energy error over the particles is not good.
I think I do it based on how much Kenetic energy is in each particle?
So the largest kenetic energy particle gets the most error adjustment?
But I think I need to do it based on an estimate of which particle has
the most error.  Which might mean the most change in acceleartion --
or the largest acceleration?  or the largest acceleation vs velocity?
I think I can do better than what I'm currently doing.  Largest _change_
in kenetic energy might be the ticket?  Or largest change in velocity that
includes a shift in direction as well?  Ah, check for the total change in
each of the three dimentions so even if total ke doesn't change becuse
magnatude of V doesn't change, if it points 90 degrees in a different
direction, it's a large change?

6) Will certaonly have to do multiple itterations on estimating the next
posoiton and velociety to keep accuracy better. Either by reducing DT to
get the next step closer to correct, or just itterating over different
starting and ending position, velocity and accelearation numbers  until
the energy error converges to zero.  Would be nice if I could just keep
itterating and converging to make it go to any level of accuracy I desire.

----

[12:49 PM]

Ok, testing code, didn't understand why enegey was not staying at zero.
It's becuase I had turned off the energy adjusting code!  Duh.  Turned it
on and got it working better.

Rewrote the energy adjust to just use the starting energy of the system
as the reference value that makes it all far simpler.

This new algoirthm turns elepical orbits into round ones very quickly
if the dt is too large. Odd.  The old algorithm that was "worse" didn't
do that as quickly I don't think.

And still, by adjusting energy only, it can't fix the problem of position
being way off when a simulation step for an electon lands too close to
proton and causes it to shoot out into space.  Smaller dt steps can't
fix that if if the pasts cross.  It can fix it if they don't cross as
long as the dt steps makes the closest appraoch accurate still.

So to allow them to cross I must update position as well as velocity
and try to get the two to converge on the best answer.

So, let me first code the position update.

----

[1:28 PM]

Wrote code to speed up and slow down simulation by cutting dt in half or
by doubling it.  Wrote reset to allow the current step to be backed up
and repeated.  Triggers a slow down if the total energy error is large
compared to the total ke.  Speeds up if it's small and has been small
for 10 steps. It's cool that the DT value is now self regulating.


Running this with 3 ep pairs has been stable for some time now.  No blow
ups. The relative position of the three protons seem very fixed.
Not driving together or apart as seen in the past.  Each electon is
staying fixed to it's proton.

Orbits are eliptical and are remaning that way, though their shape and
direction is in constant slow flux as the three interact with each other.

Dt is hanging around 4e-18 range.

Speed of each orbit seems roughly fixed as well no tendency for some to
go into wide orbit as others cirucle down to tight orbit.

But the three orbits certainlyu are not well syncchronized either.
Just close toether in speed. It feels like the three electons are pushing
each other back and forth but keeping the group in synce.  Maybe after
running for a lot longer it will change?  Or is it sable?

----

After more running, there is a tendency of the protons to move together
but it could be noise vers long term tendency.  Spacing is 4.99 4.98 and
9.98 vs 5 5 and 10.  Electons are still behaving themslves and sticking
to their own protons.

----

[2:36 PM]

Tested three pairs with extra energy in one electron that drives it to
the wall very quickly. The bounce that takes energy out was being seen
as a simulation error so I had to update the systems total energy on
the bounce.  But it took me a while to make that work due to globals not
working as expected in Phthon.  But it's cool becuase the extra energy
makes the system go craxy and fling off an electon multiple times to
hit the wall untkl enough enough has been sucked out of the system to
allow it to stablize at which time each electron pairs up with a proton
in stable somewhat syncrhronized orbits!

This is all working much better with the auto ajusting dt code now. Or at
least it seems to be. And even when the systme is going crazy, electons
will drive down into a protron, and the simulation will have to slow
way down to deal with it, but it does, and it moves on, then in time,
the orbits aren't passing that close to the center.

But I've been avoiding the hard test of them passing through each other.
Guess I need to go ahead and try that.

Also, when the bounce was broken it got into a steady state of decreasing
dt until it turned zero.  That could happen in theory if a proton and
electon got to close to each other and the simulation would lock up --
or bomb.  I don't test or cope with that yet.

----

Oh damn, going back to one pair it becomes obvious that this simulation
alogirhtm can't hold an eleptical orbit.   It quickly builds up error
that turns it into a circulare orbit. Well, I'll have to do better than
that. That probably helps these three pairs remain stable.

BTW, the tree pairs I started some time back, is continusing to see the
protons move closer together. 5.97 4.95 and 9.93.  This is three in a
row BTW.

----

Ok, one pair, zero velocity -- triggers divide by zero on the speed
up and slow dwon test.  Which I fixed.  Then the electon falls to the
proton, and the dt goes to e-42 and the simulation seems to freeze.
Even though dt is not zero, I suspect there is a rounding error that
hasn't just slowed it down to be very slow, but has actually caused it
to stop changing the state.  So it's getting stuck.

But if I give it a y veloctiy of 1 (vs 16000 the normal needed to make it
orbit, it works.  It dives in, the simuation slows way down to something
small, and it bounces back out -- and it loops maybe 10 times like this
before the orbit widens out into a circle.

So as long as there isn't a perfect overlap, this system can handle
it. But the default error seems to be going wide on a tight turn now,
which turns eliptical orbits into circles very fast.

In anything more complex than the worse case of two particles falling
stright towards each other along one axes, the system will problly not
get hung like this ever.  Let me test two particles starting at zero
velocity, but not on the same axes and see if simply the rounding error
of the two axes creates enough difference to make it not hang!

----

[3:18 PM]

Looking nice.  System still hung when the two e p started at an angle
from each other.  They would fall together and the simulation would slow
to dt of e-45 and stop making progress.

But, I limited de to be e-30 and no smaller than that, and then it works.
Sort of. It falls together, slows down to e-30, then either pops out the
other side or pops back out the same side.  But when starting with not y
velocity, it seems to stay stuck in that falling in and out but randomly
either poping out the same side or the other. I see no y error creeping in
(seems like it can't) so it can't turn this flat eleptical into a circle.
if start with an angle the error wlil probably accumuate in the x vs y
over time and turn into a cicle. Let me test that.

But limiting dt to no smaller than e-30 seems to make it behave in a very
well controlled fashion -- it will be accumutling errors at e-30 when it
passes too close to overlap, but it won't blow up it seems.  or at least,
as not as long as the velocity is not too great.  Really high velocities
would require smaller steps.

So, I need to test the angle theory.

But there's also room here to improve the simulation steps by itteration
instead of just slowing down dt and restarting. I need to explore that
path as well.


----

Yes, when testing from a dead stop with e p pair at an angle, the error
accumualted over maybe 10 orbits and then turned the flat elpise into
a circle as I was guessing it would.

So this simulator has error issues, but yet it's looking very stable
and good.  And I have to wonder even if this error of turning elipses
into circles is bad since given the complex behavior of more than two
particles.  Maybe this approach is good enough.

But I have to explore the option of itterating to improve the next step
without slowing down DT and see what that results.  Certaonly if theres
error in the other direction, and circles turn into flat elipses that
would not be good.

But this error adjust might be spreading energy through the system in
an invalid way. The reason all the electons stay in sync might be due
to the error spreading alogorithm instead of what it would do if the
simulation was perfect?

I guess truning off the error spreading adjustment might yield some
insight into what the rest of the algoirhtm is doing?

----

[5:45 PM]

Went to pizza.  Just got back.  Going to movie with Pig soon.

Started 4 different runs before leaving.  Three of them have three
pairs running to test if they pull together over time and to see how
they evolve after they pull together.

Two are getting closer.  But one which had the center of three spnning
backwards from the outer two, is drifting apart!

One had the right pair spinning in a different direction, and it's
pulling close, but the center pair is drifting down forming a triangle.
Intersting and off behavior.

I might need to create something that does a long simulation and then
turn it into a short movie.  Might run much fast4er without constant
screen updates.  All things to explore later.

Also started one in a fake hydrogen configuration with two protons close
together with one electon, and then a second electron further out in orbit
around them.  The two prtons are staying close together with one electon
in tight orbit now around one proton.  The second protron is stuck out
away but standing almost still at the moment.  It will be interesting to
see how that evolves -- will it spread apart and turn into two sperate
pairs with somewhat equal orbits in time?  Or stay together like that?
Or just what?  But because of the tight orbit the simulation is running
e-18 slow so it will take a long time.

And of course, I don't fully trust this simulation code to be producing
anything related to how it would work if the simulationj was perfect.

But it's fun just to see what might evolve with this code to get a feel
of what might be possible.

But the next step is really to look at the itteration option for improving
the simulation accuracy.

================================================================================

2016-08-23 10:08 AM Wednesday

Yesterday

1) Added DT auto adjust to lower and raise speed of simulation as needed
to maintain accuracy.  Works much better this way.  Had to add code to
allow the current step to be restarted to make that work.  Limits how
small DT can get to keep it from going to zero or running the simulation
too slow -- it's ok to have small errors is my though?

2) Cleaned up energy adjust to use starting energy -- that made it
simpler.

3) Left 5 different tests running over night for the first time to see
how long term effects worked. No great discoveries.    But I'm gaining
my first understanding of how these systems work -- at least in 2D.
The code works in 3D but all my tests have only been 2D so far with 0
values for all Z dimensions.

4) Cleaned up debug some as well.

Today

1) More work on the core simulation code still needed.  Current code is
much better than the past but still has issues to consier.  Elitpcal
orbit of ep pair quickly turns into circular orbit.  It should stay
elitipcal but doesn't for some reason.  It wasn't doing this with the
old code.  But I don't know if this was due to switching to the use of
velocity average vs ending that cuased it, or some effect caused by the
energy conservation code.  Or both working together? I need to write
the itteration code next so when when the next position is picked, we
calucate the ending force and acceleration, then go back and re-calcuate
the ending knowing how the force changes, and repeat this to try and
make it converge on a good answer.

2) Nagging concern that the energy equalizing code is putting energy
into the wrong particles and as a result, creating emergent properties
that have nothing to do with the therotical model.

2) In working on new simulation, I need better tests to compare how each
version works so I can understand what the differences are.

The current simulation is now working well enough to "look" like it's
doing most the things it should.  No more electrons flying off the
screen at high velocity due to simulation errors.  ep pair falling
stright together doesn't crash the system either.  Sometimes is passes
though to the other side (like it should) and sometimes it bounces back
out the way it fell in (wrong but not so bad really). In both costs,
the velocity and apex of the escape look good.  At least I can trust
the simulation won't blow up now when an ep get too close.  But I need
to explore the accuracly question.

Momentum is well maintaned near 0 so the system doesn't start to drift off
the screen either.  But there are clear problems still.  Two particles
in an eleptical orbit will quickly turn into a circular orbit instead
of staying in the eleptical orbit like it should.

Since nothing I write can simulate the differential equations perfectly
there is always the naging question of whether the system will demonstate
the correct emergent properoties. And if it doesn't match realility,
is it becaues the model is just not how realiety works, or is it that
the simulation error prevents us from seeing reality?

The energey equalizing code is a real question in my view  When the
simulation predicts the next step, and we recalucate energy and see it's
not constant, we know the next state is slightly wrong.  But how do we
know that when we tweak the state to fix the energy error, that we are
not putting energy into the wrong place?  Currently, I'm fixing kenetic
energy by adjusting the speed of particles -- but what I could be doing
is sucking energy out of one particle and putting it into another even
though that would not have happaned with the real system in the same way.

My over night runs have all showed the system like to keep all the
electons in the same orbit -- I'm not seeing a tendency of erngy to
flow from out of one and into another, making one have a thight low
orbit and the high energy particle a high slow orbit.  That might be
becuaes of the enregy balancing fix and not becuaes of a natural effect
of the interaction.

I guess I need to code different ways of trying to fix all this and test
them against each other by turning different ones on and off to get a
feel for what emergent properties might be a type of truth about the
system, vs a property of the way it's coded.

Ok, but first, lets dig into the itteration code approach and see what
happens.

----

[3:15 PM]

Write itteration code.  Seems to do a good job.  Had to first figure
out the doble intergral of acceleration to figure out the code.

 xe = 1/2 (2 as + ae)/3 ts^2 + vs ts + xs

 xs - x starting

 xe - x ending

 as - acceleration starting

 ae - acceleration ending

 vs - velocity start

Notice veloicty end is not even used when updating position.

Basiclly just a variation of 1/2 a t^2.

Code is somewhat a mess now.  It will be time for a major cleanup soon.

Good news is that with the new code, the epictial orbit doesn't turn
into a circle.  It stays eliptical at least for a good long time.
It does drift around slowly, but that seems ok maybe.

Tried to test the two e p falling to each other, but it triggered a
bug that drove me insane.  Finally figured out it was reltaed to the
fact that with zero velocity, the energy adjust code could not work
and it confused the dt adjust code as well.  Didn't fix the bug. but
just stopped trying to play with the fall together code...  Too tired
to think it through at the moment.

Doing a new two p and one e test, the behavior is very different than
before.  not sure if it's starting condition difference or emulator
difference.  But the e started in a tight eliptical orbit about one with
tail pointing to second p.  That sucked the second p closer, unilt the e
was able to jump back and forth between the two p in tight elitpcail like
falt orbit -- but not so regular.  Sort of figure 8 and sort of random.

Oh, learned that on angstrom was e-10 meters which is what I was using
for the Spacing var.  And it's about the diameter of h or he atom?

doing the p e p test again and seeing strange results.  The electon was
jumping back and forth and with the left p, sucking the right p close,
but then when I wasn't watching, the electon has moved to the rigght p,
and it's doing odd orbits to the outside and makign the other p push away.
Ah, but now that it's far enough away, it's truned into flat orbit with
tail pointing to other p.  Sucking it back I belive.

At 2a distance it started jumping back and forth.  figure 8 with a real
flat eleptical orbit.

----

Yes, the new code is acting very different frm the old.  With the old
three ep pairs would stay in orbits but with the new code, the paths
turn electipical and very chaotic very fast with the electons diving down
towards a protron.  Just alittle difference in the simulation lgoick is
producing very big differences in emergent behavior.  Which is a problem.

How do I know the current is anything near right?

Next big issue is how I do the energy adjust using ke.  Don't know if
that might be a key factor in what emergent beahviors show up.

The itteration logic stops improving at something like e-22 energy error
which to me shows that the assumption that acceleration chagnes linneraly
is of course wrong and this amount of error is the best you can with
that assumption.  So smaller levels of dt are needed to do better I guess.
To better fit the acceleartion differences to their true path.

So I guess other than the energy adjust logic, I need to run tests with
very small and slow dt, and compare to see how much it diverges or differs
from test with larger dt.

But, grass needs to be cut before movie tonight.....

[4:00 PM]

----

[9:54 PM]

Back from movie.  Bridget Jones' Baby.  Very touching and funny.

Two proton plus + 1 electon run seems stable at about a spacing of
2 with electon jumping back and forth between the two in a tight
figure 8 that looks so flat it's more like just jumping back and
forth.

I was playing with code earlier and doing a bit of clean up and
commenting out debug of the itteration code.  The itteration code
is a mess and is structured all wrong -- cuases it to duplicate
work in many cases.  That will need clean up.

Added dtMin to match dtMax.  When the dt value is kept small the
error as expected is very small.  I think numbers were e-22 energy
error (on e-18 total) when dt was e-18, but by making it real
slow it would get all the way to e-35 error I think.  But it might
take a year to do one orbit revolution at that speed!

Got some bug in the newly hacked code.  Trying to run the three
in a row orbits now cuases the second two to dive towards protons
instead of cleably orbiting in synce but then the simulation
locks up with a very small dt even though the electrons aren't all
that close to the protons -- still not overlapping on the graphics.
Odd.

Need to do major careful clean up of itteration and dt adjust code
tomorrow and figure out what's going on.

----

Ok, found bug.  The min() and max() code I got backwards so
it was setting dt to the max or the min with every step.
Now it's working, but the three pair test is orbting very slow to
keep up with the error rate.  The new simulation code tends
to see electrons get near to protons a lot more often than the
old code that tended to force more circular orbits.

Need to set error level for auto adjust code so I can choose
to run simulations faster with more error or slower with less
error.  But need to clean up code first.

Will let some slow but more accurate tests run overnight.

================================================================================

2016-08-25 12:03 PM Thursday

Yesterday

1) Added itteration of next position using linear estimation of
acceleration.  Found it convergs very quickly in only two or three
itterations and is certainly well worth the CPU cycles in how it
improves the next state estimate.  But the code is a mess and needs a
major cleanup.  Had to figure out the calculus to derive the formual
for using acceleration to estimate position.

2) improved DT auto adjust, added dtMax to echo dtMin.

3) Lots of testing and experimenting and code tweaks.

4) Left a couple of experiments runing with current code.  2p and one e
was showing a good tendency to lock together with the e bouncing back
and forth.  Locked at about a 2.2 A distance for the given energy I
started it with.  But after running over night, the one p has escaped
and is 13+ A away.  But the distance is closing slowly and it looks like
they might come back together in time.

This might have been a code blow up problem instead of a natrual
evolution.  I thnk I'm still looking at a blow up when the e gets so
close to the p that reducing dt doesn't fix it then it's random luck
whether it blows up or not.  So I think current code fails to prevent
blow up -- it just reduces the odds to be much smaller -- not good enough.

5) Second experment was 3 pairs in a row with synchronized orbits.
They have drifted randomly apart but are currently orbiting -- or more
accuratly in very flat pulsating orbits.

Today

Need to make DT tracking cleaner.  Need to make error level it seeks
a paramater and need to think more about what error to actually track.
I'm using energy error / totalKE currently as the measure.  Not a bad
choice, but probably not the best either.

Need to clean up itteration code. It's badly structured currently because
I just did a dirty hack to see if it was worth doing.  It is worth doing,
so I need to clean that up.

Need to tackle energy adjust next.  I spread the energy error across
all particles but I need to think about whther how I assign error is
the best way to identify which particles are most liekly to be in error.

And here's a new thought.  If I do an energy adjust, should I try the
itteration again with the adjusted ending velocities?  Ah, but that's
pointless becuase I dont use the ending velocity in the formaula to
pick the next location -- only the accelerations.  So I would need a
new formula that used ending velocity.  Ok, so will have to think moare
abot that -- what things I might be able to itterate on.  Well no what
I would be itterating on is trying to reduce all errors including energy
and monentum?

The big problem of simulation remains.  The fall through problem.  When
the dt itteration step is not small enough and the particle takes
too big a step over a complex changing force field.  I guess to continue
my attack against this problem I need to keep looking at the fall though
problem.  Maybe set up two e and two p in fall through configuation at
differeent frequencies.  Well, first, make sure I have solved the one
pair problem -- does the energy adjust work correctly?  Or might it
get the sign of the velocity wrong?  And then see if it can solve the
two or more fall through problem without putting energy in the wrong
place.

Ok, so lets first clean up the code for itteration and get that done.

[12:45 PM]

----

[6:38 PM]

Been doing major clean up of code.  Then ran into a bug I didn't
understand which caused it to run very slow.  Set up a two e test, and
added debug until I traced down the problem for that test.  Found the
reset after move code was not resetting force as one bug.  Need to get
rid of that and not move till later so it doesn't even need to do a reset.
But that wasn't the real issue.

The real issue was that PE was -19 where KE was -45 so that when we add
these two togethre to get total E, all the KE information was lost in the
lack of precision. Then when we subjtract two of these to find a diff,
the system can't see the KE diff at all.  And the PE was the same.
But once speed got fast enough, and KE shows up it looks like a huge
error ast the top bit of KE overflows into the bototm bit if PE.

Fix is to calcuate difference as (PEstart-PEend) + (KEstart-KEend)
instead of doing it as (PEstart+KEstart) - (PEend+KEend)

That fixed it so as to allow it not to keep scalling DT up and down --
but ran into the problem that it had to keep DT very small due to the
fact that the total error was large compared to size of the KE which
was very very small.  So, need to think some more on whether this is
the right thing to check in the DT tracking code.

And, more cleanup -- fix the use of move() and reset() when the system
simply should not move until it's decided not to reset and start over.

I also made a DT speed up make the move, and use the new dt for the next
one instead of restarting the current one.  That's a waste.  We already
calucated the new location using the small DT so go ahead and make use
of it.

But, off to movie now.  Will look more this later when I get back.

----

[10:14 PM]

Back from movie.  War Dogs.  Liked it.  Good story.

Rebooted skinner desktop -- needed software update.

The 2pe simulation had driffeted apart again.  The 3(ep) was running with
wide gaps between the three -- no sign of them wanting to seek together.

Need to clean up code more and think about this issue of how error was
being calculated and why on this one case it was such a problem.

Also, I don't know if the problem I uncovered with the two electons
is the same problem I was seeing before I started to look closer.  So
there may be more at work here.

----

[11:51 PM]

Lots of little code clean up and playing.

Made starting energy two vars instead of one to cope with the different
error problem.

Made a bounce reposition particle inside screen.  Not so sure about the
quality of that code however.

Made move() only happen after we decided not to restart for DT.  So that
made code a little cleaner.  no more Old values to save and restore.

Played a bit with one p and two e.  Intersting. One e stuck to the protron
and the other was being pushed away.  So while a p is attracted to an ep
pair, an e seems to be repeled from it!

Added a few divide by zero checks that were caused when the starting
configuration had multiple particles on the exact same location.

Restarted a pep test to see if it works.

================================================================================

2016-08-26 11:53 AM Friday

Yesterday

1) Lots of basic code clean up for DT adjust and itteration now that I
decided I like both these apporachs.  More still needed.  But it's not
a mess anymore.

2) Came across computational problem with energy error calculation.
With two e the PE was large and the KE started at zero and stayed
very small.  So small that it was below the significant digits of
the PE value.  When trying to calculate total E, all ke data was lost.
But doing the substraction to compute error differently impoved that (but
didn't really fix the underlying question).  The computional problem
confused the DT auto adjust becuase error was zero for a long time,
then jumped to something non zero.  Impoved this issue some by just by
keeping PE and KE separate.  But how DT auto adjust for error has some
fundamental issues that need to be thought througthrough.

3) Changed bounce to force particle to stay on screen.  Code is not
clean and I don't trust it.

4) Added clear screen to display instead of simple scroling.

5) Tried to calculate PE from total force vectors -- but figured out
that was not possible.  Energy is force times distance but the correct
distance to use is unkonwn if you are only given the force vectors. Also
realized that the force calculation and the PE calculation was duplicating
a lot of work that could be reduced by doing both at the same time.

6) Chagned DT adjust so it didn't have to undo the move.  Also realized
that when making DT larger, there is no need to throw away the small
step just computed.  So it keeps that last small step, and used the new
larget DT for the next step now.  Much better.  Restart is only needed
when DT is made smaller becaue there is too much error.  Cleaned up
restart so it didn't waste time redrawing and producing extra output.

Today

More clean up and experimentation needed.  Emulation loop is not in
a class, still just top level code and there's now world class as
needed so that all needs to be refactored.

Focus is still on testing and improving the core simulation algorithm.

Will not do major refeactor until I'm happy with the core emulation
algorithm.

pep test from last night has allowed the two p to drift far apart.  The e
is pulsating at one p towards the other -- don't understand why it's not
pulling the two back together.  Don't understand how it drifted apart.
But I thought this might be a blow up yesterday, but today I'm thinking
this is just how it works.  Maybe it pulls them together too fast, and
then switches to an orbit that releases it once it gets too close, like
shooting it off like a rubber band?  I need to watch it more carefully.

At some point, the slow speed of the emulation will force me to create
a system to save a history and play it back faster.

Also, all this is 2D.  Things might be very different in 3D.  I'll need
to try that soon.  The code should work, just need to add some non-zero
z values to get it started.

I have real concerns about whether the energy adjusting is not having a
significant effect on the behavior -- Assuming it's not working perfectly
we are dealing with a problem of enrgy "leaking" from one place in the
emultion to others that maybe it shouldn't be leaking.

If I dodn't "fix" the energy, then the system will be randomlyu adding
and removing energy -- which is clearly bogus.  So I think I have to
fix the enrgy, but what is the best way to do this?

Maybe I really need to use an itteration loop that minimizes energy
error to whatever level I need? Seems like I need to get a better grasp
on understanding where the error really is in the system.

The current itteration logic should be good to get the first
approximation.  But then another form of itteration might be needed
to take the error out while allowing the assumption of linear A to
be ignored?

Maybe it's time to go back to the fall though problem because thats sort
of a worse case to solve?

----

Playing with fall through problem.

Figured out that double prection has 16 digits of accuracy.  So when the
energy is e-19, then 19+16 = 35, so the error that happens at the limit
of the precision is e-35.  Which is the error we see in the momentum.
We can't expect better results than that since it is the limit of the
precision.

But, by keeping pe and ke separate we might be able to do something
better.  Maybe.

Also, in the fall though, which starts with zero kenetic energy, ht
hangs as dt=e-30 at the start due to the fact that the total ke error
is large due to the very very small total ke.  Haven't looked carefully
at the numbers, but I just get around that by setting dtMin to something
high to force the simulation to run faster.

The electron sometimes bounces back, and sometimes falls through.
Don't understand why this happens yet.  Same thing I saw before.  But I
need to think this through and understand why it doesn't always fall
through correctly.

----

Added max energy error measure.

On fall through where I force dt to stay large like e-18, the ke is
normally around e-19, but the max error is a bad as 0.6!  I've turned
off the energy error correction to see what happens.

The new bounce logic that keeps the particles in the windoes is allows
this simulation to keep running even after the velocity is all messed
up and since bounce takes half the speed out (75% of energy out) it
slows back down and recovers.

So, on this problem, we always have the option to just calculate what
the velocity should be, and set it to that.  Or can we?

So we have some velocity for the two particles, and a known distance.
And we know the starting energy, so we can calucate the energy error
though the significant digits could be an issue.

And if the particles are too far apart due to falure to estimate location
then we can't fix by adjusting speed alone.

And direction of velocity has nothing to do with energy, so adjusting
for energy alone doesn't fix a direction error.

But maybe momentum?  Adjusting for zero monumtum takes into account the
velocity direction.  Maybe that's the key?  Itterate to minimize both
the energy error and momuntum error?  So if we assume the position is
correct (not always true) maybe we can itterate to reduce energy and
monemtum errors to zero?  So if position is fixed, PE is fixed then we
are itterating to make ke correct.  And if assume direction is correct,
then we are just using monumtum to tell us how to distribute the KE error?
With two partiles, we only have to varibles (velocity of each), and two
varibles to minmize, so that should work. But with n particles, that
won't work -- we have too many degrees of freedom.  we can solve it,
but there are many solutions.  Like a+B+c+d = 6.

Ok, my current code assumes position, and direction is AND relative KE
per particle is all correct, leaving only one variable to adjust which
is the total KE.

So I have N*2 varibles to adjust (position and velocity or position and
momentum) but only two varibles to minimize -- total energy error and
total monumtum of zero.  So that can't produce a "correct" answer.

But what about looking at the individual particle to particle change
in PE.  If particles A and B move farther apart or close together, the
velocity vector in that direction should change by that difference no?
And the monumtum of the two should remain the same.  That sounds correct.
And simple if the angle of the path between the two remains the same.
But since it changes, how does that work?

So, if they come closer together, the velocity increases.  Ah, what if
we just look at this as an energy trasfer from PE to KE.  So if they
come closer together, the KE goes up in both particles.  Ok but we have
the same problem of not knowing how much ke to assign to each particle.

Or maybe we do?  If we look at the problem from a frame of reference
of the frist particle, we can say all the KE change is in the second
particle.  If that is the E, then the relative velocity will change a lot.
If the second is the P, the velocity will change only a little.  But no
matter which frame of reference we look at, the velocity change must be
the same! The system can't act differently just becuase we are stadning
in different place to observe it.  Which I think is the conservation
of monumtum issue.  The static frame of reference where the veloctiy
change is the same is the frame of reference where conservation of
monentum happens.

So, can we say that the frame of reference where conservation of momuntum
happens is the same for all particles in the system?

Ok, so the direction problem is that the direction of transfer of energy
shifts over the time period.  But if the transfer is always always
maintains conservation of momentum, then maybe it's valid to say that by
the end we still know the total KE transfer -- we just don't know the
correct direction.  So if KE increases by x, we can say that the KE of
each particle must increase by x such that momentum is balanced.  Ah,
but I sense a problem.  ke is v^2.  momnumtum is just v.  To calcuate
the KE difference we need to know the current v of each particle I
think. lets check the math...

u is monumtum, m is mass, v is velocity

u1 = m1 v1

u2 = m2 v2

ke1 = m1 v1 v1

ke2 = m2 v2 v2

If potentical energy changes by ed (energy difference) then I have ke1 +
ke2 .... ahhhh.....

I have two unknown velocities and two equations, one for energy change
and one for monuntum.  So that could be solved, If we assume velocity
direction is correct, and we are onky solving for the two velocity
magnatudes.

Still confused.

Ok, but I think (hope) I'm on to something here.  The energy transfer in
the system I think can be broken down to particle to particle transfers
and I think they always have to follow a general rule of conservation
of energy and momentum. The way particle a pulls on b is always equal
to the way b pulls on a.  The force is not the same (as it often is in
macro physics problems), but the acceleration is.  No, wrong. The force
is what is the same, and the acceleration is different -- the lighter
weight particle accelerates more.

Oh, oh!  If we project the velocity vector onto the starting vector of
the two particles postion difference, we can get a starting PE with the
distance, and starting KE for each paticle, but only relative to that
projection line.  And that gives us a starting momuntum of that two
particle pair.

Then, given a guess at the ending position, we can measure the relative
PE to see how that changed, and then assign how much the veloicty of
each has to change, to maintain the same momentum.  So we can directly
calculate the ending velocity vectors along the projection line of the
connecting path.  Maybe.  If this is true, given an ending position,
we can directly calculate all the ending velocities of all particles.
If this is true, then any ending position, will have one and only
posisble solution.

But this ideas is making an assumption that probably isn't valid?
That the magnatude of the momentum is conserved even whtn the angle of
the line changes?

If two particles are both in line on the horizontal, and drifting to
the right -- +x monentum, and the line rotates

Ok, so looking at two particles in issolation, the forces always match
and balance so the a will always be matched by the mass ratio, so the
change in V will always be balanced by the mass ratio  which means the
monentum will change will always be equal.  After period DT the total
chagne in monentum of p1 will be exactly balanced by the total chagne
in P2.  As a momentum vector.  So for each pair we have one monumtum
change vector.  But the energy change is u*v.

----

[7:18 PM]

Went out to pizza.  Got money at bank. Wasted time on facebook.
Switched auto payment of Verizon Fios from newsreader to ayrhillforge.
Need to shut down the newsreader checking acount.  Bitching at Michael
about failure ot grasp capitalism some more.

But back to trying to grasp the math and mechanics of orbits.

I think there is still likely something important in looking at the
particle to particle energy transfers.  The path a particle takes
is the sum of the effects of all other particles pushing it around.

So what do I think I know for sure about this?

The change in distance between to particles, creates a change in
potential energy, that has to show up elsewhere in the system.  Is
it fair to say it always shows up as a change in the KE of the two
particles on either end?

Instead of trying to analize the math, I guess I could just code
this and test it.  But what to code?  For a difference in distance
We can translate to the chagne in PE for any pair.  Then we must
allocate that energy diff to each particle.  Can we?  Do we know
how much goes to each?

I get a formula something like this that tells us the ending velocity
given the starting velocity based on kenentic energy change needed to
offset the PE change. (e is the PE change) (missing some unit conversion
constants)

v = sqrt(sqrt(2*e/m) + v^2)

Does this formula tell us anything real?  Looking at orbits, I see
it does not tell us the velocity chagne along the connecting line.
It's a measure of velociety change total in both.  No, wrong.  It's a
measure of velocity chagne if only one changed. But sinc eboth change,
it's really a measure of some undefined compoent of the total change.
I think it tells us nothing useful here.

Shit. Still lost.  What do we know about the matching pairs?  The total
change in monentum has to be equal and opposit at all times?  No, wait
the intergral of the force vector is equal and opposit in the pair.
And the intergral of the force vector is the total change in velocity
is it not?  Or at least, when adjusted by the mass?

Ok, a question.  Force and mass defines change in velocity.  does
the starting velocy make a difference to energy chanage?  So
veloicty is 0 and increased to 1, energy goes from 0 to 1.

Velocity is 10, and increases to 11, energy goes from 100, to 121,
a difference of  21, so clearly, initial velicty is important to
understanding energy change.

So with two particles, if the distance changes the pe changes, but the
energy is not just allocated by the total change in distance over time.
But,  if one particle stays in the same place and the other is moving
away fast, then the energy change is mostly in the fast moving?

Still lost.  This frame of referece question I don't know the real
answer to.  Can you adjust all numbers in the system to a different
frame of reference (moving, roating, accelerating, etc), and have all
the math still work the same to explain all the paths in that new
frame of reference?

Does the math translate along with the frame?

If we scale by changing units it must work. So changing from units
of meters to ft, the math all stays the same.  But constants may
need to adjust?  Yeah, mass units and distance and time units all
need to match so if you change one unit and not the other, constants
may be needed to adjust.  But the math would of course still work.
But if all units were just cut in half for example, maybe nothing
changes? I would have to check the math to be sure and I'm not going
to do that at the moment.

What if we add velocity to all objects?  lets just check 1d.

To particles are still at start, so ke = 0, pe is something.

We tranlate all to a moving frame.  The moving frame has far more ke at
the start.  But force is the same.  To change in velociety is the same.
They will just accelerate tworads each other.

Yes, switching to a moving frame of refence changes the nmbers but not
the results.  The loss in distance between two particles still translates
to the change in KE of the two particles.

But the change in KE of the individual is far greater, but the sum of
the change, is identical.

So from a stand still, 0 KE at start, to accelerate to 1 m/s of a mass
of two, the KE increased by 1 for each, or 2 total.

When moving at 10 m/s, one incrases to 11 and the other drops to 9.
Which is 21 unit increase and a 19 unit decrease.  But the sum is still 2.
So the indivdiaul chagnes in energy look very different, but the relative
change is the same.

So change in energy value for a given particle is senstive to the frame
we choose to measure from.  But total energy change is not.

And total monumtum is different based on frame, but change in momuntum
is not.

So what if we define a frame relative to one particle and look at all
others relative to that?  That's not just a simple constant v frame, but
one that is changing.  What might that do?  It seems it has a chance
to work.  But is it if any value to look at the system from that
perspective?

All this is going back to the problem of why doesn't the simple fallthough
test work right.  (bounce sometimes instead of pass through) And the answer
I think is that we can calculate the correct total energy, but not the
correct velocity direction.

================================================================================

2016-08-27 8:46 AM Saturday

Yesterday

1) Lots of time looking at fallthrough problem without comming up with
any answers.  Grasping at straws trying to find a way to even attack the
problem.  It should pass through, but sometimes bounces back instead.
Trying to understand why the algoirthm doesn't work and how to fix it
so it will always work.  Didn't come up with a good answer to either
question.

2) More time trying to understand a different attack on the entire
algoirthm to converage on perfect solutions without comming to any
conclusion.  Lots of time thinking about analsing it on a pair by
pair approach.

3) Gave up and watched TV.

Today

Time to attack again and my head is a little more clear on some of the
confusion from yesterday.

Ideas from bed.  Graph the force of the fall through problem and we
get a 1/x looking curve with 1/x^2 values (aka the sign flips as it
passes center.  Our current alrorightm connects the dots on this force
aka accelerationgraph.  If the line from dot to dot passes through the
orign correctly, then the area under the dot to dot curve on both sides
will be the same and the aprticle will slow down as much as it speed up.

But if not, the area is not the same, and we get a totally invalid
behavior.  The two particles be traveling too fast apart never sllow
down to the stop they should stop at.

The estimated curve is a bad match for the real curve because it fails to
"see" the sigulartiy(s) correctly.

But doing the pe vs ke energy correction, fixes that. Sort of.  If the
error wasn't so bad that in one step it exceeded what could be fixed.

Ok, so this line of thinking I belive will allow me to grasp the problem
of the fall though -- which also helps grasp the same problem whenver
the particles get too close together for whatever min DT we are using.

The DT adjust greatly reducees the odds that we get too close for a
given minDT, but it doesn't fix the problem.  A given DT will always
blow up if it gets too close.

One idea was to change the curve to flaten the singularity.  Aka, for
any distance less than x, the force is maxed out.  Does that maintain
energy conservation?  How does it change orbits?  It would mean tha PE
was no longer infinte but instead finite whcih would give us an actualy
total energy number.  But how much does it change behavior?  I'll have
to look into that further.

So I think this appraoch of looking at the force curve on the fall though
problem will help me grasp the weakness and find a fix. Thaat's good.
Understanding the probnlem is the first step.

Now. I can't remeber if I did Starbucks yesterday or not.  I think
I didn't.  But it was Friday.  Did I decide not to to be extra good?
Or did I get confused?  Let me check the app to see when I last was there.

Nope, I didn't.  Wed was the last visit.  I'll go today.  Maybe that's
a good approach anyway -- M W Sat instead of MW F?

But, Tague wasasking about our human nature and AI and rewards
for the econ debate so I'll answer that first on Facebook.

----

Just played a bit with code.  was going to do a quick hack for
force max.  Then remebered other problem -- the fall though now
hangs at the beginnign with dt of e-30 becuase of an energy error
issue that shouldn't be an issue.  So I need to look at that
and fix that as well. It's beause the KE is 0 at the start so
energy error relatve to total KE is large.  PE I think is dwarfing
KE and the change is distance is so small that PE doesn't change
due to it being below the 16 digits of perciston.  So PE looks
like zero chagne, but KE is a real change so I think I'm computing
energy error at 100% of KE when it's not really.

I think I need to careful computer enerrgy error differently
not by computing PE befre and after and subtracting, but by instead
using the distance before and after, and and computing the change
in PE.  Ok, put that on the todo list for today. Not not right
at the moment.

----

Ok, capping distance at 2 Angstrom didn't fix the bounce problem
in fall though.  I don't undrestand why.  But it happens at the
disconnect switch from +A to -A.  And it randomly falls through
or bounces.  EnergyFix is on, so I think this is a problem
caused by the energy fix and how the dt step falls in realtion
to the zero transission.

But coffee.  No, Tauge response to Facebook! first.  Then
coffee. Maybe I can just avoid startbucks?

----

[12:49 PM]

Ok, back from facebook.  Trying to understand why the fallthough does
a bounce.

Oh, I did skip starbucks.  So far today at least.

It must be the energy correction getting the sign wrong I guess.
But how?  Why?

I now have a 2A wide constant force field the particle is traveling
through near the proton but that does't fix it.

Ah, but wait, something is broken.  It's traveling at a constant velocity
though that section.  Ah, I fixed the calucation of PE but didn't fix
the actual force calcuatoin -- so the energy fix is constantly fixing
the KE to be make it constant.

Lets go fix the calcuation of force as well...

----

[1:32 PM]

Fixed the force to use fudged distance as well.  Now it bounces
back and forth perfectgly.  That's a joy to see.

But the code turned out to be mess of duplication and confusion.

And it's wrong.

The fixed codeis still showing constant velocity when inside the
protection circle.  I didn't want that. I wanted constant force.

My error is that I'm Calcualting constant PE as well, but it shouldn't
be constant.  That doesn't match the constant force.

Hum, let met trun off error fix and see what happens now....

----

Ok, turning fix off allowed velociety to change while iniside the
circle but i was confused becuaes dt adjust was changing the value
when it hit the circle makeing it slow down so I though it was
slowign down in the cicle. But it wasn't actually.

But without energyFix it accumlates errors quickly.

But, it bounces back and forth correctly all the time so far!

So, let me figure out what PE formula really should be if force
is fixed for inside the circile!


----

[7:44 PM]

got the RLimit working I think.  Fall though works just fine
with it.

But damn, it clearly fucks up the simlation for other cases.
Even for a very small RLimit of 0.01A (vs 2A that I was testing
with).

Well, that was progress, but maybe I'll have to rip it out and
do something different instead.

I guess the problem is that once the electron gets close enough
to hit the circle, the orbit becomes fucked and it pops out some
random direction it should not have.  Maybe the answer is that
I just need a really really small RLimit so it only hits it
for extreemly small values of DT?  I'll test later.  Or
tomorrow.

Food now.  Maybe Wendy's salad.

================================================================================

2016-08-28 4:26 PM Sunday

Yesterday

1) Got RLimit code working to cap force in a small circle around the
particles to cap the inifity problem.  Had to adjust force and PE
equations to match each other.  Sort of a mess how I coded it still.
But as long as DT can reduce enough, this allows the system to track the
force curve without making errors that the energy fix can't fix.  Which
means fall-though falls through every time now and doesn't bounce back.
But I have real issues with the accuracy of the simulation unresolved.

Today

Started playing earlyer without updating notes.

By using a very small RLimit (0.001 Ang)  the sytem works better with
fewer questionable simuaiton behaviors.  Basically, make it so small
that it's almost never used.

Still, however, I think when it is used, we got a problem. I guess I
could track the number of times the particles get that close together,
to track how questionable a simlation is.

But, started to work on display stuff today.  Changed screen size
to large.  Chnage bounce logic to make protons bounce futher inside
from the edge (20 pixels maybe -- one A) and electons bounce at 5 I
think. This keeps them away from the edge and allows the elctons to
nothave their orbits reduced to near zero just beause the proton it's
orbiting around does a bounce. Sort of bhelps with simlation speed By
keeping the elctons futher away from the proton.

Added code to start a random field of electons and protons.  Fun watching
all electrons run to the edges and get pinneed but keep all evenly
spaced out!

Added code to put center of mass in the middle of the screen at start.

Changed screen coordinate mapping to put zero in the upper left now that
we have auto center code.  Makes code simpler.

================================================================================

2016-08-30 1:47 PM Tuesday

Yesterday (two Days Ago really)

1) Work on displaly code. Made larger screen. Added inset from edge
for bounce so particles were easier to see when stuck on the edge.
Made protons bounce frame smaller so they could boucne without cauing
an orbiting electon to suck it's orbit down to zero at the same time.

2) Wrote new startup that created N random protons and N random electons
to test different combinations easilly.

3) Added code to place center of mass in center of screen.  In addition
to what it was already doing of setting system momentum to zero.

4) Changed screen coordinate to make x=0 y=0 uppler left instead of
how I had it in sort of the middle.

Today

Just fixed the global Bounce warning message comming from Python that
has been hanging around for days. Seems it was the second Global used
at the energy check that was cusing the error beuase it was not inside
a class and was in the same scope as Where Bounce as declared at the
top of the file.  I thought it was the "global Bounce" in the class
in the dsiplay code.  I thought that was the one getting the error by
apparenetly it was not.  So the second global Bounce as just not needed
and I deleted it and all is working fine and the error message is gone.
Phyton name scoping is just odd and hard to follow at times.

--

Yesterday, did a bike ride out to the BBQ place on the W&OD (30 mile round
trip).  Though it was cooler but it was 95.  Really hot.  Did fine but had
bad legs cramps after getting home at night from being too dehydrated.
Haven't been riding this summer because the damn temps have been up in
the 90's most the summer and it's just too hot to make biking fun. But
temps seem to be improving this week so maybe I'll do some more.  I need
the exercuse.  Both for weight loss and to help the strees I'm coping
with due to running out of money and having a very uncertain future.
didn't work on physics due to bike ride.

Also, the damn bike rack frame bent down again on the ride due to the
weight of battereis and water bottle banging on frame when I hit bumps
and bent enough to start rubbing the tire. But didn't happen until I
was back in Vienna so it wasn't a big problem.  I really need to build
byself a new frame.  Or something to prevent this from being a problem.

--

Idea I had about Physics. I need to zoom in the display to see a different
scale and see what happens when I use electons in tigher smaller obrits
around the protons.  Does the behavior of the system stay the same when
we re-scale like that?  Just smaller orbits, fixed by zooming in?  So we
get higher speeds but the same relative behavior?  Or does the behavior
actually change?  Since I read that a hydrogen atom as an outer shell of
about 1 A in diameter, it means a netron must have a far smaller orbit
if there is any hope of it being an e p pair in truth.  So maybe if I
chagne the scale and look more at what such a tight orbit does, I will
see new behvior due to the force of attraction being so much stronger
at the smaller scales?  So, I need to be able to zoom the view  of
the window. Currently it's hard coded as 20 pixels per Angstom.

When the orbits get tight in this view, all I can see is the electron
on top of the proton, and the simulation slows down to a point I can't
see anything moving.  So zooming will allow me to watch the lower level
interaction.

Of coruse, even nicer will be a large simulation that allows me to zoom
in on one part of the screen, then zoom back out -- or zoom in on
one particle and track it.  Maybe zoom in and track the center of
mass of all the particles that were in the first zoomed in screen
or something?

--

Relativity.  What the fuck is it?  I really need to learn physics more
to get a good grasp on what the experimentental evidnce for relativeity
is.  I undertand that Einstein did no experments to come uip with the ideas
but instead just looked at the formulas that experimental work on
electomagnatism had crated -- maxwells formuals and the like.  As I
understand it, he conclued that if the univese worked according to
those formulas, then relativeiy had to be true. But I don't really
understnd what the formauils were, where they came from, and how the
conclusion was reached from the formulas.

But the results, is that speed of light is a fundamental limit.  That light
has a speed limit.  That gravity bends light.  That time slows down
when we accelerate.  And mass increases?

Ok, so reading some, I see that the key to relativity is the idea
that mass and energy are interchanable --- aka e=mc^2.  And there's a notion
of the rest mass -- as measured when the observer is moving with the
mass. Or where the mass of the system is measured from the  center of
mass of momentum of the system.

But in my simlaiton, only velocity and position are being traded for energy.
There is no transform that allows mass to be traded for energy.  Mas
is fixes. And total enegy is defined by e=mc^2 in relativity.

KE = 1/2 mv^2.  Why is e=mc^2 and not 1/2 mc^2?

Oh, wait.  If two partics are moving away from each other at a relative
speed which is equal the speed of light. Then their absolute speeds
might be 1/2 the speed of light?  So the total kenetic energy is 2 *
1/2 m (c/2)**2?  Or 2 * 1/2 mc^2/4 Or mc^2/4.  Not mc^2.  Ugh that cuts
total energy in half not double it.

Ok wait.  How does energy change when the frame of reference for
measuring velociy change?  If I measure from the center of two particles
flying away at a relatve velocyh ot c, then we get that mc^2/4 number.
But if the frame of reference is taken from one of the particles so
it's velocity is zero, and the other is c, then the energy is 1/2m^c2.
Which is twice the system energy then the other frame of reference.

So, fuck, the total energy is not independent of the frame of reference.
Is this really the simple foundation of relativity?

Ok, but even if the total energy changes, doesn't relative energy changes
over time remain the same no matter what the frame of refernce is?

Ok, so if we take the frame of reference to be one particle, then look
at what's happening!  The observer is moving relative ot the center of
mass of the two paticle system.  Is that not hidden energy we are not
measuring that exists in effect IN the OBSERVER (so to say)?

So as a system, we can look at the nergy as a system of mass 2m moving
away from us at 1/2 c.  So energy using 1/2mc^2 is 1/2 2m (c/2)^2.
Or 1/4 m c^2

One particle moving away at speed of light is 1/2mc^2.  But two paritcles
moving away from the center if we are at the center, at 1/2c each, is 2 *
1/2m(c/2)^2 which is also 1/4 mc^2.

Ok, so the energy of a system is constant, if veloicy is measured relative
to the center of momentum (mass?) of the system.  Probably center of
momumtum.

But energy is really a made up measure.  What we have is just a system
of relative psotion and relative velocites in my netownian simluation
that is totally semetical.  It needs no defined "center" or "frame of
reference" by which to operate. It only needs reltiave locations of
particles and relative speeds.

So I have speculated that relativity effects are just an emergent
property of this simpler newtonian model.  And my thinking was that the
speed of light is really just a limitation of how fast kenetic energy
can be trasfered from one particle to another.   And though that seems
to be a possible fall out from the simple netown model due to how the
time dely effects of how force (potential energy) can chnage velocity
(kenetic energy), how do we explain the other effects of relative such
as the notion of time itself slowing down.  Which isn't just arbitray
nonense -- it's been measured and confirmed that atomic clocks slow down
when you put them in in a plane and fly around the world.

So, this is all pointing to the idea that all particles interact with
each other in the univese, but that the center of momentum of all those
particles establish an effective reference frame from the transfer
of energy from particle to particle.  Beuase when a small subsystem
of particles moves relative to all other, we can simple look at that as
a movement relative ot the center of momenutm of all the particles.

And if we take that sub-system and add kenetic energy to, we must have
removed energy from somewhere else in the univese to make that happen.
We must hae either taken kenetic energy from other particles, or changed
the relative potential enegy of the system.  Like particles move further
away, dislike move close.

So, does it somehow work out that when we accelerate an atomic clock,
we end up having to remove enrgy from it? And in so doing, slow it
down at the atomic level?  Reduce the size and speed of obrits and the like
in all the atoms in the clock?  But if we take the ke out of an obiting
electon, doesn't that speed up the orbit even if reduces the size of
the orbit? So if all the atoms of the clock saw KE removed, so that
they could all have higher KE reltave to the center of the univese,
all the atomic strcuture would shirnk, but wouldn't it speed up?

How can the an electon speed up, if we are removing KE?  Ah, what really
happens is that we remove PE?  So to make the clock as a system move
faster, PE is removed from the clock?

Ok, attack from a different direcftion.  If we hae a field of e and
p mixed and balanced, then in a system as large as an atomic clock we
have lots of particles so the average distance from any one particle to
ever other, will be approxiamtly equal -- no net PE? Whatever attraction
there is to all the e particles, will be balanced by the attraction to
all the p particles. Ah, but if I'm an e, then there there will be one
more p in the p set than there are total e to be repeled to!  So the
average attraction of all the other p wll be slightly higher than the
average repelling force of all the N-1 e particles.  So the movements
of this one p relative to the center of mass, is likely to repreasent
a net potential energy source!  So if the total system compacts, the
PE is reduced!  Even if the system has a balanced number of e and p!
Which forces the total KE to increase?

Or inversely, if part of the system sees an incrase in KE, either that
KE is lost from elsewhere, or there must be compaction in the system!

And, maybe even a sub-system like the clock, could compact relative to
each other to create that KE?

ORRRR, what if half the energy came out of KE to slow the clock down
and half came from PE to make it smaller? What if there was a natraul
tendency of the system to keep the PE and KE in some balance that forces
the lost energ to come from both?

This doesn't seem consistent with the idea of a simple orbit which seem to
require an increase in KE with a decress in orbit size. But more complex
orbits might be very different in this regard?  The complex orbit of an
atom might shirnk and get slower at the same time?

Or, the orbit of a netron might shirnk and get faster while the orbits
of electons shink and get slower or something odd like that?  So the
clock seems to slow down beause we arne't seeing the increase of KE in
the nutron orbits?

--

Ah, I just found an intersting idea in the wikepedia under Controversy
in the Mass is special relativity article (of how to tach relativity).

The argument seems to be that relative mass is a bougus soncpet, and that
the correct approach to explaning/deriving the concepts of relatiivty
is through the "conservation of monumtum" in all all frames.

Ok, so I think that's the key to understanding relativy.  It's what
falls out of the conservartion of momentum in all frames.

So I guess what we could be looking at, is that a newtoniam simulation
as I hae coded here, applied to the entire univese, then, if we defeine
a frame of reference in a different momentum frame, that it would also
see conservation of monumum in that different frame.

That seems like it could fall out simply from the idea that the entire
univdse hs conservation of monentum (which is clearn that the simulation
I've coded does), that if we pick a different frame of monumumt meaning a
moving frame of reference, that the the monumum would still be constant,
but offset by the monumum of our frame of reference.

And if our frame was accelearting (monentum changing) then the system we
observe, would not have constant momentum, but would instead, be chanaging
in monumum to offset the change of monumum of our frame of reference.

But what if the entire point of relativity was that even when we look
at a system under an accelearting frame of reference, conservation of
monentum as observed is still perserved?

But, of course, there's a huge question here.  The only type of observer
in this sytem, is made up of the same particles as the system we are
tring to measure.  So what relativity I wonder might reall be telling us,
is something about the innate problem of observation of a system from
within the same system.

So, could conservation of monument in any frame of reference, be
explained as a measurment effecvt?  That anythoing you can measuire,
must show conservation of monumtum relative to the measurement system?

This could all be true due to the fact hat all physics experments are
forced to work under this limitation -- all real world measurments are
done inside the system. So relativity concetps, derived from real world
experments, could all show the same global liminations of what measurments
can show.  Is this idea even logical? I don't know.  But it could be a
genral porblem of physics not being able to separate measurement limits
from their models.

----

[4:31 PM]

Just went out and sprayed week killer on lots of weeds in our beds.
Turned on some water to help water the grass.

Pig wants to do a movie at Bow Tie in Reason tonight.

So in physics, how fast does kenetic energy move through the system?

--

And, if we just accept the "truth" of relativity, it means the mass of
the particles increase with speed -- which would easy enough to code
into the simuation would it not?  If we just coded that into the system,
is that all that is needed?  Or are other distrotions to distance and
time needed as well?

================================================================================

2016-08-31 10:26 AM Wednesday

Yesterday

1) Finally figured out and fixed the startup warning messgae about
"global Bounce" being declared after it's set.  Was simple really.
I was just confused about which global it was talking about it seems.

2) Did some thinking about relativity and a little reading and learning.

Today

Think it's time for a quick zoom experment that I didn't get done
yesterday...

----

[12:38 PM]

Clean up display code.  Add pixelsPerAngstrom var to paramatize zoom.

Added functions to map space dimensions to screen pixels and back.

Made protons and electrons both bigger in display.

Expanded size of random starting cluster.

Wrote generic code to display distances between all protons in world[]
instead of the hard coded test using P1 P2 and P3.

Now looking closer at the two proton one electon behavior of how the
elctron likes to suck the two protons together and hold them.

I've seen a tendency for the two protons to suck together, but when I
leave a long run, I often find the protons have flown apart and are not
being sucked back together.  When they get too far apart the ability
to suck them together is too weak even if it's there.  Also when too
far apart, the electon doesn't see as likely to orbit towards the
other proton.

But I think I'm gaining some insight.  When they are too far apart they
get sucked together.  But then the monentum pushes them too close, and
that causes more of a figure 8 orbit where the el3ecton spends a lot of
time outside the pair, so the protons push each other apart.

So the protons end up oscillating back and forth.

But, there's a distinct change in the electon orbit.  It can be in a
arched figure 8, where the outside orbit of electon is very tight and
short lived so it spends a lot of time between the two protons sucking
them together. Or it can swtich to a winder figure 8, where it spends
more time outside allowing the two to push apart.

This switching of orbit styles I think is somewhat random and adds noise
into the proton ossilation.  I think if the protons start off too far
apart, the ossolation is near the escape velocity, and this random
noise of the orbit style, can trigger it to swing to far inward and
then fly outward too fast and too far allowing them to escape the hold
of the electon.

But I think maybe if they start off with the right distance relative
to the energy of the system, they don't ossilate as much and tend to
hold together.  I also don't know how much of all this is simlation
error vs real justified behavior.  Just saying.

But the idea, is that a given amount of energy, will define the center
point of the proton oscilation and we start the system too far away
from that it's random ossiclation could cause the sytem to fly apart in
time. But if we start it near the blance point, it should hold together
far longer?

Formal testing of this could be done.

Oh, the test I had running that seemed more stable seems to be flying
apart now.  What seems to happen, is that the protons get too much kenetic
energy in them flying part, which at the same time steals connectic energy
from the electon shorting it's orbit and "reach" to hold the two together.
So they fly off too fast while the electon looses power and it can't pull
them back.  Ah, but wait, they got out to a distance of 7A and now they
are comming back together. But I fear this oscilation pattern might be
amplifuing out of control and it will fly apart in a few more cycles?

All this was just getting ready to zoom and see if the behavior looks
the same when closer or if there is behavior change when we zoom.

----

Ok, started to play with zoom.  Saw really really odd behavior at first
like the second particle was being ignored and had no effect on the orbit
of the elecotn.  Then relaized I had added some Z offsets that I couldn't
see on the screen. The two particles were in fact so far apart in the
Z dimension that it didn't have any effect!  So I got rid of the Z offset.

Now I have a test running with a .003 A spacing between the protons.

And now, it's looking like the behavior is exacatly the same at the small
scale, as at the large -- except all the velocites are much faster and
the simluation has to run at a smaller DT.

And I had to reduce the RLimit to make it smaller to keep it from messing
up the orbits.

NO, wait, I'm seeing something different.  The protons seem to be getting
closer together and I saw a few orbits where the electon was able to do
a loop around both of them!

I'm thinking inertia has more effect when zoomed in -- as if the mass
has been increased relative to the zoomed out version.

Behavior is very similar, but orbits of the electon seem more rounded
and the speed of the oscilation of the protons is slower.

And now the protons are flying apart after they can very close together
and I think they have reached esscape velocity.

----

[2:46 PM]

Just scheduled bike ride with Dave.  Point of rocks to Sheaperstown. For
Saturday. (labor day weekend!)

Playing with 10p10e run.  With z added as well. But no z bounce.
Just thought I'd let it run for a while.  Since I can't "see" the z
dimension it's hard to grasp what is happening.  Just wonder if it might
converge into something intersting given a long enough time.

But I can't tell if some particles are flying off to nowwhere in the z
dimension taking energy with them.

Would also be intersting do to unbounded tests to see if the cluster
stays together or keeps expanding or what.

ideas for better UI would include showing close particles as larger
circles and far away as smaller to get a 3D view of distance.  Having
roate and spin options would be good.  And maybe a wire frame cube to
mark orrientation.  And zoom and shift.


BTW, added RlimitCount and BounceCount to display so I know if the
simulation has been distorted by these effects for long running
simulations I haven't watched.

Oh, other UI -- a record and play back in real time would be fun.
Or, just a number cruch run with no display followed by a display.
Or pause play with number crunching and recording always running
in background.

----

[7:31 PM]

Went to home depot and got spring and tube for possible use on rebuilding
bike rack before Saturday ride.

Went to nodles and co with pig.  Then back to merifield post office to
drop off package from Pig to Julie.

Back home playing with hose to water side yard.

--

Added size chagnes in display based on z axis.  Sort display by z to
draw front to back.  Still doesn't look very 3d however.

Need to look into mouse control of rotation.

Could add persepctive reduce to back as well.  With size change matching
perspective.

Not seeing any tendecy of particles to self organize into atoms.  May be
very sensitive to total energy vs density in the sytem?  Might of course
be becaue this model has no hope of doing that anyhow!

================================================================================

2016-09-01 2:06 PM Thursday

Yesterday

1) Lots of little stuff to display.  Added size shift for z axis.
Cleaned up code. Made more things paramatized. sort by z to draw back
to front.

2) Added zoom and took a look at behavior when we zoom in and then adjust
for simulation speed.  Mostly, it looks almost the same.  But yet I
think I'm seeing a difference -- inertia seems to play a more important
role so objects tend to keep going straight more?  More circular orbits
instead of flat at this scale?  Don't know if this is true or not.
I should look at the math and try to see what it suggests.

Today

Need to fix rack for bike ride Saturday. Could go to meeting tonight
with the group I've never been too -- humanists or something -- but
they are debating the book that suggests mind uploads could domanate
the near future.  Need to get web site for BGOP set up on new service
as well.  Not much time to really work on physics today but I'm playing
with it anyway.

----

Made massive changes in scale from:

pixelsPerAngstrom = 0.00001

pixelsPerAngstrom = 100000.0

And I'm not seeing any general difference  in the behavior.  The random
start recales the distance between particles to match the zoom, and the
dt adjust recales the time step, and in the end, it all looks the same
on the scale.

So one is running at e+7 velocity while the other is e0 to e2 range.
But on the screen, the rescalled sizes all run at the same general
visual speed.

So without checking the math and thinking carefully, these particles
are scale invariant.  Relative to each other, they can expand or shirnk
in there would be no aparent difference.  But not energy invariant!
But energy is measure din the units I choose so that's true and just
really a choice of units at the different scales.

--

So thought, I'm looking at 2 electon and 2 proton and they seem to like
to go into one tight orbit and one electon bouncing all over the screen.

So, this orbit behavior has a lot to do with the random starting
psotion and the total energy the posotion represents.  So it seems to
me it would be useful to chagne the random code to allow me to set the
starting energy in the system.  So each random starting psotion could
hvae the same starting energy to compare how it evolves.  And so I can
create systems that are slow energy enough that they don't send electrons
bouncing all over the walls as well!

----

Maybe, create random protons. Then create random electons all a fixed
distance away from each matching proton as a way to establish the
starting energy.  Won't make it exactly the same, but puts it in the
ball park I'm thinking.  And easy to code maybe.

Or, maybe just a kenetic energy fudge for all particles is easier?

================================================================================

2016-09-01 10:40 AM Friday

Yesterday

1) Did massive change in scale to compare very small with very large.
End result is that the behavior of particles didn't change at all.

Today

Did no work on bike yesterday.  Must do it today to get ready for long
bike trip tomorrow.  As well as pack and load up things and charge
batteries.  Also would like to get web site done before going to BGOP
meeting tonight.  And need to remember to bring $50

Went to DCT meeting at Pentagon City last night.  Took silver line train
to Blue line.  Went fine.  Parked illegally at Tyson's.  Then it was
raining comming home got somewhat soaked running in rain.  Meeting was
fun being around nerds talking nerdy stuff.  5 of us total.  The other
4 for regulars of the meetings as far as I know.  They only get together
once very 3 months.

---

On scale tests...

Once the system speeds up and slows down time to maintain same relative
error in simulation, the behavior seems identical.  Which implies
for these rules of simlation the system is totally scale invarant.
Smaller systems run faster, but act just the same as larger systems in
the same relative configuation.  So when the particles spread apart,
they don't act any different they just act the same way and slow down.

Relative path is identical at large scale and small.

So I can study the system at any scale to learn it's behavior.  And,
oddly time and force and distance are relative, mass is relative, but
there is no absolute at work in these equatons.

So the path an article takes is depent on the relative path and speed
of all other particles, but if you scale distance up you can scale speed
down and the path remains identical.

But in my tests, I didn't change mass.

If we change relative mass the behavior most certainly changes --
like making both particles the same mass instead of the 2000:1 ratio.
But what if we just change the mass by multiplying each by 10000?
Does the behavior stay the same but the speed just changes?

----

Ok, tried using mass *= 1000 for both and mass /= 1000 for both with
random starts of 2p and 2e and again, I couldn't notice anything that
looked like different behavior.  The larger mass made them run slower, but
once rescalled for time and speed the behavior still all looked the same.

So with these simple newtonian equaltions and only two particles, the
only equations are f=ma and f=1/d^2  And the ratio of the two masses.
So if this is the foundation of the univese, there is only one constant
which is the mass ratio, and these two equations (in 3D space).  All other
constnants are just arbitrafy unit conversions.

Lets try to verify that by making the mass of the electon 1, and
removing making the KE (Columns constant) 1.  And just mass of proton
to be relative ot mass of Electon.

----

Added FakeConstants = True flag to change all constants to 1.0 except the
mass of the protron which is 1836.15267376 to maintain the ratio to the
mass of electon at 1.0.  BVehavior seems the same except the simulation
starts up moving faster. I think this is just due to the fact that my
Random start sets velocity to 100*random() and does't scale for units.
Behavior seems identical otherwise.

So, units and constants are not important other than the mass ratio.
Behavior is otherwise all the same just scalled to different units.
So the only thing that's important for the starting configuation is
density vs velocity I guess?  So whatever constants I use define what
the distance vs velocity or distance vs montumtum ratio is?

Real units are of course important when trying to match the real universe
but othersise, seem unimportant in understanding how these systems of
particles behave.

High speed forces smaller orbits.  Which is odd.  So higher starting
speeds should lock into smaller orbits?  eh?  Confusing.  So a proton will
capture an electon if it gets close enough. But the higher the speed of
the electon, the close it needs to pass before it will grab it I think.
So with random staring higher speeds will force it to fly around longer
before a proton is able to grap the high speed electon?  But in the end,
it will?

And very dense but low speeds means they all lock into orbits isntantly
but then form very complex behaviors?

I'm trying to grasp if we will see very different behaviors at different
densities and speeds.  The sun is very different than the rest of the
system it seems.  Black holes should be odd if graviy theory is correct.
But if gravity can be shown to be an emergent property of atomic orbatal
dynamics. (AOD).  This sort of invarant behavior of these equations make
me wonder the different really is?  How can we have a hot plasma where
nothing is in orbit and steady?  Won't all the high speed particles end
up locking into tight small orbits? then the pair holds that energy in
a tight space -- taking it out of the system?

Well, I feel I need to test with some very high starting velocities and
see what by room does -- with no energy loss, do all the electons end
up pairing with protons in tight orbits?

Then we have the KE of the set vs the KE of the individuals?  Ok
this is a direction to exllore.

--

UI thought... draw a perspetive box with the window being the front face
and map the particles to that perspective.  Draw shadows on the floor,
or maybe walls to help create the undestanding of 3D motion!

----

Ok, playing with speed is odd.  Setting to zero breaks the simulation due
to the auto adjust of dt using KE error to set DT.  It forces dt to be
soo small the particles never move.  Or maybe if I wasted a few days they
would?  Setting it to 10 gets things working, but is about the same as
before with it set to 100 (well random*100 which os 0 to 100).  Setting
to ta large number like 1000000 or somnething breaks the simlation as
well.  The particles move far off the screen in one jump and the "bounce"
code moves them back on instead of calculationg a valid bounce.  Particles
end up being suck in the corners flashing.  Let me see how fast I can
set it without breaking the simulation.

--

Also have noticed that my ball size code is wrong.  Balls that aren't
moving are changing size as other arpticles move.  I think I'm using
min and max z to scale vs true min and max room size boundties to scale.
Gota fix that.

---

Leting 2p2e run.  One in tight orbit, the other electon keeps flying
around and bouncing off the walls.  Wondering if the second proton will
cpature it at some oint.

But now I realize I probably had an error in my thinking.  When it's far
away from the protron, it has a speed, that if it were close enough,
that speed would lock it into a tight orbit. But as it appraoches,
the speed keeps increasong.  I think we have a situation where the Ke
is just too great to allow it to lock into any orbit at all. As it gets
close enough to orbit, the speed keeps increasing to the point that it
won't ever orbit.

Too much KE prevents it from ever orbting.  Even having the system
spread out won't change that.

Oh, but wait, if the e and p were moving together, in the same direction
their relative speed would allow them to lock into orbit.  But the system
as a whole would still have the KE in it.  So they could bounce around
as an energetic pair of particles maybe.

But in my current test, the protons seem to stand still mostly and the
electons fly around.  It even feels as if the electons are taking ke
out of the protons in their interactions.

I think I need to look at the KE vs density (PE) vs speed closer to see
how things tend to shift.  Adding the averget KE tracking per particle
seems useful still.

Doing tests where the elecotns have all the monumtum and prtons start
standing still and see how things change would be useful.  And likewise
the inverse.

Being able to randomly start with a fixed KE or fixed total energy is
likely to be useful as well.

I need to better understand this basic sort of stuff about how these
interact. fixing the emulation to llow a zero ke start and where high
speeds don't break the emulation seem important as well.

================================================================================
================================================================================
================================================================================

2016-09-17 9:39 AM Saturday

Yesterday

1) No real work in a few weeks

Today

Who knows.

Life

I was distracted by the bike ride Sat.  I hung out at the forge I think
Sunday. Then I came down with a cold that wipped me out for a week.
When I got better, I got the grass cut.  Replaced the blade on the edger
and edged the yard for the first time in a while as well.  Been doing
watering -- Ma's tree and the birch are suffering.  Leaves dying.
I think this was caused by the high heats we had this summer and the
fact I didn't pay attenten and water the yard -- it got really dry.  up.

It's been a week almost since I basical got better from the cold, but
I'm slow to get back to work.  Lots of time wasted on facebook.

Had a common core debate on facebook.  These closed minded conservative
types really can't even grasp something like 2nd grade common core math.
The question example was "how to make 10" on 8+5.  They think it's asking
how to get 10 by adding 8+5 which is stupid.  But it's asking how to
do the re-grouping to get the answer.  But not only do they fail to
grasp what it's asking when they frist see it, even after I take time
to write long explanations about what they are teaching and asking,
they still CAN NOT UNDERSTAND WHAT I"M TALKING ABOUT!

SO fucking frustrating to have to debate stupid people on facebook.

So fucking frustrating to know that over half the population are stupid
in this way.  The Trump Clinton crap is on going.  Trump has been ganining
in the polls and Clionton slipping.  Down to a 1.5 point lead for Clinton.
OMG -- I live in a nation of idiots.

Been watching a lot of TV at night on netflex.  Watching Angel
lately. ABout half way though the series.  FInished all of Buffy a few
weeks back.  Watching Person of Interst at the same time but slowly.

Been working on losing weight.  Stopped my daily runs to Starbucks
for the first time in like 20 years.  Figured all that milk wasn't
helping my high liver fat problem or my total weight.  Been drinking
coffee from the Keurig instead.  Trying to cut back on caffeine as
well.

Been up just above 230 for a while which is as fat as I've been in two
years -- since before sarting Ingress.  Was down to as low as 215 avg.
212.3 was the lowest recorded last November after a fall of lots of bike
riding on my Sondors.

In the past coupld of weeks I've crashed down to a low of 222.9 as of
this morning.  Something like 7 lbs in 2 weeks.  Might be dangerous to
lose too much too fast, but I'm going to try and keep my food intake low
and see if I can get below 210 soon.  I need more exercise.  Yesterday,
I never went out of the house -- pig brought be home a quater pointer
and that's all I had to eat yesterday exacept the coffee and other such
snacks. The step count was like 140 this morning.  Ha ha.  Only 140
steps all day!  Not the 10K they want you to get!  But that is of course
lower than real beause I don't always carry my phone with with me to
get the steps.  But still, was under 500 for real I'm sure.

Oh, moved the BGOP web site from iPage hosting to godaddy to save a few
hundred dollars.  $4 a month for 3 year intro rate vs $11 renewal rate
on iPage.  Having that on my todo list was also distracting me from
working on physics or AI.

Life continues to be very depressing.  Lonley.  Now that newsreader is
shut down and I'm not working at Mount Vernon and I'm backing off from
participation in the Guild, my life is mostly just empty.  I just hang
out at home and I'm not motivated to go out.  I have trouble sleeping
due to massive fears of what's going to happen when the money runs out.

I wake up in a panic -- sometimes just a dream that ha snothing to do
with reality -- sometimes panic about real life.

I'm desparate for validation and attention, and I bassically get none
in life any more. I try to post clever things to facebook to get
attention, but most the time, people are just quick to call me an
idiot.

I've solved the hard problem of conscioness.  A fucking 2000 year old
problem and I've solved it.  I've seen nobody else share this answer.  I'm
the only one in the world that seems to actually understand the answer.
It's a simple illusion crated by the fact we can't sense our private
toughts with our external sensors.  But yet, no matter how many times I
post about this on the internet, no one seems to really undrstand it.
Some people understand that the answer is it's an illuison. But they
don't undrstand how this fully explains that this IS the illusion.

I've spent my life, trying to do something great to get attention,
and now that I've solved one of the great mysteries of moderen times,
no body undrstands the answer.  What a fucking torture.

And of course, I sovled AI as well.  Simply by understanding that skinner
had solved it 50 years ago -- and nobody understood his answer.  Poor guy,
he died knowing he had solved the mystery of human behavior but that the
world was too stupid to understand his greatness.  He had the concepts
correct, but because he never built the answer, they didn't undrestand
he was correct.

I'm very close to having a working solution to AI.  I have to get back
to that and try to finish it before I run out of money.  Temporal, Deep,
Reinforment learning.  Where every node has temporal memory.  That's the
full answer to Ai, but yet, most the world is too stupid to know it's
the answer.  Fucking idiots.

I dreamed as a very young kid of being that guru on top of a mountain that
eeryone would come to for wisdom.  I would be the person that knew all the
answers. Or at least, was able to help people undersand things they didn't
understand.  I dreemed of being famous -- the person everhone knew about.

Guru's are known for spirtial enlightement.  Which is not my thing.
Maybe it should be.  But I'm an engineer and scientits type, not a mystic.
My idea has always been to get famous by creating something great.
Thomas Edison was my hero -- I remmebr reading a book about him in about
the 7th grade I think.  That is who I wanted to be.  Invent the light
bulb and change the world forwever.  Go down in history for changing
the world.

Or the Write Brothers. With flight.

Or Steve Jobs.  Or Dennis Ritchie Ken Thompson.

Or Bill Gates.

Or Mark Zucckerberg.

Or Alexander Graham Bell

But I'm not a strong leader. Not a leader at all.  So I really don't
have what it takes to be a Steve Jobs or Bill Gates or Mark Zucckerberg.
Because all of them are not just smart, and creative, but strong leaders
that push others into implementing their dreams.

Even Thomas Edison was that way -- pushing the likes of Nickola Tesla
to implement his dreams. But he only had small sets of reserach assitants.

because I don't have the drive to make people work for me.  My strength
is in my strong (ENTP) Percpetion skills that gives me a very open and
flexible mind.

So, I'm really more cut out to be the type that creates something
great like Ken Thompson, or Bell, or the Write Brothers.  I'm not good
at following through to implment my ideas -- but that lack of drive to
stick to a single path is where my strugth to find andwers that others
never find comes from -- I have the ability to endlessly explore in
different directions -- never liking to stay in a single path too long.

This is where teaming up with the rigth person would pay off -- teaming
up with someone that has the drive to stay on a path and finish a dream
vs just finding it.  But I've struggled to find anyone to team up with.

A lot of my posting on the internet about AI for the past 10 or 15
years has all been about trying to find someone to team up with --
and all I've found is endless idiots telling me I'm a fool instead.

But I'm no fool.  Even if I'm the only one that knows it.

[Don Cooper just called -- didn't really want to answer but felt rude
not do.  He wants to borrow my pressure washer.  Ugh.  Fine, he's comming
over later. He's about to put his house (next to ours) on the market.
I'll have to dig it out, and get it working and show him how to use it...]

[10:48 AM] -- timestamp

So in all my debates about AI, I've solved the hard problem of
consciousness as a side project.  But no one understands the answers
so I get no credit for this.  A few seem to accept the answer, but no
one has replied in a way that I feel they really got it.  It was mostly
just a polite, "ok, I hear you, I guess" sort of reply.  Most just tell
me I'm wrong, or I don't understand what consciness is.  To me, when I
relized it, it was a massive eye opening epipthany.  No one I've shared
the idea with seems able to connect like that.

It was an epipthany to me not just because it made sense logically,
but beause I could SENSE the ILLUSION IN ME.  And once I relaised the
illusion existed in me, I was able to fix it (though it took weeks or
months to unlearn the error).  No one I've shared the idea seems to be
able to sense the illusion at work in them.  Even tbhough, based on how
they talk, they clearly are under its spell.

So, I solve a 20000 year old puzzle, and no one understands I've solved
it.  Depressing.  Frusting. Creates great rage in me when people call
me a fool, becuase thye are too stupid to recognize my genius. And there
is nothing I can do about it.

And despite my genius, simple things like spelling I can't get right
to save my ass.  Becuaes I have no ablity to care enough about how a
word is spelled, to take the time to memorize stupid rules like that.
So this lack of attention to detail that gives me my cognative strength,
also makes me look weak minded and stupid to those that do memorize how
to spell words. Which makes it harder for them to accept that maybe in
this weak minded bable, there is a genus idea lerking.

In addition to solveing the hard problem of consciousness, I've solved
the problems of understanding ethics.  Ethics and morals all derive from
goal driven systems and the ethic of a goal dirven system ot to reach the
goal. Any behavior that hleps get closer to the goal, is a good ethic,
and any that pushs away from the goal is a bad ethic.

All ethics are relative to the goal system we refer to.

And in humans, there are two major distanct goals that drive our behavior
-- which is why ehtics is not absolute, but relative in humans.

But moreal relativity that implies a culture can have ANY moreals and
be correct, is not true. It can only have morals realtive to a goal
driven system.  And the two goal dirren systems that have major contorl
over human beahvior is first our brain, which is driven by rewards.
So seeking pleasure and escaping pain is the goal of our inteligent
ethics. But evolution has the goal of surival, which is what shaped our
brain and it's goal.  And though the process of evolution of culture,
evoution fills our brain with surival ethics.

It's this conflict between the ethical goals of intelience -- which is
pain avoiing, and the ethical goal of evolution, that our culture becomes
filled with, that creates the moral and ethical conflict in our society.
It is what creates the conservative liberal split in society.

So, I've solved the mystery of AI, and the mystery of ethics, And the
mystery of why we have a consevative/liberal split in human society.
But yet, when I share this wth people it goes right over their head,
because the way they have bene taught to talk about these things, don't
leave room for my words that acurutally explain the truth. They can't
understand my words, because th words they have been taught, are too
different. They can't find a way to fit them togehter.

Likewise, I fully understand that phsyicalism is the only correct
philospohical descirpiton of reality.  But 99% of philosophers seem unable
to understand or accept this beause their undrestanding of reality is
based on (false) words, not actions and behavior.

The illusion that creates the cofusion of conciness, I call the illusion
of duality. And I now understand now deeply this misconcpetion is woven
into the fabric of our language and cutlure.  It is what allows people
to accept the logically abusrd idea of a god.  If our body comes with a
non-hsyhical "mind" why can't the unviese (as a body) also come with a
non-psyical super-mind?  Aka GOD -- it just "fits together cleanly. But
wrong.

And of course, mind uploading is all baesed on the error -- the believe
that the mind is seprate and can be "moved" is all a false idea that
comes form the false seperation.

The idea of information being a "thing" in the unviese, when it's not.
The idea that information is separate from the phsyical is the same
misake of duality.

And the idea that econmics is all a big AI.  And that the way to fix
econmics is to create a Basic Income to fix the reward function to
optimize th fit of this AI to the needs of INTELIGENT humans (vs law
of the jungle surivalist humans).  An idea I don't seem to see well
understood or accepted that I figured out.

I think the world can't accept this, beaues they don't undrestand
that Skinner was righ and that inteligence is nothing but a simple RL
algoirthm.  And I can't make  progress getting though to these idiots
until I build a working human level AI and show them how stupid they
have been. So that's on my short todo list.

Then there's all this cool phsics stuff I'm hot on the path of.

I've shared with Tauge and Chappell, and a few places on th einternet some
of the ideas, and Taugue just seems to think I'm a fool.  But Chappel
seems a bit more open minded to the idea that emergent behavior could
be intersting.

But what is already OBVIOUS to me, is that wave particle duality of
quntum mechanics is all an illsion -- an emergent property of how the
atoms work, not a fundamental propperty of nature.  So, photons are bull
shit. They aren't particles. They are just an emerent profty of the fact
that energy gets INSTENTAIOUSLY trasfered by the electrostic field,
though large systems of particles but results in quantum sized jumps,
when enough energy has been transferfed to cause an electon in orbit, to
be kicked out of orbit -- whcih uires a precise amount of escape velocity
enety to kick it out -- and that is what is falsely seen as a "photon".

So the answer to understanding how to reduce all of phsyics, will be
found by understanding how fundamental praticles work in the atom.

And what's 99% likely, is that neetruons are not paritlcs but just
an electon and proton coupled in an tight orbit.  Making electons and
protons the only particles in the universe.

And likewise, all of high energy phsyicls that believe they are
discovering more fundamental sub atomic paritcles, is all bull shit.
They are studying the emergent beahvior of larger and larger SYSTEMS
of internacting paricles and extending the same photon error, to larger
and larger scales of nonense.

Relativity and the speed of light might well be a fundamental emergent
propety as well, where the speed at which KENETIC enegy can move though
the unviese filled with interacting particle, is limited to the speed
of light.  But I'm not 100% sure about that yet.

But if I continue down this path, I bet I can crack the fundamental nature
of the univese as well -- which is likely just electons, protons, and
one type of field.  But maybe with reltivistic effects at work as well.
The strong force and gravity are both likely emergent forces as well.

I don't know what is beong worked in on phsyics very well at all, but
from what reports I see friends share, I see no sign anyone is working on
this paproach of modling simple particls to find that the ergement propty
crated by complex orbital interactions is the source of all the other
higher level features like quantum mechans, and realitify.  So I think
I probaly have deacdes to finsih this work and still beat eveyone else.

But in AI, others are getting really close finally after all this time.

So I probably need to finish that if I'm going to make first place
in that historic endevor.  What that seem to be missing, is that they
are using deep learning networks to calcuate Q values, instead of
calucting ACTIONS. Which means the outputs are low dimension, instead
of high.  And they don't seem to have solutiosn for temporal learning
which requires memoryh in every cell. They are doing it with extra
time delay inputs , and with feedback loops, instead of with memory
in each cell -- so I probably have them all beat that way -- if nobody
else has gapsed this. But I've only got a few years lead on this
now I would guess. Not decades anymore.

So, lets sumurize my Ai work todate.  45 years of work thnking about it.
Say from 1972 to today?  Something like 10th or 11th grade in highschool.
That's 44 years.

1) I decided since the brain is a nework of neurons, the solution
would have to logically be a network of processing nodes as well.
So inputs from senosrs on one side, outputs ot effectors on the other.
Massive parallel network archtecture at the core.

2) Must have learning, and due to skinner making it clear we are
RL machines -- and due to the fact that no other form of learning
_hardware_ fits logicall, that we need a reward signal to shape
the ntwork of neurons. So rewadinput to the entire network.

3) Decided it must be general purpose so that the algoirthms and
learning are independent of the data -- meaning we can connect
any sensory data to any input, and connect any output to any effector
and the system will learn just as well -- no algorithms inside
the network to optimize to the data domain.  But "tuning" of the
system to optimize it's fit to the problem domain is valid and
expected.

4) Decided there was a percpetion problem here that needed to be solved
as well as action learning.  The perception algorithm turns raw sensory
data into an internal state represenation of the enviornment, through
a compression like alogrhtm so that the internal state contained as
MUCH information about the state of the envioment as possible -- an
information maximising appraoch that would remove redundant data in the
state reprastion so as not to waste represnation space.

This same perception algoirthm can be thought of as a compresion
algorithm, or as an infomration maximising, or as a feature extraion
algoirthm.  All are valid ways to look at the same problem.  Close
parallels to what PCA and ICA are doing.

5) Decided that perception and RL aciton learning, had to be done
TOGETTHER -- or overlap -- so that tystem learns what state data is
the most important, and distortes the internal state mapping, to not
just maximizse data about the enviroment, but to maximie the VALUE of
the internal state represenation.  It automtiacllyh focuses more state
"attention" on the things that matter the most!

My solutions tend to merge both percpetion and action learning into the
nodes of the network so each node is doing both.

So the state/action nework maximises information VALUE at evey level!
Or, action learning to maximisethe vale of actions, should be one and
the same thigns, as state value maximisnig!

Thew hole system is linking state to action so the goal is to maximise
the value of those links!

6) FEEDBACK to create action sequences --- The above network that
maximises state to action mapping to learn behavior, solves the problem
of what ACTION does the system take given the state of the envioment. It
combines patterhs to creates ouptuts.  Bulding a large feature extraion
network. But it doesn't do the reverse that is also needed -- combine
features and micro-actions to create action sequences.  That is sort of
a revfers tree bujilding problem.

But I figured out around the year 2000, that the way to do that was simply
with feedback. feed the neowrk outptus (what the network is telling the
effctors to DO) back to extra inputs so the system can "see" what it's
doing -- so that what the system is "doing" controls what it "does next".
This is a solution I've known about now for 15 to 20 years, but never
implemented, buase until I get the basic action learning network working,
adding the feedback is pointless.

But this is a key concept to how to make it work!  And what I strongly
be,.live to be the answer ot what the motor cortect actually is!
It's the motor feedback half of the cortext.

This I consider another of my great revelations that I've never seen
anywhwere else in the neurscience lit.  But, I did share this idea on
the discussion web site for Jeff Hawkin's ON Inteligence book, but got
no recognition of it being a cool idea.  And them, a few years later, I
saw Jeff ecohing my idea in a video recoding of a talk he was making. He
said it was known by neurscience -- but I think maybe he just picked
it up from me. :)  Or maybe others do know it? Again I've not seen it
ANWYERE except in my writing, an in Jeff's youtube video.

7) My recognition that _temporal_ patern matching is the KEY to everything
the percpetion algoirtthm is doing. It's how the brain learns to extract
objects.  And it's why, so many of the patern recogntion algithsm, that
end to be made to work on images, do such a bad jobs of correctly parsing
3D objects in images!  It's how imges chagne OVER TIME, that tells us
what a cat is!  Not what color it's hair is compared to tbhe backgound!
It's IMPOSSIBLE to learn hnow to correct object parsing without using
temporal image procssing.

This was all made aware to me by that crazy guy in comp.AI posting about
Jeff Hawkin's obssesion with temporal processing. And through that, I too
came to relaize how right Jeff was about that. Even if the crazy dued was
crazy, he was dead right about this. (I think he's a scientologist that
seems to be trying to crate AI to do a mind upload thing for his cult).

But the reason all the current system fails to parse iamages correctly
is exactly beuase it'x not a temporal algorithm. And the reason, even
though you can TRAIN a spial network to get very high scores on image
recognition, that it can break so badly at times (doesn't see the same
way we do) (described as very fragile learning) is that it's not doing
the temporal assocation learning -- it's all based on SPAITAL assocation
learning.

So, again, a big mystery to many is how the brain parases data and
even though we have algorithms that score higher than humans on image
recogntion, they don't seem to "see" the same way we do.  And tey don't
"understand why". I do.  Fully solved it.  It's temporal assocation
that sovles it.  But again, I've not seen anyone I've shared this show
an understanding.  I'm uncverong the mysteries of the unviese, and ntye
hitnk I'm crazy.

8) The scaling problem is solved by using a distrbuted learning network!
Another key breakthough concept I figured out.  The entire problem
of building RL that duplicated human inteligence has long been only a
scaling problems.  We know how to build perfect RL learning algirhtms
for small problems but no one has yet figured out how to scale it
to sovle the hihgh dimension, non markov problems the brain solves.
And the key to how to solve the scaling problem is to make the system
re-factor the big problem, down to smaller problems, and then sovle each
smaller problem. All done in a distributed network.  JUST LIKE CAPITALISM.
Each node solves it's own small part of the puzzle, and the collective
societ then all working oether, produces the inteligent solution, as
it's EMRGENT behavior!  It's able to learn ver quistly, becasue it's
billions of small indepdent learning machines, all learning in parallel,
at the same time! Another KEY concept to how it works.

9) Information preservation.  To learn by RL, the infomration from the
inputs, must be perserved, and sent to all the outputs.  I had problems
in early neworks were information was lost half way through the network
(multipied by zero in effect).  A network can't learn to react
to that input data, if th data is thrown out and not used to control
any behavior at all! And likewise, if input data is not sent to output X,
then otput X can never learn to use that input data.  So in effect,
alli nput data must be sent to EVERY output!  With the only thing that
needs to be learned, is how the data is mixed and matched and how each
data chagnes the mix of other data!  But the key concept is that every
output in effect always needs to be a complex mix, of every input,
or else learning by reinforcement is not posisble!

This lead to the pulse sorting approach that every input pulse had
to be sorted all the way to the outputs and not lost in the network.
But also not forked and uplicated (the problem I had in the old neowrk
long ago, was that one input would expand and flood the net, blocking
the ability of other inputs ot reach that part of the network.  So
to make sure no data was lost, I also have to make sure no data
takes over control of the network.

9) Async pulse networks are a better fit to the temporal pattern
learning problem.  This I think is true, but it's not yet proven.
Our classic tpe of value networks that act as syncrihous fucntion
with many inputs, falls directly out of our thinking that is biased
by written lagnauge -- with drawing state diagrams on a fixed
peace of paper -- just like an artist drawing of life, is a frozen
point in time of the world -- and a book or commic strip, is a step
by step adving form one frozen moement to the next -- just as how
we make movies work.  And how most classical neurla netowrks are
coded -- accepting one set of inputs as a SPATIAL slice in time.
Our historic use of writting on paper, and rawing on paper, had
distroted how we think about these problems as spatial problems
instead of temporal problems.  Switching to async pulse networks
has allowed me to learn to thn about the probnlem in a very different
perspective that I think is helping ot lead me to better answer!

But, it might be possible to slove the problem with aync pulse
networks, then translate the soution, back to a spatial syncrhrnised
solution. (each input value is just a count of the numebr of puses
over a fixed time interval), and this trasnlation might run better
on our modren off the shelf hardware.

10) Secondary reinforcement.  A key concept from behaviorism that is built
into all RL algorithms but which most people that don't understand RL fail
to grasp the imporartnce and power of.  The system mustnot just learn from
rewards, but must also be a high quality future reward PREDICTOR and that
most learning, happens by changes to the PREDICTIONS, not hard rewards.
We learn to see money, as a reward! (a classic strong exmpale) and we are
trained by the money, not by the food or sex that money buys.  So the act
of getting money, is a reward for us, and that act trains the behavior
(just like a clicker is used to train a dog without having to give him
treats all the time). The network MUST implement reward prediction,
and use that to train itself.

11) Adjusting actions based on rewards, can be the same thing as reward
prediction.  Something that hit me one day. The q value that is used to
shape actions, is the same q value that is the reward predictor.  Or,
if we biase action with a weight, that weight that controls the biase
of action, is also valid to be used as the reward predictor!  Not the
only wy to do it, but an intersting connection I made one day that I had
missed before.  It applied more strongly to the one archictecture I was
exploring at the time than to the general case. There is a complex problem
of exploration vs explotation that can mean that our action policy is
not just argmax(q) but reward prediction using arg max is more accuate of
what is possible, than what we actually try. (the fonation of q learning).
So, in the end, this 11) is not so imporant.

12) Deep learning, like RBM are doing the precpetion algoirhtm I need
to make this work.  Except it's only a SPATIAL soution, not a temporal
solution.a

13) for 15 yars I've know I needed this percpetion algoirhtm to work
and all this time, I've not been able to figure out how to code it,
until the beginnign of this year (2016) when I finally figured out
a dead simple souiton that looks really strong -- but needs testing
to confirm.  It's using the eliglibity trace as the fountion of
the nodes short term memory of activity level, and using Bassian like
logic to biase the sorting of all pulses to the node that is the most
active. This mirrors the act of multiplying probablity distirbutions
to create a prio estimate!

Then adjust bias weights, (w values) BACKWADS form teh not downstreem
node that received each pulse!  Decreaes the w the path took and
increate the path to the other upstreadm nodes! This backwards looking
learning aproach seem to be the key I never found before!  It makes
the down stream nodes FIGHT each other, over the right to receive
pulses!

14) I have what seems to be a pretty good RL algoirthm for this type
of net as well. But I'm currently stuck with the problem that the
two solutions don't play well together!  Ugh. I need them to both
work together in the same node! Maybe alternatin lvels of percpeiotn'
and RLlearning could work?  But it seems to be there SHOULd be a way
to make them play well together --- to value maximise the mapping!
So that is what is left to resovle and may be the ONL thing standing
betgween me and human level AI (that an enough hardware to duplicate
the human brain that is). But if the algoirthm is strong and good
it should do cool things in small versions as well --- like make
robots act alive and conscious!

15) So all the above adds up to the simple idea that the solution to
AI is just a statistical probabistic data mapping system that learns
the best way to map sensory data to actions, so as to maximise rewards.
It's a massive parallel statstical probablity system that shapes behavior
out of chaos.  It's a random chaos generator, that can be shaped by
rewards. Which must also act as a very strong reward PREDICTOR system.
The stregnth of it's learning, is really all hidden in the strength of
it's ability to predict fugure rewards!  And the lack of strong prediction
is why no one has made it work yet. But these distributednetworks I
am working on with the strong temporal percetion system, shoudl do it
just fine.

----

Ok, so I really need to move the above to the AI file.  Will do that
later. But just seemed like fun to write it down and remind myself of
the progress I've made.

Lets turn to physics since this is the phsics file..

-----

So I've got a simulator working pretty well.  I've got ideas for making
the display better using shadows and allowing to roate the display.
And by skipping draw ccycles for the times where it's doing high speed
caculating, itmight help improve the performance and the look. So I
could play with that.

And the idea of recoding the siuatlin with fixed smaples, then playing
it bck as a movie instead of plaing while simlulating might be a good
feature to allow long runs to be visulized in short time to see what is
really happening!

But the bigger issue, is that it doesn't seem to be working. That is
this model is not self forming into atoms.

A few things seem to be true.  My suspection that orbiting ep pairs
would tend to syncrhronize in velocity and energy seem to be true.
The synchronizing is not absolute -- there can be a 40/60 energy split
in the two orbits so the frequences are a bit off.  And I've not done any
careful test of largeer systems to see how the entire set act. Including
understanding how this requires each to be on different planes. But on
what I'm seeing now, I would expet a fairly even distirbution of spin
orriention thoughtout the set with all of them in the same ballpark
creating a lot of vibrational background noise in the entire system for
all spin angels.  But maybe it would tend to cluster in a few different
directions???  Lots of fmore care formal testing could be done down
down path.

And two p with an e can have the e doing figure 8 like patterns jupming
back and forth, and with the e in the middle more than on the outside,
the system trys to shrink todatgher -- causing the orbiting e to "glue"
the system together!  BUT, it ends to pull togeher get too close, then
suddenly swtich a different orbit path that is no longer able to hole
them dotether, whcih acts like a spring relalse, that makes them fly
apart! They, then get too far apart for the action of the electon to
pull them back together!

So there is some Some tendency for this to explain how the atomic nuclear
could be held together, overall, it doesn't work.

Maybe a more complex nuclesue with lots of protons and half the number
of electons will have wild chaotic behavior that glues it all together.
But the current modele shoudl form a hydrogen atom on it's own, and
it doesn't.

Two e, and two p, all close together should in theory self form into a
hydrogen atom with two p and one e near the center, and the other e in
a high outer orbit. I'm not seeing any tendency for this to happen.

They form into two separate syncronized ep orbital pairs and end up
drifting apart!

Something that holds to the atom together is missing!

----

[2:57 PM]

Don showed up.  Went over the use of the poower washer and chated
with him.  John the mailman showed up and Don chatted with him
We got the story about the kid he called the police on a few weeks
ago.  Argjun.  John said he went "running" into the woods.  

Ok, so the problem with this simulation is the atoms don't hold
together.  And the loose sort of attaraction that seems to exist
here isn't going to explain hos amazingly stable atoms seem to
be.

AKA, that's why we have the strong force.  So so far, I can confirm
there is a tendency for electons to glue together protrons, but nothing
as strong and as stable as needed to explain atoms.

So, more tsting of this simulation in large configurations and for
longer time periods would be possible. But I think we can ignore all
that and just assume that as it is, it's not producing the right emergent
behaviors.

So, one idea -- add gravity as it's defined on top of the electostatic
force and see how the simulation behaivor chagnes.

My basic understanding hoswever is that gravity is such a weak force
compared to electomagnatism that it likely will have no real effet on the
simlaution -- might be below the resultion of the current system totally.
I guess I should actually find the formaula and calcuate the real numbers.

Second idea, add the strong force!  But how? I don't think they have any
formal defintion of how it works -- just the informal need for an extra
force to explain how atoms hold together!  I could do more reerach into
the strong force and see if I can find any formal definiton. It would be
good to know what if anything is known about it so that even if I recreate
it with other emergent behavior I know if it matches expectations!

Thrid idea -- relativity.  Make the particles move according to
relativity. They are close to the speed of light in these simulations
already -- e7 type of velocity range where 3e8 is the speed of light.
So relativistic effects will have some effects here.  But I need to better
understand reality in the first place. I never have understood what the
basic evidence Enstine was working with to justify the theory.  And I
still think there is real potential for relativity at the large scale to
be emergent, not fundamental.  But this is a path to attack. I don't even
know how to code it however.  Do I make the mass depand on the velociety
and adjust behavior according to how mass has changed?  What about this
idea of time and space warping?  Does that have to be coded to make it
work correctly?  This is all a research path to chase down.

3.1 idea -- Does the system act the same from all obseration
perspedtives? Form all frames of reference by which we measure velocity
and location?  Something I've read made me think relativity falls out from
the changes that happen if you make the system the same from any monumtum
frame of reference.  Or maybe, if we change are frame of refence to be
one of the particles, is the system behavior the same no matter which
particle we use as the frame of reference?  This is just more of me tryig
to grasp relativitys and related concepts.  If relatiity is explained by
NOT needing or having a frame of refence, that would be cool beuase there
is no idincation the unviese would have a bacground frame of reference.

Forth idea -- what if electomagnatic force was not the simple 1/x^2
but somethign more complex that produced more or less force when the
particles got closer together? Weather though a odd shaped force field
or though effects like relativity, this could create a dynamic lock-in
point in the system.

There is an effect I've seen at work, when two electons are fighting
over the orbit of a proton, that one seems to like to "win" and grap a
tight orbit, throwing the other out into space. Which can be the issue
of some electrons having all the excess energy sucked out and turning
into a tight orbit of a neurton, while others take the excess energty
and turn into lose orbits.  Maybe something that creates a cap on how
close of an orbit can form?  The speed of light limits could explain
this for exmaple because the tigther the orbit, the faster the electron.
If higher velocityh crated high mass that creaed more nergfor the orbit,
there could be a balance point where the speed and energy blanced into
a fixed speed orbit -- defining the neuron orbit velocity and energy --
explaining the fixed amount of extran enry relaced in beta decay?

That seems like a good idea to hang around.  The idea that ep pairs have
a fixed stable point where they can't give up any more energy and go into
a tigther orbit.  And the escape velocity to get out of that config is
the extra energy that apears to be "given off" on Beta decay.

Oh, and right, the extra mass of the neturon, and the extra extra energy
given off in beta decay, is said to match the e=mc^2 formula.

If relativity is emergent, then how can this extra speed ever end up
looking like extra mass?

If neutron orbit speed had an inherent low energy stable point, then
we wouldn't need the univese to snychronise all the neurons to explain
how they might be attracted to each other.  Or, more improtant, their
ability to synchronise would be far stronger and easier if their orbital
energy was automtixcaly seeking the same stable point.

Then, all the neutrons could be better attracing each other to create
gravity?

Another reserach direction -- if neuron orbits create gravity, we should
be able to simulate to syncronized ep pairs and measure their attraction
and calibrate it for the gravity -- allowing us to calculat the average
orbital diamters needed to match gravity!

But, since netrons don't all align with each other, it would have to
only be a fraction of the total that attracted each other! So that
would have to be factored in a well!

But we could at least get a number -- how far apart does the e and p
have to be, to equal the weak force of gravity?  And see if we are in
a ball park that could match the size of a netron?

Shit, I feel like that is such a simple thing to calcuate that I have
to actualy sit down and do it.  At tleast the saimple first pass of an
e p together, and an e or p further way.

Ok, coded it, and it didn't work exactly beuase the gravity force was
more than e-15 below the electrostaic force. So trying to subtract the
two em forces produced loss of all precision.  Gee, I have to do the
math now.

ke = 8.99 e9
G = 6.67E-11

em force is:

   ke / d^2

gravity force is:

   G m1*m2/d^2

mp = 1.67e-27
me = 9.11e-31

So, when does gravity force equal em force, for a p by itself, and an
ep pair far away.  How close do the ep pair have to be to equal
the expected same force as gravity is the question.

G mp*mp/d2 = ke / d^2 - ke / (d-x)^2

D is the distance betwen the two protons.  the electon is x distance from
the electron and between the two.

I'm ignoring the mass of the ectron in this formual above and just using
the mass of the two protons for the gravity force.

So I need to solve the above for x!

m is mp .. mass of proton
d is random distance constant that should fall out.

gmm/dd = k/dd - k/(d-x)(d-x)
k/dd - gmm/dd = k/(d-x)(d-x)
(d-x)(d-x)(k/dd - gmm/dd) = k
(d-x)^2 = k/(k/dd - gmm/dd)
(d-x)^2 = k * dd/(k-gmm)
d-x = sqrt(k * dd/(k-gmm))
d-x = sqrt(k)* sqrt(dd) * sqrt(1/(k-gmm))
d-x = sqrt(k) * d * sqrt(1/(k-gmm))
d-x = d * sqrt(k/(k-gmm))
x-d = -d * sqrt(k/(k-gmm))
x = -d * sqrt(k/(k-gmm)) + d
x = d * (1-sqrt(k/(k-gmm)))

Ok, so d didn't drop out as I expected.  Or I got the math wrong

Ah, right, as I change x, the em from the electron to distant proton
will change.  d can't drop out.  Shit. Try again.

----

[6:22 PM]

Fell aleep in my chair trying to think about this. Got up too early
today and not enough sleep. :)

No, wait, shit.  The change is most certaonly a function of the distance.
My thinking on this has been all wrong all along.

If the two particles are at the same distance so x = 0 in the above
equaltion, then the force is distance becuase the two charges perfectly
cancel out.  But when you move one a small distance closer, the change
in force (what we are trying to calculate) will be very different
depending on distance.

Ah, so for very small x, we are talking just x*df!  The deritive of
force with respect to distance.

f = k/d^2 so the derivative is -2k/d^3 right?  yes.  Times x

-2xk/d^3

Setting equal to gravity:

gmm/dd = 2xk/d^3

solving for x:

dgmm/2k = x

Which looks like a very different answer than the first way, but yet the
d is still in their as a basic factor either way.  Meaning the way the
force changes is not like gravity at all it's not 1/d^2.

If x was always proportional to d, instead of a constant, then it would
be a 1/d^2 answer.  But there is no fixed x that makes it work.

Ok, unless I've really messed up the math, mu entire concept that gravity
could be an emergent propety of the attraction of orbiting nutrons,
goes right out the window! Cool.

Could it be an emergent property for other reasons????

Is there some way to adjust how the field works like relativity that
would make it fall out this way correctly???

I still can't help but have a gut instant that gravity is some type of
emergent property of electo magnatism, but the math is suddenly make it
look much harder to explain! The gut comes from the fact that it's
a 1/x^2 field just like em, and far far weaker as if it somehow "leaks"
out of the far stronger em fields.

Ah, ok, so if x needs to be relative to make it work, what if the orbit
of the proton/elecfton did adjust itself as needed in gravity to create
the 1/x^2 effect correctly?????  Could something like that posisble?

The + and 1 effect of electostatic fields is not like gravity, so the
synchronised orbits was always required as a way to turn polorized
attraction into universal attraction.

================================================================================

2016-09-18 12:20 PM Sunday

Yesterday

1) no real work, lots of writing trying to get caught up

Today

More writing.  Had trouble sleeping last night.  Woke up at 2 am ish
after a few hours of sleep  from a bad dreem.  Something about watching
a war or attack and getting shot the head.  It felt oddly real weling my
head jump.  Had to wondr if it was some sort of brain-fart back filled
with a fake storty?

Anyhow, I started thinking about stuff and got into one of my more manic
phases of excietment and couldn't sleep.  Don't even remember what I was
thinking about.  AI and robotis I guess.  Exciement from day dreaming
over bulding them and showing them to the world.  I think.

But took Niqule to sleep but it was areadly like 5am at tha tpoint so I
sleept unil 11 and have been slow to get going due to that niquire haze
that lasts for hours.

Just having my first cup of coffee now.  Taking the 5 vitiman pills i've
been taking.  Just had a bananna as well.

Physics

Thinking about the fact that the electon and protron close together
doesn't create a 1/x^2 field...  That it's actually something different
due to the fact that the d term didn't drop out of the math.  I wonder
if it could be 1/x^2 accurate to N digits?

I don't think it is.  But that's math I should consider checking.  If the
two charges were like +1 and -0.95 then the overlaping field would be a
close approximation to a -.05 strength 1/x^2 field. But because electons
and protrons have the exact same charge, and only the difference in
distance create the resulting difference in force it's a different effect.

Maybe this is also why the three particle test is not tending to glue
together as much as I expected -- the force is really not a 1/x^2
like effect?

If I make up a new force curve for electons and protons, then I might
find it works, but I would be working in the dark at that point making up
a curve from whole cloth instead of data.  I need a data justifiation for
an approach to making the atoms stick together.  Reshaping a curve has too
many degrees of freedom if we don't have some data to reduce that freedom.

So gravity isn't as easy to answer with neutrons attaracing each other.

And I don't have an answer to why atoms so strongl stick together.

If I can find an answer to explain how protons get glugged together with
nutrons, then the fact that an electon cloud would form is easy to grasp.
And if atoms with glued together protons were protected by an electon
cloud, it seems easy enoughn to grasp that atoms would stick together
by shared electons.

But wait, is not the question of how an electon cloud can glue atoms
totgether the exact same problem as electons (creating nutrons) can glue
the nucleas together?  And with the scale invarance I'm seeing in the
system, might there be a lot more in common at both levels?

The problem is that the nucleus is net postive charge but yet sticks
toether somehow.  And atoms that share an electon will also have a net
postive charge -- but far smaller with only net of +1 for the whole
system.

Ah, but wait.  This field created by the orbiting electons in nutrons
might still be the answer to whta the strong force is even if it's not
the answer to what gravity is.  I was assuming the offest e and p would
create a weak 1/x^2 field but no undersand the field is not that simple.
So maybe this field is very strong when clsoe together and very weak
when futhre apart and this is effect is what holds the nucles together?
And creats a bias so we have half the electons in the tighly bound
nucleas and half in the outer orbits?

Ok, so that theory still leaves open the option for being a path to
an answer.  But I'm not seeing that work in my current simluation.
The protons are't getting glued together. There is attractive force
at times but not stable enough to keep things glued together.

I could try to argue that the electons in the outer shells help hold
hold the nucles together.  But I don't think that could be true.
I think the outer electons are ready to fly off at a moumebnts
notice and it's the strength of the gluged together nucleus that
is attacting and holding the electons in orbits.  So the answer to this
puzzle if there is one, is really in simulating the nucleus I think.
In explaining what the strong force is that glues together the
nucleus is.

A thought -- fake it,  Force protons to be glued together with a fake
force in the simulation, and then see how electons act?

----

Looking for data on strong force let to Nuclear phsics article (and I
found the simple wikipedia for the first time -- didn't know it existed).
And led to this:

https://en.wikipedia.org/wiki/Nuclear_physics

Which is opening my eye to the history of studing the nucleus.  And that
an older model was my model -- that the nitrogen-14 had 14 protons and
7 electons in the nucleus.  And 7 in the outer cloud. But that model
failed to explain the "spin" problem.  That electons and protons have
a spin of 1/2 that cancel each other out.  So the nuclus of the n14
atom should have a spin of 1/2 but that it actually has a spin of 1.
The invention of the nutron solves this problem.

Ok, so what the fuck is spin?  At least I'm learning more...

The article tells a lot about the history that is eye opening.

I didn't even really understand that the term "nuclear physics" meant
the study of the nucleus! So much to catch up on.

But, I also understand that the discoer of the nucleus came from the
ration tests that showed how particles scattered and the fact that most
when straight though but a few bounced a large angles (when they hit or
came close to the nucelus) is what gave evidence that thee was a small
dense positively charged nucleus.

And all this work is what gave rise to the world of sub-atomic
particles like gluons and hadrons and quarks and all those things I
don't understand yet.

And there are people proposing and working on new models of the nuecleus.

Nuclear fusion is the fusing together of two smaller atoms.  Since
the nucleus is postivly charged, they don't easilly come together. But
if there is enough nergy to force them together, they can fuse,
but release lots of extra energy in the process -- which is the source
of energy in the sun.

But why does this cause the release of enrgy? Where is the energy comming
from? Is it the energy that is needed to push them together the energy?
That when they bind, the energy to foce them goether then comes back
out in a different form like electomagnetic radiation?

The fact that beta decay produces an electon and protron is still to
me a strong indicator that the system is not simpler. I still (with
my limited knowledge of the experiments) feel like all this sub-atomic
particle crap is really just adding degrees of freedom to explain the data
rather than creating a true justification of the sub-atomic particles
existing as "real" particles. But that's easy to say when I'm ignorant
of the experments and data as I am.

So, spin is where it started. WTF is spin?  Let me dig.. (this has been
bugging me for a long time)

--

Ok, it's angular monumtum. It really is a type of spin.  Not just a name.

Ok, cool.  Spin is a real type of angular monumtum, but it's aparently
strongly quantized at the quantum level. So it's fixed.  Somehow.
But how did experments actualy show this??..  Don't know yet.

But spin is seem as a fundamental property like charge or mass. And
seems to be quantized, but has "direction" (up or down?)  which is like
clockwise or counterlockwise I guess?

In reading the paper about all the funny names they came up with, I
still I'm left feeling like this is all just crazy talk used to explain
quantum effects -- like phtons can be seen as "crazy" talk to explain
the quanlitized transfer of energy from one atom to another.

Ok there is much confusion in reading all the stuff about spin.  But what
I'm left with, other than a need to learn more, is the feeling that all
this complexity is a direct anology to classical orbial angular monumtum,
but with quantun effects where particles can only be in fixed quantized
configurations.

So all this math and logic and ideas falls out from the quantization of
angular monumtum.  Which to me, again, raises the flag of possiblity
that this is all an illusion created by a simpler answer to why the
system is quantized -- as I'm trying to do with photons.

All these use of sub atomic paricles seems to me, to just be giving
names to the quantized effects, vs actualy finding real particles.

But electons have spin an in my model, there is nothing there to
allow it to spim.  Ah, but wait.

What if neutrons have real spin due to them being an electon and protron
in a very tight orbit.  And this then creates a vibrating force field
that makes all electons and protons vibarte as they move?  Could the
vibration of the paricle due to the background vibrational energy of
all the neurons, be the spin?

For it all to be strong quantilized, I think we are talking about all the
nutrons being forced to lock into a fixed frequency and all synchronized
with each other.  And I'm not feeling that the tendency to lock into
a fixed frequency can be explained by simple density or the like --
some other greater force is needed to exlain why it locks into a fixed
frequence I think -- a balance point beteween two forces or something
like the strong force and electomagnetic force.

Or maybe again, just simple orbital mechans, but yet something
more complex than the classical mechans I've been exploring -- like
relativistic effects on speed capping speed at the speed of light --
so electons and protons can't get into orbits tigther than the speed of
light for the electons?

In all this, I still don't understand magnatism.  I don't understand how
roating electons create a magnetic field.  It's said everywhere that
magnatism is "just rotating electrostic forces" (if I understand what
they are saying corrctly0 but I don't understand how rotating electostic
forces attract each other in the way described by magnatism.

How is it that electons in rotaing orbits attract each other?

Oh, but here is where spin all comes from:

https://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment

Soooo  passing particles through a magnetic field delects them up and
down dpending on the orrination of their magnagtic dipole!  But instad
of random spread deflection explained by random orrientations they are
quantized into up and down deflections (must be where up and down spin
comes from).

And this effect works the same as polorized light -- turning the magents
to different angles changes the answer in the same odd and intersrting
way that light works -- it's not "filtering" the photons by a property.

So, now we have come full circile. I don't (really) understand magnatism,
so I can't say much about how this works or what it tells us!  But as
with light, I feel that what's wrong is thinking the light is a particle
that carries state -- the state is in the resonators of the glasses.
And the resonators of the source, then partially activate the resators
of the filter, which then re-transmit energy at a different angle.

So if the silver atoms or particles have quntized spin, why not is
it in the complex orbital mechans of the atom?  Or orbital mechans of
the nucleus?

There is a comment that says you can't do the experment with an indivudal
electons.  Due to the charge overwhelming the rsult -- so it's done in
an electric field to offset the charge or something?

What if all the electons were in fact vibrating in sync with the neutrons
that all acted as resonators that created a universal background vibration
for all particles?  Could the universal vibrations all hovering at the
same average frequency be the true source of spin and all the quantized
effects?

Seems possible.

But again, now since I know a little more about spin, I have to try and
find an answer to what magnatism actually is!

----

Ok, but the Xchrodinger equations are yet another mystery to me and it's
a fundamental foundation of quantum mechans.

https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation

But also, this is an interesting comment:

"The Schrdinger equation is not the only way to make predictions in
quantum mechanicsother formulations can be used, such as Werner
Heisenberg's matrix mechanics, and Richard Feynman's path integral
formulation."

The path intergral I was sort of undrestanding when I read his small
book on that, and it seemed to be nothing more than tracing the path of
kenetic energy flow in systems thorugh many different paths.

"Schrdinger, though, always opposed a statistical or probabilistic
approach, with its associated discontinuitiesmuch like Einstein,
who believed that quantum mechanics was a statistical approximation
to an underlying deterministic theoryand never reconciled with the
Copenhagen interpretation.[22]"

Ok, so this is exactly the same thing I'm hunting for. The same intuition.

The Schrodinger equations seem to be (as I'm trying to grasp them)
a mamthematical expression of probablity waves -- the probablity of
_MEASURING_ something like a particle, or photon, and how that probablity
tranforms acrss time in a wave function.

But is it the most fundamental way we are limited in describing rality
or is there a lower more fundamental determinstic theory that would
yield this higher level abstraction of particles(measurements) being
(measurment probability) waves.

---

Ok, so I have an very crude into the Schrodeinger equations now.
I scanned the wikie page on them.  Most I didn't really understand. But
it describes the sytate of particle sand systems as a wave functions
based on conservation of kenetic and potential energy.

And I think there is room here to understand that this all falls out
from measurment proiblems -- it's all describing what we can expect to
measure -- not what is actualy there.  Due to us not having any tool to
measure at a higher level of accuracy.

---

So, now I need to move on to trying to understand magnetism better.

How does magnetism fall out from eletrostic force???

Ok, it's defined as a phenonmenon the produced by the moiton, of electic
charge! Which is I've sort of always understood and accdpted as a
fundamemal truth, but clueless to explain what it really MEANS.

I'm hoping to find a simple answer here....

Ah, Maxwells equations link eltricity and magnatism, and by requiring
that these equations hold true in all inertial rference frames Einstein
creates speical relativity.

AH ah ah.  magnatism arises from TWO sources -- electic current, and
SPIN of elementary particles!  So, if we can show spin is really
moving (or vibraing) charged particles, we make it one souce!

OH FUCKING HELL..  Great stuff...

https://www.quora.com/How-does-electricity-work-in-combination-with-magnetism-to-create-electromagnetism-As-in-how-does-electromagnetism-work

Speical relativity EXPLAINS magnatism as electrostic forcing being
bent by relativity!

So the movement of electons causes relativistic shortening which creates
atraction due to it acting as if it's compressed and holding a higher
charge -- or soemthing fucking odd like this.

Or, the inversely, the ideas of spcial relativity falls out of the
electromagntism.

Fuck, now I get it.  My model won't work without the needed adjustemetns
to explain speical realitlvitye!  My model won't have MAGNATISM if
I only classically model elctrostcic forlce!  When charged particles
are in motion, their FORCE is different -- and I'm not making the force
change based on motion!

Ok, so no clue how to model this, but this is exciting.  If i model it
correctly, the orbital dynamics are all going to change!

So now I'm getting a better grasp of how relativity falls out of maxwells
equestions. They were showing how electic current and magnatic attraction
were related.

So that's what I have to dig into.  I need to figure ouw hot to code
magatism into my model by changing force attractions with velocity.

Which means coding relativity into my model. How fun.

Ok, national cheesburger day.  Talk Pig into going to fosters for a cheesburger fix.

[5:04 PM]

----

[7:13 PM]

Back from burger trip and a bit of pokemon.  Buddy feature recently added.
need like 8 more candy for squrital upgrade to get that missing guy.

But physics.  Looking at minumte physics.  Darn,m just so fucking cool.

So without modeling reltivity effects my system won't show mangnatism!
I had never been shown that before!

But, I have no clue how to add relativity effects to my model?  Bend space
(xyx), Bend time? Bend force?  Bend mass?  WTF?

Does relativty assume some sort of background frame of reference all
others is tied to?

I could code mass as a function of speed, making speed change in different
ways than it does no due to shifting mass.   I doubt it's that simple.

I can't code variable time -- the whole system has to operate in the
same time frame.  But a moving system ending up "running" slower seems
possible.

If force was different due to relative velocties, that could be coded.
So the froce between two particles is relative ot the relative motion?

In fact I wonder if that mibght be the only thing needed to be coded?
That would make it look like variable mass I think maybe?

================================================================================

2016-09-19 12:48 PM Monday

Yesterday

1) Learned highly important point!  My model doesn't implement magnatism!
Even though electostaic force and magnatism are said to be one and the
same things, my model that only implements electostic force, fails to get
magnatism correct!  I actulaly sort of knew that magnatism was more complex
than electostaic force but had sort of failed to fully grasp that and
was assuming that if you implment only electostaic force in the simulation
it would show magnetic effects as an emergent property.  Wrong.  Because
when charged particles MOVE the laws of magnatism start to control their
motion not just the laws of electostaic force. So MOVING charged points
act differently!

2) Big point I learned -- Relativity falls out of Maxwells equations
that document the realtionship between electrostatic fields and magnetic
field when you assume that the equations must work the same no matter what
reference frame you measure from!

Today

Been studying electomagnatism to try and gain a deeper and more
fundamental understanding of it all.  I thought I actually sort of
understood this from work with electoncis, but I see at this fundamental
level, I have lot to still learn.

Added -m to my ciall script so I don't have type the "checkpoint" message.

Reading about maxwells equations.  I don't understand them.  Trying to
better understand all sors of stuff -- like volts, and how they relate
to charge.  If voltes is presure I would expect it to be newtons, but
it's not. It's newton-m/charge.  confusing.  Or potentital-energy/charge.

I'm not getting a conceptual understanding of this yet.

Also, in trying to understand magnatism, it's said that elementrary
particles have innate magnatism (spin) which is a type of angular
monentum.  Or at least is measured as if it were.  And it's quantized.

But orbiting electons create magnatism the same way.  And the total
magnetic effect of an atom is the combined effects of orbits and particle
magnatism.

But, in my appraoch here, I'm expoorting the idea that neutrons are in
fact orbiting ep pairs which would then create their own spin due to
this and when synchronised with other pairs, create a vibarting field
effect that could spread though all of sapce.  Making individual electons
vibrate at this same rate a well -- and I'm wondering if this vibration
of individual elections is the real orginin of the notion of spin.

Which would suggest e and p don't innately have magnatism, but rather
their complex vibrating orbital paths create it.

So, then we have the question, WHAT IS MAGNATISM?

We are sugesting that the moving path of one electons, will cause a
change in the path of another.

But when we look at the eqations, so far I'm seeing macro level effects
being described (electical experments of currents and wires and magnaetic
fields), not mirco effects.

For example, the concept of the Amp has no meaning for a single electon
moving through space!  Because Amps is a count of electons moving past
a point but one electon in motion can't have an amp measure.  So, wtf.

How is this translated to partical motion?

More questions.  The simple film in the link from yesterday gave some
intuitive understanding of magnatism as one charged particle in motion
next to a wire with current -- saying the relativistic shorting of the
wires acted as if the postive ions that were staic in the wire but acted
as if moving relative to the charged particle, would shink in length,
making them act as a strong postive charmnge, but the moving elecotns,
moving along with the external charge did not shirnk, making the system
act as if it were more positively charged and attract the extrnal
moving charge.

So what does this mean for my simulation?  If I look at two particles
and calculate the force, do I have to look at their reltiave speeds, and
adjust the effective charge, or effective force, based on relative speed?
So the higher the relative speed, the less force it has?

Can this be transltaed to looking at dr?  Meaning the derivitat3e of the
distance between the two particles?  So if the distance is not changing,
my current force is correctd, but if it's changing, the force will be
higher or lower?

Could the change in distance by adjustd by lorentz contraction to make
the effect happpen?

----

Reading about space time and lorentz contraction.  These things are
constantly descirbed at "what it looks like to an abserver".  WTF does
that even mean? How do we analize the motion of atomic particles under
the idea of "an abserver"?

It feels to me that much of this might have fallen out due to measurmeent
errors -- which is a key driving issue of my work here.

The limits of what we can MEASZURE can be understood dfiferently from
what might actually be happening.

So if two electons are moving together though space in the same direction
relative to another electon elswhere, what is the force those two apply
to the third?  Do we look at them as if they shirnk by lorentz contraction
and get closer together, and then use this new shorter length to calcuate
the effectdive distance and force?

That's what the video implied.  But that wouild imply that a pritcle
moving perfectly perpendicular to another, would have no contraction
effect.  Only those further way would "contract".

In fact doesn't this then end up implying that if a particle was approach
another one straight on, the distance between them would be contracted?
So it would have a stronger force?

Well, lets think.  If we have electons in a row all moving together
creating a current they are said to create a magnetic field that would
then attract another electon in near by wire IF it were moving, but NOT if
it were standing still!  So we hve relative motion between the electons,
and yet no attraction!  But in the wire, there is also logical movent
of holes.  Electons get ripped out of the obrit of one atom, creating
a +ion or hole, and that then sucks an electon from a nerebuy atoms --
creating the cascading current flow.

So we have the ions at play here as well as the electons.  The ions are
standing still, but the electgons are moving.

This relativty crap just doesn't add up when looking at it from this
perspective.

Current flow in two partallel wires, when the current is flowing
inthe same dirction (as happens in a coil) PULLS THE WIRES TOGETHER.
And inversely pushs them apart if flowing in opposit directions.

Why does that happen?  What would I need to re-code in my model to make
that effect happen?  Is it the electrons pulling towards each other or
the ions or what?

Ah, what if relative motvation of a pe pair made it more attractive and
relative motion of ee pair made is more repulsive?  Or oddly reverse?

And there is all this talk of spacetime where we must thinking of events
in 4 space.  But why?  Is it needed to define events affecting each other
across time?  So the past location of a particle is what is determining
the current force on another particle?

That would be consitent with the idea that that force was limited to the
speed of light.  That to see what force particle 1 had on particle 2,
we have to rewind the clock to see where particle 1 was at a point back
in time?  So when a particle is not moving then when we rewind the clock,
it's at the same distance.  But if it's moving, when we rewind the clock,
it's a different loction and that different location is where we have
to calcuate force with?  Maybe that's what's at work here.

Another comment talked about very old theiroes talking about the
electosatic field being distroted by the motion of the charge.

----

Side note -- Pig bitching at me about how much coffee I'm drinking!
I just got my thrid Kcup of coffee. [2:01 PM].  She's trying to agure I'm
wasing "money" by having too many Kcups and I should just be making pots
of coffee if I drink this much.  For 20years, I've had a daily starbuck
habit where I spend $5+ per venti lattie.  And she had bitched about this
for 20 fucking years.  Now, for the first time in 20 years I was doing
as much as 3 starbucks run a day at one point long ago. But have cut
it back to one a day for a very long time now.  Now, due to the fatty
liver problem, I deciced to cut back to 3 a week, but once I ended my
daily trip, I find I'm not going ther much at all maybe more like once
a week.  So now I'm doing Kcups at home.  That cost like 60 cents each.
Three Kcups is $2.  Vs $5 I was spending on Starucks.  And she is not
bitcing becuse I'm doing $2 a day in Kcups. Spending MY OWN FUCKING
MONEY -- which she thinks is hers.

I should docuemnt all the crap I end up taking from her.

----

Back to physics.

But the stuff I'm reading eally indiates that not until Einstein did all
these ideas convert to using spacetime to make it all work and to explain
all the effects -- that spacetime is distroted by stuff, like gravity.

But, WTF?

Ok, so I think that might be it.  To understand the force on a paritcle
you have to figure out where it was back in time, to account for the
force effect being transmitted at the speed of light to the "receiving"
particle.  So in effect, how a particles motiion is changing at time t is
function of the location of all thother particles back in time relative
to how far away they are from us?

And likewise, if p1 effects p2, we have to look back in time to see
where p1 was short time ago, to see it's effect on p2. But likewkse
we have to back up p2, to see how it's now effecting p1.  And maybe,
the way to best describe all this in math, is by making spacetime a 4
point coordinate system?  That makes sense.  Oh, but no, does it?

Ok, and again, all that view makes spooky action at a distance impossible.

But if we assume the electostatic field has no time delay, then we can
conceive of how quantum action like that works.

So, to keep that, we are left still having to explain magnatism some
other way?

Which is what it seems everyone was stuggling with 100 years ago and
all setteled on Einstiens solution.

Which, I don't really undrestand beuase honsetnly, I don't really
understand magnatism.  And could magnatism be a false emergent propertyu
a not a low level fundamental one?

I understand some of the basic taught in EE work at the macro level.
But I don't grasp how that translates down to this simualtion of paticle
model level.

Let me dig more into basic magnatism to get a better grasp of how the
system SHOULD work.


----

http://physics.stackexchange.com/questions/87082/how-is-relativistic-n-body-simulation-possible-without-knowing-the-entire-histor

"How is relativistic N-body simulation possible without knowing the
entire histories of each particle?"

Well, this seems to be the very thing I'm looking for.  Found it
yesterday, but revisited it today.  Sounds like this guy is trying to
do the same thing I am -- N-Body simulation of atomic particles.

The implication of the question and answer seems to be that you need to
use retarted positions for EM forces.  But that with gravity, it will
distort space time.  But for this level of modeling, we ignore gravity
and just code retarted positions.

But if EM field effects propigate at the speed of light, making it
required we do retarted position calcuations, then we have no answer
for QM spooky action.

So, if there's a cool answer to be found hidden in there, I think we
need to find a way to make magnatism manifest itself without time and
space distortion. Or without using retarted position to calcute foce.

What if we use speed and dv to bias force but not actually retard the
position?  That would act a bit like retarted position effects but might
produce important qnd cool orbital dynamics that explains the rest?

My attack is still the idea that there are complex orbital effects are
creating emeregnt propety that is being documented in all these qm and
reltavistic equations but yet can be explained with sipler low level
fundamental effects.

So, let me go back to magnatism.  How does it actually work at the
macro level?

----

Next Pig bitch.  I was walking circles in the kitchen thining.  It's
helpful to walk and think at the same time sometimes.  She then
comes in and starts to bitch about how she "hates" when I walk
around with "nothing to do".  I tell her I'm thinking.  But she
then says "I have to do do this work -- get out --- I don't want
you here when I do this work".

This is the shit I've put up with for 20 years.  No repect at all
for what I want or what is important for me.  If she wants something
my needs and desires are irrlivant.

----

https://en.wikipedia.org/wiki/Lorentz_transformation

Ah, wonderful.  Finally a simple answer to what an inertial frame of
reference is -- two frames moving at constant velicity!  When moving
at non constant velociety, it's a non-intera frames and the Lorentz
Transforms don't apply!

And, a clear(er) answer to what is meant by "observer".  It means any
measurment made from that frame.  No human required.  But it does beg
the question of what a measurment is!

Ah, but the speed of light is the same in all frames!  Freeky!

So I can go twice as fast in my frame, but light is still moving as fast!
So that's why reltiave time/distance distort -- to allow for such constant
speed of light!

Ok, maybe the unviese really just is weird like that.  But the very
notion of a "measurement" opens the door to explain all this as measurment
effects and not underlying fundamental properties!

Which leaves open the door to the idea that all this complexity is needed
to explain what can be MEASURED about distant events in space time,
but all the complexity could still emerge from simpler fundamental
properties that don't include such distorations.  GOing back to my
idea that the speed light is an effect rated to how fast kenetic energy
can be transfered separate from how fast potential energy and electic
fields move!

So, is there some way to use my newtonian view of particles and change
the calcuatoin of force, to create magnatism, without complex time and
space distorations, but which cuase the emergence of complex time and
space distortion in the measurments!  That's stil what I'm digging for.

So, MichelsonMorley experiment 1887 -- speed of light is the same in
all directions measured -- so the rotatoin of the earth, or movement of
earth orbit, or moveent of solar system though some background reference
frame does not exist.

But the simple laws of particle intereation using charge create a
speed of transfer of eenrgy, just beuase of the F=ma delay and the
1/r^2 distance effect.  Aka, the futher way a particle is, the slower
it responds to chagnes in potential enrgy --- aka the slower kenetic
energy can be transfered.

I've not worked out the math, but it seems on the surface to be a sipmle
answer as to why the speed of light is a constant.

And again, if the source and receiver is in motion, it doesn't change
the speed becuase the f=ma delay effect is not a fucntion of underlying
speed, but only reltive speed of the sorce and receiving paticle.

I should look at the math.   But the issues of why this wouldn't work
could be simple.  The idea is that if we add kenetic energy to the
source particle, how long will it take for some fixed amount of kenetic
energy to be transfered to the destination?  The answer will of course
depend on how much is transfered.  x Joules would take less time than
2x Joules. For there ti be a "fixed" speed, the energy transfer threshold
level would have to be fixed.

But, if we put 10x as much energy into the source, would not 1x of
enrgy show in the distanation a lot faster making it look like the
speed was that much faster?

----

So though photons have a frequency and the amount of energy they carry
is based on frequencye, the speed of transfer is constant.

How could that fit into my odd ideas of the speed of transfer of
kenetic energy?  Different speed ossolators could define the frequency?

But why would the apparent speed of transfer, be constant?

Maybe, if an ossolator transmitts energy at a given frquencye, the
only thing that can receive it, is an ossolator of matching
frequencey???  It won't buildup energy and throw off an electon
in the atom if it's not matched in frequency???

So them there would need to be some mathamticaly reason that matched
ossicilators always take the same time to transfer energy at a given
distance.

----

Ok, cool.  Just found the ladder paradox!

https://en.wikipedia.org/wiki/Ladder_paradox

This is what I've never understood about relativity?  If I'm moving wtih
the ladder it shrinks and fits in the barn.  But if I'm moving with the
barn the barn shrinks and the ladder doesn't fit in.  Or is the inverse?
Anyhow, one way it fits and the other way it doesn't how can it fit at
not fit at the same time?  The answer is that the time one end o fthe
ladder SEEMS to exit the barn vs the time the other end enters (fits
inside) are two events that are seen differently by the two observers!

So one thing happens, but the two observers see it differently!

Is this just nothing more than the idea of time delay of information flow?
That is the relative time that information to get observer A is different
than the relative time the information gets to B?  It seems that way.
Let me think a little deeper about whether that's all it is.

And what it makes it all so non clasical, is not the fact that there is
infromation flow that is delayed across time but the apparent speed of
the flow is the same in all timeframes!  So all must distort for that
to be true!

So to get this all back to classic (my wild dream) we must explain why
infomration flow is relative to the moving objects not to the refence
space (which sort of just works already anyhow with my ideas of speed
of kenetic energy transfer).  But now I need to look deeper and see if
this can really all be true.

That is, if informaion flow for reasons of how kenetic energy flows is
fixed by distance but not by relative speed, does everything else work
out weird like this in terms of what happens at one observation point
in space?  Even if the observation space is moving?

The fact that clocks slow down is wired however.  That doesn't make a
lot of sense in this other prespective. Does it?

I need to understand what the atomic clock experment really tested and
showed to make sure I'm not assuming something that didn't happen...

But if you take kenetic energy out of system, the entire system could
maybe shirnk in sice and speed up.  Making time seem to pass slower to
the abserver since the abserver is running faster?  But I think that's
100% backwards from what relativeity is saying.

[6:58 PM]

Just finished folding my laundry.  Shirts and socks.  Been doing that
for a few years now I guess (don't remember when it was that Pig decided
she wasn't going to do that for me anymore).

--- OK in the garage frame, the ladder is moving and becomes shorter.
Which is NOT consistent with the siplified idea of delayed infomation
flow I don't think.  So the head enters the garage, and I notice it.
A short time later the end enters the garage. In another frame, the head
as already left, but in my frame, I don't find out about until after I
find out the tail enters, so it looks to me like the tail entered befoe
the head exited -- it looks to fit.

But what If i'm at the exit side.  I see the head exit, but I don't find
out about the tail entring for long time later, so it doesn't fit. So
this doesn't work at all!  It only works if I'm standing at the entrance.
doplar compress heading to me, expansion when heading away. So doplar
effects don't explain relatiivty in this way.

Ok, but what if it's just the opposit?  That is there is no doplar
effect measured?  The osscilating source seems to mantain the same speed
of oscilation heading to me as away?  And anti-doplar effect?

That doesn't make sense.  If there is information transfer delay, it
has to create the doplar effect.  And red shift is argued to be just that.


================================================================================

2016-09-20 10:47 AM Tuesday

Yesterday

1) Just spent time reading lots related to magnatism and maxwells
equatoiins trying to better understand magnatism and relativity and how
it all applies to the atomic modleing I'm trying to do.

2) Found the ladder and twin paradox pages on wikipeda -- I always
saw this problem of relativity that I didn't understand and now I have
a better grasp of what I was failing to understand -- that the entire
concep tof "when" (and where) something happens is distroted in different
frames of reference -- time is not universal under the weird stuff that
happens in relativity. I still don't have a firm grasp of it all but
I have a far better undrestanding of what I didn't undertand before!

3) Learning about diverence so I can understand what maxwells equations
are telling us.  I now have a basic grasp that the first equations is
telling us the electic field at any point or (surface?) is the charge
inside the point (adjusted by e0 factor for unit correction or material
property maybe).  And the second equation is that magnetic fulx has no
begenning or end point so the net flow in and out of any area is always
zero.  The 3rd is the relationship between magnetic and electic fields
such that the change in the magnetic fux is always equal to the electic
field.  Well, something like that. Not actually that. But it relates
chagne in magnetic field to electic field.  And the forth I don't
understand.  Some relationship between current and change in electic
field producing a magnetic field.

Today

Still more jumping around from reference to reference trying to
understand magnatism and relativity.


================================================================================

2016-09-21 4:52 PM Wednesday

Yesterday

1) NO real work -- started to edit notes above then got side tracked.
Did spend time thinking.

Pig left for the day, and suddenly I found I had no motivation to work.
odd how motivation can shift so quickly for me.  I felt compeled to
go do something but yet not feeling free to actually go do antyhing.

So I went to pizza, and played pokemon.  Rolled over to level 29.
Almost attacked a gym at the freeman house enough to add myself. It
was like level 9.  Too like 10K of traning, which was going like 200
at a time. Slow.  Beuase even the weakest poke was like 1900 level
vs my strongest at 2400.  I could only kill the first 2 in the list of
10 even with my strongest. If I'm real lucky, it's still up, and I
can add myself later today.

Today

Got side tracked working on Monty Hall problem and facebook.  I've got
to do a blacksmithing demo tomorrow for the kids for Phil.  Phil has
done this group for a few years now but he's in England and flying back
tomorrow so he talked me into doing it. Don't really want to. But felt I
had to. But I'm procastanating instead of dealing.  I need to go to shop
and make sure it's clean and read for tomorrow but I've procrastincated
on facebook instead of doing that not it's late in the evening already.

Physics

Well, been studying magnatism but I'm still fucking lost.  I have still
not grasped enough to have a clue how to code magnatism into my atomic
model. Maxwell equations are sort of beyond me. Sort of.  I have to better
study the math of vector calculus to fully grasp it. And of ocurse,
I also don't fully grasp how all of relative falls out of magnatism
either -- but I can accept that it does and sort of grasp why.

But most of magnatism is descrinedint terms of fields.  Which doesn't tell
me how, for a given atomic particle like an electgon, how to calcuate
the force on it that is crfeated by magnatism vs just the electostic
force I calucate now.

Of course the entire idea of flux lines is an imaginar concept to
express the interaction of magnetic field and objects, like a compus,
or any magnet.  The bottom line, is that the path an electon will take,
though space is relative to what all the other particles are doing. But
in the electro static model, only the distance to the other particles
is impotant. But magnatism, shows that the their velociy, is important
as well. Maybe the accelercation they experence as well.

So I knjow the formula for mapping distance to force. But I'm lost when
it comes to how to use velocty and acceleration as well.

All the relativity and spacetime talk implies that the problem is (most be)
solved in 4-D space not 3-D. But I don't yet grasp the full comlexityh
of any of the math.

More imporant the math seems to be documenting macro level emergent
effects instead of micro level effects. It documents how coils of
wire create magnetic fields and  how magnet field create electial
field -- or are cross related to electic field.  I don't know.

The "length compression" idea implies that when I calculate the distance
to particles to caluave the total force, I need to use different locations
in space adjusted for compression in order to get the true "effictive"
length to get the true "effective" force.

But for complex motion, this seems to imply the need to not just make
a simple adjustment like that, but rather, I need to revere time and
figure out where particls were at different points in time.

This is all the idea that time and space is being distroted by moition so
for a single particle to know what it "sees" -- how the other paricles
act on it, it's must use a different version of the current "state"
of the universe which has all the particles that are in motion acting
as if they are are different location in space.  So for each particle
which I want to know the force so as to estimate where it will go next,
I need to do some hediously complex time reversal problem based on motion.

But is that sort of approach REALLY the ansewr?  The only answer?

If electomagnatism moved though space with a time dealy, then we would
of course have highl complex problem of backing up time based on how
far each particle was to find out what force "waves" were reaching a
particle at any instant.  There are videos on theinternet that seem
to be indicating complex calcuaus to rethard things to cope with that.
But I can't belive it really solves the B body problem. I thnk that math
might obnly be used by EEs to copew ith a singl esource redial transmitter
and how it interacts with things like waveguides and receiving antenas.

But, if there's something cool to be found here, there's the need to
expect intentanious interaction between particles, not time delayed
by the speed of light.  And that the time delay, is onl the result
of effects that take time to move though the medium of many paricls.
Partical A mvoes, is effects B, which effects C, which effects D, but
since these are all differental equation type effects, it all creates
4 chained delaed reaction. And it's the total complexity of that chaned
dealy reaction that I think maybe all this space time crap is documenting,
vs the true underlying effects created by intentanious interactions.

So if there is an inestenanious anserw, it needs to be some approach
that converts not just distance into foce, but velocity into force
as well.  Meaning a moving electon, creates a different force on the
particles aroudn it, than a stantionalry one.

And for this all to be relative, and fit the gernaly truth of relaitvity,
it needs to be relative velocities.  That is, we can look at particle
A as standing still, and B is moving, or we can flip and look at
it from the perspective of B standing still, and A moving, and the
results of what forces get applied, should be identical.

So, hoping to find something simple and cool, I should just assume there
IS a solution like that, and guess at what it has to be to make magnatism
show up in the model.

So, is the velocity direction vector of the second partical important? Or
only the current effective change in distance between the two maybe?

And if the direction is improtant, is the acceleration vector important?
Not just the velocity vector?

Maybe there's some basic rule here that the total foce on a particle from
all other particles in the universe, is what defines the force between
to particles?  Something oddly self recursive that can't be directly
calcuated? Where we have to itterate across all paritcles for a while
to answer the question of what the total foces are?  Just thinking......

But I'm lost becuase I don't really even have a clue what magnatism
should do.  THe problem, is that if we have two wires with current
flowing, they pull together, is that the wires are themsleves complex
systems of electons and ion atoms. Is it the moving elctrons alone that
pull it together, or is it interaction with the ions?

Two parallel streems of electons in free space seem to me like they would
never show a force pulling them together (aka even if you subtract out
the obvious force pusshing them apart). They woudl be moving in perfect
unisoh, which means they act as if there is no relative motvion between
them, which means they should act the same as if they were static.
So the relative motion of the electons relative to the +ions in the
wire, is where the maganetic effect it seems must acutally be happening.
But them we are talking abot simple ecotons, interaint with higly complex
atoms -- which opens the door that the interaction is anything but simple.

So what is the "simple" interaction between two electons or between
an electon and proton, we need to explain or create magnatism effects?

What the hell even is the magnatism effect?

I know that current in two parallel wires, flowing in the same direction,
cause he wires to pull together. That's a smiple example. But rquires
as talked about above, +ion atoms in the mix.

And magnets, bend electon streams, in a CRT.  So a magnet flying through
a megnetic field, bends it's path.

And the magnetic field can be created by curent in a wire.

But what abouut two streams of electons, at different angles, and truning
on and off. How do they interact?  Does one electon in orbit around one
proton, create a magnetic field like current in a coil?  Would it cause
a near by stream of electons to bend?  how?  What direction?

Ok, let me look up how a magnetic field makes electon streams bend and
see if that starts to approch some basic infrmation that might be helpful.

----

Wonderful answer here:

https://www.quora.com/In-a-cathode-ray-tube-the-rays-get-deflected-when-a-magnet-is-brought-near-them-Is-that-because-a-moving-electron-creates-a-magnetic-field-around-it-or-because-a-magnetic-field-will-bend-the-path-of-any-charged-particle

"The forces produced on moving charged particles by a magnetic field
are always perpendicular to the direction of the field.  The forces are
also perpendicular to the direction of motion of the charged particle
in the local frame of reference."

NOTE the "local frame of reference" comment!!!!

"The magnetic force on a stream of moving charged particles is calculated
as the amount of electrical charge, q, times the velocity of the streams
of charges crossed into the magnetic field vector, F = qv X B."

"F=q m1 q m2 / 4r2"

----

Ok, cool so this gives me something a bit more concrete  at a level I
can understand to seek my teeth into!

A moving elctron, will create a magnetic field, and an electon moving
though a magnetic field will have it's path bent by the force creatd
by this cross product. So if we have to electons, and that's all, how
do they interact. Which is creating the field, and which is having it's
path bent and can we look at it from both frames of reference and always
get the same answer!

And, the force is a linear fucntion of the vfelcity of the chraged
particle.  The fast it moves, the more force, the faster it curves.

Does this mean by chance that the path is the same no matter how fast
it moves??? I'll have to look deeper into the math to see what that
implies.

Hum, if a magenet field runs N to S, like the field lines are drawn. And
the electon cuts across that, the cross product seem to say it would
bend right or left due to this. But the spin experment foced things
to move up and down in the field didn't it???  Not left and right?

Ok, but cross imples the force is always perpendicular to the veloicty
and perpendicular to the magenet field direction.

So, if we take a thrid "ground" frame of refenec,e and make two elecotns
zip in parallel motion, we can say they are both moving.  both creating
a magenetic field that circles them.  And both, will be crossing
perpendicular to the field the other one is creating.  But on different
sides of the field.  One will point up, the other down.  And with right
hand rule, both will see a force outward, or both inwards. That seems
to be consitent with the siple idea of currents pulling wires gogether
with the electon effectg alone.

But, if we shift to a frame of referece of either electon, we have
no motion at all, and no added force!  So that doesn't work!

so, lets look two electons traveling parallel, but in opposit directions
at the same speed.

From one frame of referce of one electon, it's standing still, and
the other is moving creating a magnetic field. But we are not moving
so there is no motion though a field, so no force?

But is it valid to think the magnetic field is moving along with the
electron in this case, and that counts as the stationary electon "moving"
through the field?


Ok, found kahan acadimy lectures as well.  Basic magnatism like I've
learned in the past.

But cool think is F= q(v X b) is the basic force equation!

So, I don't understand relative issues of how to define B relative to
veocpety of moving elecotn yet, but I'm getting there.  And all this is
telling me that when electons move, they force each other to chagne path,
not just do to the static charge, but due to the fact that are moving!

But looking deeper, the magnetic field generated is propotiona to the RATE
OF CHANGE if the electical field one of the maxwell equations says. But
that's nt right. A DC electo magnet produces a fixed magentic field with
no chagne in the current.  Again, contricitions in my undresanding show
I don't undrestand something.

Oh, I see, the fixed current creates a fixed magnetic field. But a
changing current also adds or subtracts from that field stength as
well! So it's again not just the velicty, but the chagne in feleicty
that effects the field strength.

So, if I understand, a moving electon creates a magnetic field. But a
moving election undergoding accelaration, creates a different strength
field!  Could that be an odd time delay issue?  Or odd self feedback
comlexity? The type of thikng we could remove from our simuation and
see re-emerge as feedbackloops in the system?

Ok, so on the belief that we do this with field that have no time delay,
we cna look for simpler answers first, and code, and see what happens,
or check math etc, and see if the more complex effects is what energes
from the sytem due to the infinite number ofeedback paths in the system
between all particles all the time?

So maybe the slimits ide ais moving elecgon is current, it crates
a magnetic field, that cuuses other elctons to bend their path.

----

Thought -- forces must be equal and opposit always!  So however we
calcuate the force on one particle, the matching opposit force
must be applied to the other!

When the two pass each other they create an effect it seems.

So all other velocties I think must be projected onto the line
ebtween the two, and we ignore the effect of them moving away
or towrds each other, and only look at the relative velocoty
of the two moving at right angles to each other.  And the faster
the relative perpenduclar velocoety the stronger the magnetic
bending effect!

But, two electgons travelling partallel to each other will end
up with zero net effect on each other in this case.  Which might
be perfectly fine!  Meaning for two wires to bend together,
it really quest there be postive ions going to the other way
in relative terms!

so we have two frames of reference there, the electon FOR
or the ion FOR and we could check the math to see if it all
works out the same from either?  Or from any made up other
ground FOR.

Yeah, and two of the same charge moving oppois of teach other
will push aprt but a + and - moving opoisit, will pull together!

So an orbiting e p will pull together!  And the faster the
orbit, the more it pulls together?  And what about distance
aprt?  The closet together, the stronger tbe magnetic field?

So the closer the orbit, the faster, and the more the magnetic
effect pulls the two together as well?  Weird.

----

fuck yes,  Mag field generated by moving point charge:

http://www.phys.uri.edu/gerhard/PHY204/tsl210.pdf


"strength of a magnetic field pased on motion of a charge"

Googling this produced a list of intesting papers...

----

Charged particles produces a perfect circle in a magenetic field! Or will
spiral up or down...

Oh, fuck, and orbting electon will produce a magenet field that will
cause another chagned particle to spin as well parallel to it!

Oh, and e and p in orbit will spin in opposit directions!  But due to
their opposit charges, their fields will add I tihnk.  Or wait do they
rotate in opposit or the same directions! gee, I'm confused now. I think
due to center of mass, they roate the same way. Yes answer seems to be
same way.  so their fields would cancle out except the electon moves
faster, so makes a strong field since the e and b have the same charge?
or do their different size orbits make up for the difference somehow?
Oh, so so so confusing all this.

Oh, I've made progress on nailing down more formauls and con cdepts but
still muchconfusion to be worked out.

And one paper clealry implied dealyed time effects of maganetic field
effecting distant particle.  Time delay for something at least, maybe
just magenetic, maybe both electostaic and magent effects??

Dman, need to run to BGOP.  It's fucking 8:20PM already.

[8:20 PM]

================================================================================

2016-09-22 10:27 AM Thursday

Yesterday

1) Found some more interesting papers and formulas on single charge magnetic
effects.

2) More thinking and digging and learning.

Today

Giving BGOP demo to shool kids at 1am.  Must leave soon to get shop set
up.

But I had a collection of thoughts while thinking about magnatism effects
last night in bed.

One, I now understand the comment I read about magnatism not doing work!
The force on a moving particle is always perpendicular to motion so
it will never change it's speed or KE!  It will only change it's direction!
Odd, but intersting.  Electic motors do work. But I guess the argument
here is that the magnetic field is not doing the work?  Not sure how
to recocile that idea.  But it's like making the particles bounce at the
wall in the simulation doesn't change energy since it's only a veloctiy
direction change.

How does an electic motor do work if a magnetic field can't move energy?
I guess the obvious ansewr has to be that the electrostatic field has to do
the work in the end.  Or, in other words, the resitance to flow in the
wires of the motor is created by the electrostaic field even if the flow
is created by the magnetic field?

Ok, so, also, I was wrong (I think) yesterday when I said foces have
to balance.  The equations don't work that way.  The movement of p1
relative to our point of view creates the magnetic field at p2 and
the movement of p1 defines how much force there is on p1.  So the
movement of p1 creates the force on p2 to make it's direction bend.
And the movement of P2 creates the force on p1 make it's direction bend.
If we change the point of view so p1 has no velocity, then there is no
B force on p2 and no direction change.  There must be movement in both
P1 and P2 for there to be direction change, which means both P1 and P2
will end up bending their direction.

This is all based on the formula that says B = u/4pi (q1v1 X r^) / r^2 and
F2 = q2V2 X B

This sure seems to violate inertia frame of reference independence.

Both have to be moving before there is any effect.  So if we change frame
of reference to either of the moving particles it kills all the effect.

I guess maybe we could consider relative velociety for both.  So the velocity
used to define the field is the same as the one used to define bend?

So, if P1 is standing still and P2 zips past.  We can say P2 is generating the
magnaetic field at that speed. But that p1 is logically moving through the field
at the same relative speed.  Or, we can think of the field generated by the
moving particle as moving itself, and use that as part of the formula.

Ok, so later I can check that logic to see if applying the formulas in that
way makes it point of reference invariant.

---

Ok, another point, magenetic constant is DEFINED as 4PIe-7 so above, B
reduces to B = e-7 (q1v1 X r) / r^2

Or total force is:

F = q2 V2 X (e-7 q1 v 1 x r) / r^2

I think constants can moved outside in a cross product so:

F = q1 q2 1e-7 (V2 x V1 x r) / r^2

If we do the relative velocity trick, I wonder if this works out to be
equal and opposit forces anyhow?

Hey, wait. If it's not equal and opposit forces, then we fail to maintain
conservation of monumtum in the system!  And that won't work.  So I
think we do in the end have to end up with equal and opposit forces,
and maybe that works with the idea of thinking the B field as "moving"
to resolve the frame of reference issue.  Or, in other words, we can
also probably calucate the froce once, and apply it to both particles
in equal and opoosit ways.

I'll have to think though all this math and the implications but this
sort of feels like we are headed to the right answer. Or one that seems
like maybe it fits the rules of physics.

But I'm not seeing anything in this that suggests a speed of light limit.

But again, that might still be a kenetic energy probigation speed though
space effect that shows up in fields of particles vs indivdiual particles?

And all the complexity of bending space time is an effect that might
only show up as an emergent property of the kenetic energy probigating
though space as well.

[11:09 AM]

Must pack up stuff for demo and head to BGOP...

Wait, but if the force is equal and opposit, it will end up doing
work won't it?  Or maybe due to the double cross produts, it still
maintains it's property of applying force perpendicular to both
velocity vectors?  Ok, got to check for that...

----

[4:51 PM]

Back home from demo at shop.  Took shower.  Now I'm tired and about
to pass out.  Going to see magnificant 7 movie at Angelica tonight!

Hungry but don't want to eat. Will have popcorn at movie.

Funny story about shopt.  Wheen there an hour early to set up.  was
going to go yesterday to make sure all was fine but didn't.  Got
there today and damn lights wouldn't come on in the half of the shop
I was going to the demo!  Got the ladder and checked out what was
going on with the wires and like.  fianlly figred it out.  I hadn't
turned the light swtich on! It's been so long since I opened the shop
and turned on the lights I forogt there were two light swtiches
and I only turned on one!  How stupid....

--

Thinking about physics and dot product.

The formulas I've found implied a moving particle creates a magnetic
field at the other particle which is basically B = v1 x r in direction.
Then v2 x B is the force vector to causes v2 to curve.

And we do this backwards to see how p2 effects P1.  But the two forces
that make P1 and P2 move I'm pretty sure need to be equal and opposit
in order to mantain contant monentum of the system.  And at first glance
these two formulas don't look like they would be equal and opposit.

I guess a simple place to start is code it and calcuate it for random
sets of two particles and see if the resulting force vectors work out
to be equal and opposit!

Then, if it is not equal and opposit, we could make it, by assuming
that if the magnetic field of P1 puts a force on P2, then we must put
the inverse force to P1!  And does the inverse force do no work on P1?

I bet it does work on P1!  Ug, but no, it can't!  if it does work on P1
but not P2 then we have an energy imbalance!  if there is no work done
to P1 then there must be no work done to P2!

Ok, so we force the inverse to blance montum.  Then sum the two resulting
vestors and then what?  They balance as equal and oppsot, but do they
do work? I bet they will!  Meaning magnetic field do in fact do work!
But they do it by pushing back on the source of the magnetic field!

And I bet, there's a simpler way to calucate that combined vector in
one equation!  LIke v1 x v2 sort of stuff.

Ok, so I think coding this is the simple way to go instead of struggling
over a few pages of math to try and figure it out.

================================================================================

2016-09-23 12:51 AM Friday

Yesterday

1) Still slowly trying to make progress on undersanding how to code
magnatism.

Did the school BGOP demo for the groups at the forge.  All went well.
Phil was in England so he couldn't do it this year.  He's normally been
doing it for this group.

Saw the magnificant 7 movie last night.  Western.  Cool stuff.

Today

Just posting on facebook about.

So, I think I have enough understanding of magnatism to try and do some
coding to confirm how it works or doesn't work...

Using the point charge formulas for defining magnetic field, and for
defining the force creted when a second charge particle moves through the
field, I can code that.  And I've decided, for now, that if P1 creates
a magnetic field that creates a forc3e on P2, that we must apply the
equal and opposit force to P1.  P1 is "pushing" P2.

So, in a large simulation, we are talking about adding matching forces
4etween every set of two particles in this way.  But we have to do it
both ways -- how the mostion of P1 creates force on the motion of P2,
but inversely how the motion of P2 creates force on P1.

Which is all odd, since the end result needs to be to find out what
the sum of those two forces are!  And treat it like one force that is
created between both due to the motion!

The math I a bit beyond my easy undrstanding, so I think th ebetter way
to get a grasp of this is to code some experment.

For example, how strong is this force compared to the electostatic force?

is it normally small compared to the electo static force?  Or can it be
very large at times when the particles move fast enough?

Maybe I could/should code some graphics to draw the force and veloctiy
vectors?

So lets jump into the code.

----

[4:11 PM]

Coded some tests.  Calucating magenet field B and force F that p1 will
apply to P2.  Fixed some coding bugs/typops, but still trying to grasp
if it's correct.

Using two particles, on the same x axles so y and z are the same (0).
Switching from Proton's to Electons only fkips signs. The different mass
does't come into play because only the charge counts and the carges are
the same but opposit signs. Two electons produce the same numbers as
two protons beucaes of the double flipping of signs.

Need to check if my vectors got the signs right and all correctly follow
the right hand rule.

Tried added Z and thought it would not change results. Fuck, now I see.
Adding X should not change reuslts (if they move towards or away from each
other it won't create force -- only moving sidewise causes the effect).
I got my axes confused.

Oh, right.  The movemtnt of P1 which is creating the magnetic field has
the velocity towards the other particle rmoved by the cross product. So
in my test, the x velocity of the particle crating the field, isnot
important.  Changing x doesn't change the B field created.  But when this
is reversed, the force IS affected, by X.  And I'm thinking that when
we create the invese and sum to get total foce as I'm guessing I must,
that all velocties will end up changing the final result!

----

Ok, code seems like maybe it's doing the right thing. But boy is this
all so amazingly hard to understand.

For creating the magnetic field, the motion towards or away from the the
other particle is factored out. But creating the force from the field,
motion in line with the field is factored out, but the field is 90%
out of line with rHat the vector to the other field.   This all ends up
being too confusing for me to visualize or intuitively undrestand yet.

There must be something intuitive here to understand somewhere.

Oh, and the force strenth is like 10 to 20 orders of magnature weeker
then the EM field strength! So it's going to have like no effect on
orbital dynamics unless we do very large experments I think.  But I need
to verify I'm using the right units and conversion constants in my code
first to veryify I've not messed that up.

Also, all this code is based on velocity only.  Isn't there a requiment
for magnetic field to be based on change in velocity???  Ah, but ok.
If FORCE is baesd on velocity, FORCE then translates to the change in
velocity.  So we get the velocity of one particle defining the change
in velociety of the other!  Or, in effect, the current in one place
defining the change in current in another.

I feel the need to ask WHY would the universe work this way?  It feels
like there should be a simpler or more fundamental answer hidden under
here relating how the position and movement of one particle effects the
position and movement of another.

Ok, ignore that for now, lets double check units to see if these numbers
are correct.

----

Ok, did a bumch of double checking and I think that what I'm seeing
with the magnetic force is correct. Units look corect as verified
through a few exmaples and checking units -- but there are not many
formuals for exctly what I'm trying to do so much is still sort
of unclear and uncertain.

This site as intersting iformation and new insight for me:

ttp://hyperphysics.phy-astr.gsu.edu/hbase/electric/elefie.html

Inlucind for exmaple:

c = 1/sqrt(u0 e0)

Relating electic and magnetic permablity.

Odd fucking stuff I don't fully grasp.  More relativity stuff.

The formulals I'm using need the U0 constant to convert motion to
magnetic field, to force to motion.  It relates how the motion of one
charged particle forces a changein the path of another.

So what the fuck does this have to do with c???

But maybe this is all consistent with my thinking that c is a fall out of
how kenetic energy moves though a collection of particles and where the
true force is instant, but that the transfer of kenetic energy is limited
by c.  So the f=ma is what causes the transfer speed to be be limited
then if velociety (aka kenetic enety of one) is related to velcoiety
of another, then it seems valid, that c can part of the equation.
Roughly. Without careful thought.

electrostatic charge is what makes capactiors work!

magnetic is what make inductors work!

This is cool way to understand these forces I didn't think of until this
web site. Even though I basically knew this for 40 years, I only made
the connection just now!

But again, this way of thiinking continues to make these two foces
seem to be totally unrelated forces in nature, when in fact they are
basically the same force in a way -- they are how electons (and protons)
push each other around.  There must be one simple formula to describe
it all insead of treating it as two sperate forces?

Is it our notion of nation being in a fixed state and moving though time
that is causes us to break it into the static force separate from the
magnetic foce?

Is the magenetic force, maybe just an adjustment to our bad understanding
of the static force to "corect" it ot be how it really works?

---

Ok, thinking of a EM wave as it's often described as electrostatic
and magnet fields moving though spac3e....  What if this only happens
due to propigation though fields of particles?  So, inteting question,
how much "stuff" is in space? What is the particle densoity of space?
Like between earth and the sun?

---

Answer, even in deep deep space, it's estimated to have a density of
about one hydrogen atom per cubic meter -- not empty at all!  And the
space in our solar system will have much more!  So still billions and
bilions and bilions of particles in the field betgween the sun and us
and evetything else to relay em radition!

So, to support this idea, I have to show how the math works out for the
spread of kenetic energy through a field of particles or atoms. Got to
keep that on the todo list...

But magnatism is just fucking weird and I don't really grasp it when I try
to express it in terms of two interacting particles like I'm working on.

The force is so amazingly weak, that for much of my simulation it seems
likeit will below the 16 digit resolution of floatin gpoinst!  And only
when near the speed of light, will it start to show much effect for
indiviual particles!

So, tomorrow, I'll have to add the magnetic code to the simlation and
see hot it changes behavior!  I think my approach to making the inverse
force applied to the "sending" particle and vise versa is valid.

And, then, can we somehow reduce and simplify the formauls so as to
make one calcuation of the paried force, between two particles, isntead
of two backwards calcuations?  If it were simple algreba I could doit,
but wtih the cross product stuff and vectors, I don't know if I know
enough to rewrite and simply the formulas into one!

I guess with the help of computer code, and guess and test to find it! :)

But now, soup for dinner and TV!  Watching Angel still. The gang figured
out Cordy was evil, and she just had the evil baby -- the woman eveyone
loves! I forgot about her!

[8:32 PM]

----

OTher big hanging unsnwered questions is frame of referece problems...

My formulas are maybe wrong due to not dealing with this correctly!

I think maybe, the B field created by a moving particles, needs to be
understood as moving itself!  And the speed at which the other particles
is "moving though" the b field, needs to include this face to get the
forces correcgt, and my current code is ot doing that!

The first article is moving, and creates a field, but I'm treading the
field as if it's standing still.

So, in my current code, two particles moving in a parallel motion,
create fields, and create force -- but they probably shouldn't!

Think about that tomorrow! And play with test code!

================================================================================

2016-09-24 8:44 AM Saturday

Yesterday

1) Wrote first code to calculate magnetic field and force to try and
understand how it works.

2) Found the magnetic field and force created by a moving electron is
MANY orders of magnatude weaker than the electostaic force.  In some
cases, more than the 16 digits of floating pont precision making it

3) Spent time to verify my units and constants were all correct in the
formulas to make sure the number calucated was correct.  Seems to be
correct, but it could use more checking.  This is all very questionable
beause none of the stanradard formuals you find all over physcis books are
set up to define how to electons interact with each other. They all define
macro level emergent behavior of large systems of electons interacting
(flux generated by an electro magenet, etc).  A single electon is not
the same thing as a current flowing in a wire.  The wire creates a
background frame of reference by which to measure against and a static
background of postive ions the electons jump between.  Not the same as
one electron in free space with no background ions or anything else to
establish a frame of reference effect.

4) Still have unsanswered questions about how motion is dealt with in
the formuals and my current approach produces very different results in
different inertial frames of referece which means it's wrong since the
frame of reference is a pure measurement fiction.  At the end of the day
yesterday decided that I should check out the idea that the magnetic field
of a moving particle should be thought of as moving with the particle.
This might make factor out the frame of reference, but at the same tine,
it makes the very concept of a magnetic field invalid if it must move with
the particle that creates it beucase then the magnetic field fiction is
a function of the arbitrary choice of how we choose to measure it.  So,
first, need to check out if doing that actually resolves the frame of
reference problem -- aka produces the same force answer no matter what
frame of reference we use. Then, if that's true, it means what we are
really calcuating is something simpler that doesn't include the pseudo
concept of a magnetic field.

Today

Push forward on trying to identify the correct forumauls.  And if I think
I might have valid formual for magnatism, add them to the simluation and
see how it effects motion of particles.  Might be so small as to not
be able to see any real difference until I simulate large sets of atoms.
But it might change small effects in important ways in terms of how
ep pairs syncrhnize -- or what the orbit of one e and two p looks like
and how well they glue together!  Which would be key and imporant if
it helped that!

Useful comment from Tesla wikipage:

"A particle, carrying a charge of one coulomb, and passing through a
magnetic field of one tesla, at a speed of one metre per second,
perpendicular to said field, experiences a force with magnitude one newton,
according to the Lorentz force law."

This makes the units clear for the force as F=qvB.  So I know I have
those units correct in the formula since I'm using coulombs and m/s.

But the calculation of the field strenth is not explained by that.

The field strenth is u/4pi q v X r / r^3  or rHat/r^2

Ok, now that I look at this, WTF!  If the magnetic field is defined by
the motion, how is vXr needed?  vXr factors out the velocity component
in the direction of the point we are calculating the field strength.
So which part of the veloctiy gets factored out is controlled by .... ah
no it's not as strange as I thought beuase it's controlled by our choice
of 3D grid which defines the point as well as the velociety relative to
that grid.

Ok, but we factor out part of the velocity and ignore it. We ignore all
velocity in the direction of the distant point.  So moving towards and
away from the point doesn't change the force we create on it.  But on
the other side, moving towards and away from the source, DOES effect the
force.  Which is non-symmetrical so it's wrong.  Or, it's hiding a deeper
true meaning.  These equations MUST reduce down to a simpler symmetrical
formula when we have to electrons interacting or else it's invalid.

f=qvB

f = qv u/4pi qv x r / r^3

f = u/4pi q1 q2 v^2 / r^2 (just more units check)

Coulombs law for force:

f = 1/(4pi e) q1 q2 / r^2
f = 1/(4pi e) q1 q2 / r^2
f = uc^2/4pi q1 q2 / r^2

c = 1/sqrt(u0 e0)
c^2 = 1/u e

Ok, so coulombs law of force and the magentic force calcuation ends
up being very simular, but the firist is using c^2 where the second us
using v1*v2 of the velocity of the two charged particles.

Well, isn't that odd?  To calucate the static force, we plug in a
velociety of the speed of light (in a way).

It really feels like there is a hidden general forumal here, and that
as we have it split between static and magnetic, that the magnetic is
just the fudge factor to correct the error in the static.  And seeing
that this works out to c^2 it makes we wonder if the static is similar
to moving at the speed of light and the fudge factor is the adjustment
for it moving slower????

Or, in the way the claculation for magentic works out we are ignoring part
of the velociety vectors.  Is the part of the velociety we are ignoring
the part that would explain the static force if we didn't ignore it???

And why is it all done this way?  I would assume beuase in real material
we tend to have all the static forces cancled out with equal numbers of
protons and electons leaving only the non-static forces to measure when we
measure the effects of current and movement to define the magnetic forces.
So historicaly, we have looked at the probnlem from that direction.

So, my goal, will be to try and reduce all thse equations down to one
that explains all EM force in one symmetrical frame invariant equation
for how the particles interact. Or, one that explains what the total
force vector is.

Ok, so must push forward with trying to get the magnetic forumals on
their own correct -- aka frame infairant and symetical.  Then combine
them to one big total force equation and see if I can simplify.

Will play with moving mag field concept and see what that does to the
calucations.

----

OK, very cool.  It works.  Or at least, it makes it inertia frame of
reference invariant if I consider the velcoiety that p2 is moving through
the magnetic fieled to be v2-v1 instead of just it's own v2.

And this is really odd because even though total force is now the same
as I add and suptract random relative velociety to switch the inertal
frame of reference, the two magnetic vields are very different in each
inertia field of reference!

Which just shows that the magnetic field concept is an odd fiction.

I've seen multiple reference that said whether it looks like an E field
or a B field depends on frame of reference. But I thought that was more
to do with relativity.  This seems to be simpler because there is no
speed of light limit at work in my equations.  I don't think.

So, two particles in parallel relative motion have no force between
them, which makes sense and seems logical.  So, even though wires pull
together with parallel current, it's NOT the current that is pulling them
together! It's the reltiave motion of the current against the background
postive ions that does the pulling!

But, just because this makes the system frame of reference invariant,
is it actually correct???

----

With this new relative code, the velocity of the two particles moving
towards each other has no effect on the total force!  The odd lack of
symmetry is gone!  As it logically should be! (EDIT: NO NO NO)

So, with my two particles on the same x axis alightment, I should be
able to assign randome value to the two x velocity components and it
won't change the total force answer!

---

NOT TRUE.  Ok, changing the x velocity effects the y and z force but
not the x force!  So my first test above, y and z force was zero, so
changing x didn't change y and z still zero.  I thought this meant x
wasn't important at all. But it most certainly is.

So, I've set it up to assign random values to all 6 velocity vectors and
cacluate the total force, then randomly adjust the velocity to change
to a different inertial frame of reference.  And the total force is the
same every time.

Also -- doubling the velociety makes the force 4 times as large!
so force is v^2. Which seems consistent with other formula.

what if I double all the v components at the same time instead of just
one?

----

When I change x velocity, the x force stays the same, but y and z cchange.
My particles are allgned parllel to the x axes.

When I change y velocity, the Z forces remain the same, and the other
two change! Fucking intersting.

And as expected, when I change z velocity randomly, the Y forces remain
the same, but the x and z change!

And, changing inertial frame of refrence, while doing the above random
change of one axes, has the same effect on the results.

So, this means, looking at relative velocities, we know that when the
particles are aligned parallel to the x axes:

only vx and vy are used to calculate fy

only vy and vz are used to calculate fx

only vx and vz are used to calculate fz

Ok, so if only the relative speed they are passing each other is used
to caluculate how much they pull together or push apart, we should be
able to rotate the vy vz vector to point in any direction, and have
it produce the same fx force!  Logically it should work that way since
orrinention of grid is again, an measurement fiction.  lets check.

----

Rotation of relative xy changed all force values.  Oh, but shit, that's
not what I was wanting to code.  Wanted to code vy and vz roation,
not vx,vy! Try again.

--

YES!  So randomly change vy and vz on the p1 only, so that the relative
vx vy vector between the two particles is randomly roated but keeps the
same magnatude, causes fx between the two aka the force straight between
the two particles, to stay the same!

And if we double the magnatue, what happens?

Yes, when we rotate and double the velociety magnatude, it does a v^2
effect and makes the force 4 times as large in the x direction!

And when we do random inertia shift at the same time, no difference.

---

What about the total force magnatude???

Yes, fucking cool.  When we random rotate vy,vz vextor, fx stays the same
and fy and fz change, but total force (magnatude of force) stays the same!

Which is all like we want because gemeticlaly, if these are the only two
particles in the univese, direction is undefined against any background
so the only thing that should be able to change, is the direction of
the motion, not it's magnatude.

But when we rotate other vectors, it does change relative to the only
two particles in the system. So everthing changes!

So yes, when going back to only roating vx,vy, everytghing chagnes.
Total force changes as well as each fx,fy,fz

---

Ok, it seems like this is working very well.  At least it has the right
behavior under inertia frame of reference shift, and the logically correct
symetry effects, so that only what is relative to the two paticles can
effect how the two particles interct with each other.

And only inertal of velociety, and locaiton, can effect it!

I think maybe the next thing to do is write out the forumaul for total
magnaetic force, and see if we can simplify it!

Then, total force, combining magnetic and electostatic, and see if that
can be simplified...

And try to code ths formula into the simatuion and see how it changes
behavior (I can run parallel runs that start the same but one that
includes magnatism and one that does not and see how they differ).


-----

[2:03 PM]

HOLY MOTHER OF GOD!

Changing the distance between the two particles has NO EFFECT ON MAGNETIC FORCE!

HOW THE FUCK?  Did I mess something up???  There's an 1/r^2 factor in there. Did
I leave that out by mistake?  Or did the new formual end up canceling out the effect?

Paricles 1 angstrum apart have the same force on each other that are 1000 light
years apart!  if this is true.

Let me check the code and see what I messed up...

- 

YES I FUCKED UP.  I left out the 1/r^2 term!!!!

But fucking cool!  When I added it back in, the magnetic force is the same order of magnatude
as the es force when the velocity is close to the fucking speed of light and the distance
is close to 1 Angstrom! When somewhat less than the speed of light, it's smaller!

This means that magnatic force will make real changes to orbital patterns when the velocity
is near the speed of light! Which might, have an effect that prevents the ep pair from
going faster than the speed of light!  It might force them to fly apart into higher and slower
orbit when they get near the speed of light!!!!

This is exciting shit.

What will the orbital behavior be like when they get near to the speed of light!!!

Well, I need to cut the grass.  Should have done it yesterday.  Then there will be
food issues.  Then TV.  Might not get back to this until tomorrow!

[2:25 PM]

----

Played around with forcing  the relative velociety to be the speed of light.

When I do this, the total force always seems to be less than the es force, but yet very
close to the same at times.

This makes me beleive, that the max magnet force can be, is equal to the es force,
if the velociety is in just the right relationship -- maybe 90deg to each other?
Or 180?

Which seems possibly consistent with es being based on c^2?

--

And this is not related to distance!  That is, forces are much smaller when the particles
are futher apart, but the fat that mag force is the same as es force, holds true even
when they are far apart as long as the speed is near the speed of light!

So, very cool.  How does this all effect orbatial behavior!!! That's the huge fucking
question...

================================================================================

2016-09-25 8:25 AM Sunday

Yesterday

1) Changed code to treat magnetic field as moving with the particle
that creates it.  So when the other particle moves through the field,
the velocity we use is the relative velociety of the two working together
(V2-V1). This seems to have fixed all the frame of reference problems.
At least, the system now acts indidpently of the frame of reference --
which I take as a basic logical requirement.  Particles shouldn't move
in different ways must becuase of my choice of scale and units and frame
of reference!

2) With above change, and with dual calcuations of p1 magfield on p2
and p2 mag field on p1, and summing the forces, we now have a total
force that is totally indepdent of frame of reference!  As it must be!
Two particles moving in unison, have no magnetic force between them!
This feels correct.  Logically. But I have no real clue if this actually
fits realitly.  It feel right, but more testing is needed.

3) Found I accidently left out the 1/r^2 term!  Which made force indepdent
of distance between particles!  And when I fixed that, something cool
showed up!  The mag force is no longer orders of magnatude smaller than
the electostatic force.  When the particles are moving near the speed of
light (3e8) the magnetic force and the electostic forces are approxiamatly
the same!  Maybe under the right conditions, exactly identical?  And this
holds for all distances apart!  So particles moving near the speed of
light will most certaonly have very different orbital paths!

4) Noticed fascinating effets of how velocty on different axis effects
total force on different axes.  I have the two test particles on an axis
that is parallel to the x axes.  In this configuation, only the velociety
on the Y and Z axes has a force effect on the X axes.  X velociety has
no effect on X axes force!  So only relative sidewise motion controls how
the two particles pull together or push apart!  Likewise, intersting
relations like this work for the other forces as well.  Only X and Y
control the Z force, only X and Z control the Y force!

5) All the above is done using simple test code to calculate the force
between two particles.  Not wired into the simuation yet so I haven't
seen how these forces change orbital dynamics yet.  Still trying to
play with the formuals to make sure they are correct and to fully
understand them.

Today

Going to movie this morning with Pig. Pete's Dragon.  Later.

I stil want to get a better intuative feel for how motion controls force
with these formulas.  So I have a basic instinct of how particles should
interact.

Also, I want to try and reduce the math down to a simpler formula for
total magnetic force instead of this two step process for calcuating
B then F in both directions and summing them.  Seems like it should
reduce to something far simpler  Certainly, it should only be based on
relative velocity.

Then also try to combine electrostatic force and see if the whole
thing can be reduced down to a simpler form -- instictively, it feels
like the univese will REQUIRE it to be possible to simpliy it down to
one formula. Our abstraction of magnetic field being different from
electostatic feels to have happaned only due to how the magnetic field
tends to manafest itself probably as orbital behavior of atoms.

But overall, what we are talking about here is now the position and motion
of one paritcle, controls the motion of all the others.  or when both
particles are the same like two electons, how they choose to "dance"
around each other!  Logically, if feels like this dance must be one
formula not two separate sets of rules.

And of course, how magnetic field is conceptualized in this math, is
just crazy beuase the magnetic field changes based on our choice of frame
of reference -- which means it's not really there at all. Our choice of
measurment frame can't control what the unviese does.  How the neruons
in my head fire can't control how atoms interact.

Magnetic fields created by magenets of course are very real.  But they
are created by some relative internal motion of particles in the magnet.
The physical structure of the material defines the frame of reference
that we use to understand the "magnetic field" from.  Once the problem
is reduced down to how two individual electons intracet, there is no
seprate frame of reference defintion anymore which is why the concept
of magnetic field becomes bogus -- and why at the core of this problem
the entire abstract concept is basicaly wrong.

--

On to a better intuitive understanding..

If two electons pass in space parallel, do they push apart or pull
together?   What happens as they approach, and as they leave?

I think they should pull together. (opposit if what parallel current
in wire seems to do).  Same should be true for two protons. But a +-
pair should push apart.  So, this force of motion is acting AGAINST the
electrostaic force!  If this is true...

---

When two protons fly past each other at 1/2 the speed of light, so
that their relative velocity IS the speed of light, then the X force is
EXACTLY EQUAL to the ES FORCE!  That's fucking cool that the math works
out that way.  But is this math correct? Still no clue.

But, the signs of this?  I don't know.  Let me check...

But I'm expecting that protons push away, and mag force should
pull together?  So that at the speed of light the froces cancel out?
That would set the system up to create stable orbitals.  But my current
math looks like it's working the other way....  That the force would
just doble at the speed of light...  Ugh, that doesn't sound useful for
where I'm hoping this will lead...  Let me look closely at the signs...

---

ES  Force between p1 p2 (-2.3070872449372317e-08, 0.0, 0.0)

Mag Field B on p1 created by p2 field (-0.0, -0.0, -240.16074017921997)
Mag Field B on p2 created by p1 field (0.0, 0.0, -240.16074017921997)

Mag Force2  on p1 created by p2 field (-1.1535436224686162e-08, 0.0, 0.0)
Mag Force2  on p2 created by p1 field (1.1535436224686162e-08, 0.0, 0.0)

Total2 on p1 (-2.3070872449372324e-08, 0.0, 0.0) 2.30708724494e-08
Total2 on p2 (2.3070872449372324e-08, 0.0, 0.0) 2.30708724494e-08

---

In above, we see totel ES force as

-2.3070872449372317e-08  ES Force
-2.3070872449372324e-08  Total Mag froce

Notice one ends in 17 and the other in 24

Wonder where this difference comes from?

Ah, the speed of light is now a defined constant and meter and second are
defined from it.  So e and u and k all relate to each other from this
constant I think and I probably have a slight difference somewhere.
If not just accumulated rounding errors in how the math works outs.
I'll track down why this small difference later.

---

Oh weird.  First -- the signs check out as INCREASING the force!  So two
Protons that normally push away, push HARDER when they move past each
other!  I assume same is true for two electons, and inverse for electon
proton -- but I need to check to be sure.

I also don't trust this.  This is baesd on current flow logic.  And I
wonder if current flow is actualy backwards from what free moving
particles?  Ah, but no.  Remember.  I decided the reason two wires pull
together when current flows the same way, beuaes the electons have no
effect on each other due to moving in parallel -- but the elecotns moving
opposit the protons pull together! So this might be all right.

But here's the fun thing.  Electron at 0,0, second one at 1,1 offset at
45 deg to each other.  First is moving straight up(+y), second is moving
straight down (-y) -- so they are appraoching each other, but have not
alligned yet.  The X force from the magnetic field is exactly the same
as the ES force -- that is pushing at a 45 deg angle!  So at a relative
speed being the speed of light, the mag force is exactly the same as
the ES force. But the ES force operates in line between the two.  And the
MAG force operates perpendicular to the motion, but in the same plane
as the R revector.  At least in this case.

But due to the need for symmetry, I'm thinking it will be this way in
all cases.  which means, mag force is always operating at 90 deg to
motion!  Shit.  I thought I concluded it didn't do that in all cases?

But if that IS what it's doing, then this becomes easy to conceptilize.
of light, the mag force is exactly 

----

Yes, confirmed that switching between electons and protons the math all
works as expected.  Just flips signs.  So matchin ++ or -- will push
apart, and push apart HARDER when in motion.  and +- will pull together,
and pull together HARDER when in motion.

This seems like it would create odd postive feedback for orbits instead
of negative feedabak to make the system find stable points.

Maybe for reasons I don't undestand, the sign is backwards?

But according to documentation on the internet, and right hand rule I've
got it right.  Claims Current flow, or qv flow (charge time velocity)
works the same way.

Ah, but that is the rule for a particle does IN a magenet field.  Maybe
genertaing a magnetic field is backwards for charged particles?

---

Ok, when two particles approach each other, there is no mag force!

So, if we look at the relative velociety, translate to the idea of one
standing still and the other moving, then the mag force is always 90 deg
to the motion, and parallel to the r vector and equal in size to the EM
relative to c???

When heading straight together there is no 90 deg, so the fact it falls
to 0 force makes some sense.  But it should change in force relative to
how much it's in line with the two and my other logic suggests that's
not true.  So more testing needed to better understand!

---

Question -- if I just flip the sign of the mag field, does it change
the cauclations and produce different numbers, or just flip the sign of
the answer?

----

[2:13 PM]

Back from movie.  wasted time on facebook.

yes, flipping the sign of the magnetic field does just flip the sign of
the final force for the magnetic field.

There might be a reason that the for electon to electon (particle to
particle) we need to do the invese the sign of what seems to be normal
for an electron magnetic field.  But I don't know what it is.

So for postive current flow, they right hand rule applies.

----

Yes, digging on the internet, it's clear that the right hand rule applies
to postive current flow for direction of fulx in an electomagnet.
And that this same direction, runs outside the magenet from north to
south poles. (backwards inside I guess).

And the earth if a magenet with the S at the north poor and N at the south
pole so that a magents north pole in a compas will point to the north!

But is current flow different than electon/proton flow?  Eletron flow
in a wire, is what current really is, and that produces left hand rule
flux.  But for current to flow in a wire, postive ions have to logically
be "moving" the other way -- even though they don't move.

But the act of an electron jumping from one atom to the ion next to it,
turning the atom it was in, into an ion, has effectively made a postive
charge move opposit of the direction of the electron flow.  But the "hand"
rules give us the same field direction either way -- whether we look at
it as negatve charges moving to the right, or positive charfges moving
to the left.  How could be end up needing to be backwards?

--

Ok, so this page:

http://academic.mu.edu/phys/matthysd/web004/l0220.htm

Claims two protons moving side by side, have a magenetic field between
them.  And they just use the velocity of the parallel motion to calculate
it.  But I say, there would be no force because the relatlve velociety
is zero.

I strongly suspect (wonder) that physics think that current in a wire
is the same thing as moving point charge, when it's not.

So if you have a stream of electyons in free space, the way they would
intereact with another stream of electons, might not be the same as
how they react when the electons are a current in a wire!

CRTs use electo magnets to bend the electon beam.  So there is no dount
this effect is well studied -- how an electomagnet creates a field that
will beend the path of a charged particle, either a proton or electron.

But I wonder, have they have tested to electon beems in free space
interacting with each other?

Something I read said they "can't" test electons becuase the elecrto
static foce dominates the test until you get the electon up near
the speed of light -- and they can't get the electons moving anywhere
near that fast I think.  So at the speed they can make electons
move with milions of dollars of specialized hardware, I think they
can't test how the magnetic field is working!

I feel the entire concept of a "magenetic field" is in deep question
here!

So maybe, when electrons move through a wire, the magnetic field is
inverted from expectations, of current flow, due to complex interactions
between the moving electrons and the atoms.  Maybe the electons force
the atoms to produce the field, and the field it produces ending
up being inverted?

If no one really understand atomic orbital dynamics, then I think this
is not out of the question.

So, 1) they assume electron motion of current is producing the field, when
it's not, and 2) they have never tested the magnetic interactdion of
free electons to know it's backwards from what they think?

It's hard to justify that they don't have a clue about something so
totally studied as electo magnatism, but maybe???

So, I can certainly just test it both ways and see if one produces
intersting orbital dymaics and the other fails.

If neither produces intersting orbital dynmaics, then I don't have the
right answer!

----

Ok, I'm going to leave the code backwards -- I added comments to indicate
it's inverted from the way the physics books all say it should be.

But, this leaves the entire idea of current flowing in wires causing them
to pull toether backwards from expectation.  The parallel flowign electons
have no effecvt on each other at all! And we can no longer argue that
the electons flowing in one wire is attracted the reverse current of
the postive ions. Because when we flip the sign, as I have just done,
it m eans it should push those away!

So, to accept this backwards sign, we require complex interaction between
the flowing electons and the atoms or the nutrons etc to justfity
how electo magnatism works -- to flip the magnetic sign produced by
the system!

Something like...  Electons in orbit, create a mag field one ay, that
is counteracging the mag field created by neturons?  And once they get
ripped out of orbit to flow from atom to atom, they stop their interaction
and allow the netron or protons they were balanacing, to dominate the
creation of the mag field which we attribute to the moving electons wbut
which is not actualy comming from the (slow moving electons) all?

So, the moving electons don't crfeate the field, making them move makes
them STOP creating the field they were creating?

Ok, something like that could be happening, but it's hard to grasp how
physics got this all so wrong and never noticed the error.

All the complexity has to be hidden in the orbital dymaics.

But them again, if my theories are correct, there is no particles smaller
than protons and electons, and the effect they call spin they attribute
to them is false -- again, something that is created by orbital dynamics
and not by the electon itself!

And if they have that wrong, basically evertyhing they think they
understand about atoms and particles is falwed.

----

What next.

Ok, trying to better understand how what I have coded works.

What if I rotate the axes, to force alignment  of r to the x axis.
Then ajdjust relative velcoiety to this new rotated axes.

Is the x compomnent of the velocity always pointless to the force
at this point?

Wait, even without rotation -- my test always has x aligned with
two particles. And haven't I already determined that x is not
pointless?

Oh, yes.  Right.

If we normalize the vecoeityh to have one particle standing still
and the other with all the motion, then that's the relative motion
vectgor. And what I think I understand now, is that the mag force
will always be perpendicular to that relative motion!

And the x axis velcoiety is significant to which way that vector
points so it will always be significant. There is never a velociety
dimension that is not significant.

But, when the velocoiety is stright togethe ror apart, the mag force
is zero. What is it when it's almost straight together?

----

[5:01 PM]

OPPPS...

I just relized what seems to be a problem here!  My thnking may have
been broken!

My current calcuation makes force independent of absolute velocity!
It's a function of only relative velociety.  But force defines how
velcoiety changes over time!  So a particle going 10 mph needs to have
a -force of 10m/s/s to make it stop in a fixed distance.  But a particle
going 20 mph needs twice the force to make it stop in the same distance!

This means, when we change our inertia frame of reference, we need to
change force to make the particle take the same path ???

Ah, but maybe I'm wrong about being wrong!  In the new frame the path
will be different if the speed changes but the force doesn't.  But since
we are in a different frame, the path needs to llook different, so that
it's absolute location is the same?

Ok, I think I need to stop and figure out if my thinking is correct or
not. If the forces are the same, do the particles take the same path in
location, no matter what inertia frame of reference we apply the force to?

Ok, some quick exmaples seems to prove me wrong about being wrong.

The fixed forces do mean that in the different frames, the paths will
look very different.  But once maped back to the same frame, the paths
will still be the same!

So fixed forces IS what we need and what we got with the current code!

I might not have proven this as strongly as needed, but it feels like
we are still on the right track.  So I'll ignore it but remember it
as a back burrner question if things don't work.

----

[5:44 PM]

Ok, so next idea.  Can we simplfy the code.

An intuitive understanding is emerging.  If I translate the velociety to
a frame where one particle is standing still, and all the velociety is
in the other. Then the force created will be perpendicular to that
velociety vector.  And if we project that velocity vector onto
the plane that is perpendicular to r, and divided by c, that is the force
vector relative to the ES force.  No, this is not correct...

Ok, so if we move V to one relative value, then Vx(Vxr) seems to be
the force vector direction.   lets work out the math.  We an assume
of the double mag vectors, we can just move the v to one, and consider
the other zero, and just calculate the one way to find the answer!

----

Ok, cool.  The total magnetic force reduces down to:

F = u0 / 4pi q1 q2 v X (v X r) / r^3

Sign determined by which way r points.  Sign I'm not sure of.  But the
"broken verion" I want to try has r pointing for other back to me.
But the book way, would suggest r should point to the other particle.

And note the prefect parallel with calucation of electostatic force as:

F = 1/(4 pi e0) q1 q2 / r^2

Which rewrites to:

F = u0 / 4pi q1 q2 c^2 r / r^3

Which means total force ES and MAG is something like:

F = u0/4pi q1 q2 / r^3 (c^2 r + v X (v X r))

And I feel the vXvXr is really just trying to factor out the first part
and leave what is left to be the magnetic field?  Or something like that?
I feel this can be reduced to something even simpler for total force.

c^2(1 - v^2/c^2) ???

c^2 - v^2 ???

Ok, let me play with that tomorrow.

Let me add this total force into the simlation and see what happens!

================================================================================

2016-09-26 11:06 AM Monday

Yesterday

1) Reduced calculation of mag force to one simpler formula by combining
code that calcuated it in both directions and sumed.  Found that there is
a very strong parallel to the electrostatic force formula.  But mag force
uses VxVxR of the two velociety vectors and electrostic vorce uses c^xr.
As if they were moving at the speed of light.

2) Struggled with sign of force.  Books all indicate it should add to the
ES force as the particles move faster.  Doesn't sound correct.  So I'm
experpimenting with flipping it so the mag acts against the the es force.

3) Added mag code to simulating!  And started some test runs.

Today

The 4 test runs have been running all night.  Three of them just have
4 particles each with two + and two -.  With mag force turned off,
they spread apart but settle into similar orbits.  But with average
KE of 33%/66% in the two electons but it's drifting up and down.  Of,
they tend to synchronize sort of.

With mag force on using inverted mag field, one electon flew off near
the start but in time, the two protons drifted apart and electon found
it's way back to the empty protron and set up orbits.  But the orbits
are fucking weird.  The electon falls to the proton, then shoots out at
a near 90 deg angle.  So then falls in and shoots back out the first way.
So it's an alternating 90 deg orbit.  I can't tell what happens when it's
close to the proton -- too close to tell.  Both pairs fell into this same
orbital pattern, and both pairs seem to have this pattern aligned with
each other. Energy balance is like 43/56 -- closer than without magnetic.

Due to the 3D nature of the simuation, I can't get a good read on
what these angles or orrientations arcually are on my 2D display.
Byut these are out of sync so when one is all the way out, the other at
near the proton.

---

Thrid simulation had mag force sign matching physics books -- which means
the mag force adds to the es force.  This caused one pair to fall into
a super faster orbit and take 99.9% of the energy of the system.  it's
velociety was >1e9 -- over the speed of light.  And, instantly inside the
RLimit of the simulation so it's racking up constant simulation errors.

This makes sense becaue the faster the orbit the more the two particles
will attract when the math works that way causing them to dive down into
the simulation error range.

Real live can't work this way unless there is some other effect to
offset it -- like relativity adjustments where the mass gets higher
and higher as it goes faster or something.  But I'm trying to find a
solution where relativity is an emergent property, not fundamental --
not codeded into my simulation.

The second pair are in a very slow orbit.  Due to the how fast the
simulation has to run for the first pair, I can't see how large the
second orbit is.  I'll hve to wait a long time.

--

A forth simulation had mag on and inverted sign.  5 p and 5 e.  They have
ended up for 4 in orbits, and one electon flying wild.  But the energy
split for the electons are 0.2/75/13/10/0.03 Which is odd becuase 4 seem
to be in close orbits not just 3.  I don't know which  is which.

But interstingly, the protons have not spread very far apart, they remain
clustered somewhet near the center of the screen. 0 inside Rlimit errors,
but 81 bounces.  Probably all electon bounces.  I should track ebounce
seprate from pBounce!

But now the energy is like 0/91/4/4/0  so it's drifting.  But looking
at the orbits of the 4 are close to being in sync.  Frequency of orbit
seems to be the same accurate to about 25%?  like 10hz vs 8 hz wild guess?

Maybe small difference in orbit make big difference in total energy now
when it is near the limit of what the system allows due to the speed
causing the orbit to come apart?

I think with this magnetic balancing effect it will limit the max speed
of the orbits allowing some to fall down to this lower limit, forcing
extra energy in the system to flow into one wild one?

Ah, but the wild one here has less energy?  WTF?  Could it be in an
orbit in that case?

Conservation of energy does strange things I have trouble understanding
since the spacing of particles also effects changein potential energy.
When things spread apart energy has to adjust.  But do matched pairs
basicall cancel out in that case?  I don't fully undestand.

energy is now 2/80/10/10/0 the last one is the wild one for sure.

Oh, and the orbits of this simulation do not show that odd 90 deg pattern
thta the two pair test is showing.  The orbits are normal.

But I wonder if this is a total energy issue -- not as much total energy
per pair so the orbits are forced to be larger?

Total energy in the system might make a real difference now that there
is an effect that changes with speed. Before, it seemed to make no difference
because the system was the same at all scales.  Simulations at 1/0000
angstroms worked the same as at 1000 angstroms.  Just ran faster or slower
per size but the patterns of behavior were the same at all scales.

But with this mag effect that gets stronger as we appraoch the speed of
light the patern of behavior will most certaonly change at that speed.

--

Also -- most the simluations running with the mag effect an inverted sign
stayed below the speed of light -- but the two particle one would just
go over it a bit when in the tighest part of teh obrit -- like 5e8 sort
of speed I think.

---

Time to make coffee. I've really cut back on my starbucks habit. And
i'm eating less.  Weight is down to 224 range from 230 range but
seems to be stuck in this range for now.  Need to get more exercise.
eating less doesn't help if you never get out of your chair and go
places and do things!


----

I don't understand this.  Two electrons from the simulation with 5 paris:

e8 vx:  2.67e+05  vy: -1.29e+06  x:   5.12300 A  KE: 22.94%

e9 vx: -2.46e+05  vy:  1.62e+06  x:   5.56936 A  KE:  0.02%

Their speed in x and y are nearly identical, with e5 and e6.  But the
kenetic energy is 22.94 vs 0.02???  The smaller KE one is the one electon
that is flying free.  Output doesn't show vz.  So that might be part of
it. But all 5 of the electons have this same pattern of spee din the e5
vs e6 range, but yet total KE is so different?  How can speed be on unit
of magnatude different and KE by 3 units of magnatude different? Well,
it is mv^2.  Maybe that v^2 is the real issue here.

Maybe showing relative velocity difference would be more useful?

Or, monumtum!

----

And now the weakest electon of the 5 pair test has gone into a higher
orbit that's 10x maybe larger radius than others.

Well, lots to play with here.  But I want to get back to the basics of
the formulas before getting too distracted on orbital dynamics.

First, quick mode for eBounce vs pBounce counts!

----

[2:22 PM]

added eBounce and pBounce.  Then got lost on facebook and email.

Another quick hack idea.  Fix monuntum after a bounce to return system
to zero.

----

Ok made call to reset monumtum to zero after every bounce.  Monumtum now
hangs around the e-36 level (16 digits below total energy).

----

Running a test with 6 protons 1 electon. The 1 electon seems to be holding
toether two of the protons while the rest fly apart.  Wonder how long
it will hold them together?  That's the sort of careful testing that
will need more attention later.

But I want to return to the concepts and formulas for a while first.

It's unknown (to me) if these new magnetic forces are messing up total
energy!

Does the trade off between potential and kentic still apply now that
we have forces based on speed not just distance?  Potential energy is
cauclauted using distance which is what defines the force.

My error corfrecting code is keep total energy fixed, but it could be
breaking the system in process.

A way to test effects to turn off the error correction and see how fast
errors add up with magnatism turned on or off.

The entire error correction appraoch might be invlaid now.  Or might
need something more complex to account for the magnetic potential energy?

The idea that magnatism can't do work, I think is invalid.  If it can do
work, then I think we should have an energy problem. If in the end, it can't
do work, we should be OK.

----

Ok, a little thinking makes it clear we have a problem with total energy.

The magenetic force is most certainly not always 90 deg to the r vector.
Which means it's doing work. Wait, is that correct?  It most certainly
is correct that the mag force is not 90deg to r, but is that important?

Mag force is 90 deg to velociety vectory all the time.  But the matching
opposit force that must also be applied, is doing work.

Ok, if we don't apply matching opposit force, then we lose conservation
of monentum in the system.  And taht can't be true. There's no way the
magnetic forces and blantely violate conseration of momentum.

But, it can all change in subtle ways under relativity.  Which I still
don't grasp.

So far, I don't understand how relativity falls out of EM.

So, far, I seem to have equations that hold true under newtonian truths.

And, interstingly, the electons have trouble going faster than the speed
of light in orbits under these rules -- but some seem to have managed
a short stent above the speed of light.

Ok, faling to understand relativity or where it comes from still, lets
shift attention.

Why on earth is the speedof light so strongly wired into these equations!

How the fuck did that happen?  How did it happen that the electrostaic
force equation is:

ma = f = c^2 1e-7 q1 q2 rHat / d^2

Since the second basically comes from earth roation and meter is just a
random humnan sized measure, the only thing that is left to explain how
c got into this equation is that it must be part of how charge is defined.

The unit of charge must be defined in terms of the speed of light.
if 1/c was the fundamental charge unit, this would fit the two q1 12
charges matching against c^2.  But odly not as easilly against the 1e7
becuase that forces a sqrt(10) factor into the unit of charge as well.

But why was that done?  let me see if I can find something to help me
understand this.

----

So, coulumb is defined in terms of current. They say. And current is
defined in terms of magnetic force needed to create one Newton of force
on two wires one meter apart, each carring one amp of current.

Newton is the defined f=ma from the defintion of kg and m.  So it's a
constant that falls out of the innate inerta of mass, and how it reates
to time and and space (who's units are randomly chosen for human scale
convence).

So then this innate measure of inerta, is used to define current, that
ends up defining charge.

So units of charge falls out of inerta of matter in the unviese and how
it relates to the correlation between magnatic force and innerta force.

Or, how a moving charge, makes a mass move..

But it's not a moving charge. It's how a current in wire makes a mass
move!

Then somehow, this same unit of force was used to measure how a static
charge makes a mass move though the electostatic charge.

Fuck. Now I really wonder if anyone actulaly knows how a moving charge
makes matter move!  Maybe all they really know how how an electic
current makes matter move!

But they do know how a static charge moves matter!  That we can measure.

--

Did I take my vitmans this morning?  Shit, I don't think I did, but yet
I'm not sure?  I'm going to take them again.  But maybe I need a better
way to track this!

Will start recoding in my notes when I'm workong on notes!

Just now took them. [5:47 PM]!

--

Ok, back to units mystery.

So newton, it the force needed to make on kg change in speed by 1 m/s in
1 second.  So it's a fundamental apsect of nature based how mass reponses
to force but defined in the arbitray units of kg/m/s

Then from there, we come up with current, as a funamental aspect of how
mass motion responds to current flow.  But how current makes mass move doesn't
have to be how force makes mass move.  But we are linking the two
on the assumption that everthing that makes mass move should be the same
units.  And we throw in 1 meter of wire, into the mix randoly as well.

So we have current defined as how much current flow in one meter of wire
is needed to make mass act the same.

and current is charge past a point.  But that doesn't tell us how much
charge is in one meter.  We don't know how fast the charge is moving.

So one A is defined in terms of force.  And a C is an Amp second.

So amps are electons per second -- with units adjusted to be in terms
of kg/m/s N.

So it relates to how moving electons in a wire makes mass move in this
special 1 m vs infinite wire configuration.

of electons or measure of the charge of one electon?  Ah, no, it's really
just the measure of the NUMBER of moving electons.

So C is just a count of electon, but in units that relate to how this many
moving electrons, in this 1m vs ifnite m set up, creates the same force
that makes 1N worth of change in movement.

Or, it measures one electon, as the force it creates when it's part of
1 amp of current flow in 1 meter of wire in both directions.

But them oddly, the static force equation falls out from all this as:

ma = f = c^2 1e-7 q1 q2 rHat / d^2

The fact that we can define c using force in current flow terms, and then
it falls out like this in static force, means that static charge force and
moving charge force are highly connected to each other for some reason.

Oh, wait.  q is A s.  It's the number of electons that pass a point in
ONE SECOND.  So we use one second to define q.  And F is kg m / s^2. There
might be a possiblity of seconds used in force to cancel out with seconds
used in c.  Maybe. I don't know.

Lets look at units and see what we can find

ma = f = c^2 1e-7 q1 q2 rHat / d^2
kg m/s s = f = m/s m/s 1e-7 A s A s / m^2
kg m/s s = f = 1e-7 A^2 

So, A is sqrt(kg m/ss) if I got this right.  WTF???

Maybe taking out c as m/s is invalid since these are constants and not
vairbales in the equation.

So that changes to:

kg m/s s = f = A s A s / m^2
kg m^2 / s^4 = A A
sqrt(kg) m / s^2 = A

Still WTF teritory.

What if substite in e=mc^2 or c^2 = e/m 

ma = f = c^2 1e-7 q1 q2 rHat / d^2
ma = f = e/m A s A s / m^2

And e is kg m^2/s^2 so:

ma = f = kg m^2/s^2 /m A s A s / m^2
ma = f = kg /m A A
kg m/s^2 = f = kg / m A A.
m/s^2 = /m A A
m^2/s^2 = A A

A = m/s

This is cleaner.  But still WTF!

If A is m/s and q = A s them q is m.

None of this makes any sense.

So, we have the simple fact that current in a wire creates force.

And, that static build up of charge, creates force.

When we try to xplain these two different effects, as one, what falls
out is speical relativity as a very odd way of tying these two different
things together.

But yet, general relativity, that falls out from that, explains much
other things as well in the univese.  Like gravity making light "bend".
And famisly, e=mc^2.

Ok, so wait, I've spoted what must be an energy problem with my total
energy conservation code based only on kenetic and potential energy since
magnetic force is changing kenetic energy but not measured in potential
energy of distance.

I still need to verify this is a problem however.

But, could we add mc^2 as a measure of magnetic potential energy to fix
it?  Or something like this?  Or, if because all velociety are relative
in how I've coded it, might all the changes in potential energy in the
system all cancel each other out?  So that our addition of magnatism
didn't create an energy problem?  It might work out that way.

Oh, here's a thought.  Even though the speed of electons in a wire is
very slow compared to the speed of light, the information they carry is
moving near the speed of light.  So the flow of kenetic energy might be
moving thtough the wire at near the speed of light!

So, if the basic inerta of f=ma effect, cuase the flow of kenetic energy
to move at the speed of light though systems of particles, like the
elctons and ions of the wire, this this flow of kenetic energy can be
the foundation of em force.  And when they are able to trade off qv and
I in the magnetic field equations as if they were the same.

E=mc^2 is not 1/2 m v^2.  But A is current flowing in opposit directions
in two wires.  Could that make it 2x the energy and turn 1/2 mv^2 into
mc^2????

----

Ok, rereading magnatism stuff, I think I've spoted something I failed
to understand in the past.

Moving currents don't just create the magnetic field. They also create a
change to the electic field!  But it's the change in the magentic field
that creates the electic field.

Ok, not sure if I've really coded that correctly!

----

Oh oh.  A current flow causes a buildup of charge (push N electons in a
second past a point).  And that build up, cause them to push all these
other electons down the wire -- which adds up to a stransfer of kenetic
energy down stream.  But the speed at which it can force others to get
out of the way is controlled by basic relationsips of electostic force
and inertia.

And if my thinking that the speed at which force can turn into motion
is what sets the speed of light limit in the sytem -- the speed at
which kenetic energ can move through a field of particles...  This is
how current in a/s can translate to velociety concepts -- the buildup
in charge caused by the current only "unwinds" into actual motvation,
at a fixed speed regulated by the speed of light.

This is not preceise, but it's sort of grasping at starws approach
to undesetanding how current can be related to cv in a magnetic field
equation.

A charge build up, in effect translates itself into cv -- charges moving
per speed of light effects.

Is there any chance that current cuases a voltate drop that cuases a
changing charge dinsity over the wire that in the end explains magnetic
attraction as changes to charge density.  No, it doesn't look like that
works out correctly at all.

----

Not only is the use of c^2 in the em force equation just odd as fuck
the e-7 is equallky odd as fuck. Where the fuck did THAT come
from???

But it has to have something to do with how moving particles affect
each other vs how stationary particles affect each other.  The relationshp
of this is tied othe 

Ah, but fuck no.  Because magnetic field equation and static force equation
both have the same e-7 factor in it.  So it's part of the unit conversion
problem somehow.

ah, more later.  Food now.  Debate soon. In minutes.

================================================================================

2016-09-27 10:03 AM Tuesday

Yesterday

1) Lots of thought about the very foundational evidence of electro
magnatism.  Trying to understand how on earth the speed of light shows up
as a fundamental constants in the force equation.  And how the magnatism
equation I've derived for use with particles is a perfect mirror of
the static force equation. Why are these two the same yet describe
very different physical effects?  And how on earth can qv be the same
as I (current) in magnetic field equations?  I is electons/second not
electon m/s?

Today

More thinking.  First debate between Hillary and Trump was last night.
I could only watch a few minutes before I couldbn't take it any longer.
Trump was just being Trump, looking for ways to insult Hillary, proposing
tax cuts to help create jobs, trickle down stupidty, and being a goon.
Hillary was just running though talking points.  I can't stand to watch
the games or the stupidity of Trump.

Physics

Ok, in the magnetic field equations, I'm finding sites that talk about
the current equations being current times DL time a "charge density"
factor. Now that is making more sense!  or the internal of current times
DL.  That does turn it into electons per meter/s that matches qv.

Ok, on the Lorentz Force wikipedia page, it's saying the force is:

F = qE + qv x B

Which is basically what I'm using sort of.  With the question of what the
correct sign of B is in the particle case and with the qustion of how
to measure velociety to calcuate B and how to calcuate total force on
the two particles.

But it's saying that there is another form of the equation needed to
adjust for reltivity effects when the particle is moving near the speed
of light.

So, my thought then, is whether all this relativity stuff is emergent
from the field of interacting particles (I suspect it could be), or
whether it's fundamental.  So at the macro level we need relativistic
adjustments to explain how the field of interacting particles behavior,
but at the mirco level, we don't.  The books imply it's all fundamental
and is needed at the micro level.  But I'm not sure if this is true.

Or rewriting:

F = q(E + v x B)

fMag = 1e-7 q1 q2 v X v X rHat / d^2

fEs  = 1e-7 q1 q2 c^2   * rHat / d^2

Oh, later on the page:

F = Il x B

Current times length!  And if the length standard is 1 meter, guess
what happens?  It becomes F = I x B!  Ok, this is making more and more
sense now.  That might not be true. But it's possible.

And, many questions that were confusing me were acxtually current DENSITY
equeations and not Current equations.  And produced force DENSITY answers,
not force.

I don't understand the math still but I'm getting a little better picture
of what was confusing me.

Oh, this looks interesting and important....  Talking about forces
between two moving chagres!

http://newt.phys.unsw.edu.au/einsteinlight/jw/module2_FEB.htm

It says if you run with two electons in each hand, the electrostaic force
will push them apart. But the magnetic force will PULL THEM TOGETHER!

I course, I reject the idea that there is an magnetic force when they move
in parallel.  But it's intersting he thinks the mag force is operating
OPPOSIT to the electrostaic force!

And Kes / Kmag - 1/u0e0 must have units of speed squared!

Where V0 = sqrt(1/u0e0)

"So what is v0? If you have studied waves, the format of this expression
may seem familiar. The speed of a wave is given by:"

Vwave = sqrt(elestic paramter / inertial or density parameter)

For waves in string, or water, etc.

So he's making an anology between how the speed of a wave is determined
in media with how Kes and Kmag related to each other to define the speed
of electrostiact waves!  Which explains how the fuck c^2 shows up in these
equations!  It's the NATRUAL SPEED in fields of particles connected
by the fucking electrostic force!

So this alone, says a lot about the fact that this is the force that
makes waves move at the fucking speed of light through this medium!
Or at least it MIGHT be saying that!

fEs  = 1e-7 q1 q2 c^2 * rHat / d^2

Which is what I've been suspecting was a truth about where the speed of light
really comes from.  Not a truth about how particles interact at the micro
level, but a truth about how a field of particles can pass a wave!

Then it goes on to say total froce reduces to:

F = Felec(1 - v^2/c^2)

Where Felect he defined as what I are using, q^2/4pie0 r^2.
And Fmag is what I am using u0/4p q^2 v^2/r^2

And using F mag as an adjustment to F elect, he subtracts the two (instead
of adding them)

F = Felect - Fmag

Then rewrites as:

F = Felect(1 - Fmag/Felect)

Fmag/Felect is:

[u0/4p q^2v^2/r^2] / [q^2/4pie0 r^2]

Which reduces to:

u0/e0 v^2

And since c^2 = 1/u0e0
u0 = 1/e0c^2 = 4pi e-7
e0 = 1/u0c^2 = 1 / 4pi e-7 c^2

u0/e0 = 1 / c^2

Ok, so the Fmag/Felect becomes:

v^2/c^2

which gets us to:

Ftotal = Felect(1 - v^2/c^2)

Ftotal = Felect / y^2

Where y = 1/sqrt(1 - v2/c2)

Which of course that y crap is the lorentz contration factor.

Which all falls out, of the basic static formuals, not relativity adjusted!
At least informally.

But, this isn't dealing with any of the real complexity of direction of force
addressed by v x (v x r)

But it does agree with my instinct that mag force must act AGAINST static force!
even though signs from standard foremula sseem to indicate otherwise to me.

But, it gives me some hope that I can combine the force equeations to something
simpler that defines how force isn't as strong under motion???  is this how
arodynamics works?

Also..

https://en.wikipedia.org/wiki/Biot%E2%80%93Savart_law

But the below is only true for v << c:

e = 1/4pi e0 q   r/r^3 b = u0/4pi   q v r/r^3

The more complex version has v^2/c^2 and sin^3 theta etc.

Oh, more interesting ideas.  Magnatic field thought of as spin -- vortex
of spinning charges.  As current in a loop.

Linear motion of charge creates a votex hence the idea of a circulare
magenetic field around a wire.

But constant current in a straight wire won't enduce (vortex) current in
a coil around it!  Only if the current is changing will there be induced
current in the coil around it!

----

Ok, all intersting exploration.

Ok, another thought I had this morning.  Electons in a wire move very
slow.  But the logical information flow is near the speed of light.
Could this information flow (the wave speed) be inportnt in how the
wires attrct, or is it the real speed?

Multiple places say it's the real speed of the electons.  And though
it's slow, the loreze relatively attraction of the slow moving elctrons
creates enough chagne in force to create gravity.

So, it's the electons slow motion relativie to the ions that create the
magnateiv force of current flow!

Ok, so electons moving past protons, pull together. But that's fucking
backwards since electons and protons already pull together! Ugh.

In the frame of reference of the electons, the protons shirnk together,
and attaract us.  In the frame of reference of the protons, the electons
shrink, and still cause attraction!

Ah, but if we invert all this, and claim nothing is shrinking, and all
that is happening is that electons and protons change their attraction
under motion, we have to say they attract MORE under relative motion
to explain wires being pulled together.

But what if the motion of the electrons, changes the magnetic proprty
of the atoms in the wire?  And that change, is what causes the wires
to attract?  So the atoms act like little randomely allined magenets.
The motion of the electons then forces the magenets to align, and that
alignment is what pulls the wires together?

Or, the motion of electons in one wire, changes the magenets in the
other wire?  (in the same wire it's too chatoc and cancles out?)

But this would be true no matter which way the electons move.  So it
must mean the motion of electons in the other wire are also needed to
allow the magenets in that wire to align?  I'm running wild here with
made up stuff stories...  In case it wasn't obvious.

--

From another direction -- the reason two electomagnets attract is beause
their N and S poles match up.   Which we can see as the same thing
at work when two wires attract when the current flows the same way in
both. Adding an iron core makes them stronger, in theory because the
magnetic field makes the magenets in the iron core change alightment --
or it's described as the iron core attracing MORE lines of flux into it.
But it's an obviouss alightment effect.  I think.

---

Another thoughts.  If electomatic field move though fields of particles
in waves, then these waves will of course interact with each other over
time and we can explain all the talk about changing e making changing
b and visa vers -- but only at the macro level of waves moving though
feields of particles!  Not at the micro level of to particles interacting.

So all the complex maxell equations based on density and change
over time might only really apply directly to field of particles, not
individual particles.  I'm continusing to feel this could be a true and
valid argument.

So I'm searching to understand what the fundamental interaction needs
to be to create these wave effects.

It seems from that math above that C can be explained in terms of basic
electrostic interaction with mass and inertia.  No magnatism needed.
But is there any conceviable way to explain magnatism at the macro level,
using only electrostic at the micro?  I don't think so.

Ok, so we have resobnale questions about what GENERATES a magnetic field
but we don't have any doubt they exist, and that a single electon will
take on a circular motion though it.  How could a single electons be
forced tohave it's path bent into a circle, if the only things in the
other material generating the effect, was electrostatic force?  So we
have something like electrostatic force moving in circles. I see now
way to explain why that would make the distant electon bend in it's path.

An electon in a orbit around a proton, spinning in a tight cirucle, will
not bend the distant electon path

An electon in a orbit around a proton, spinning in a tight cirucle, will
not bend the distant electron path only because of electostatic force.

Could it somehow get all the surounding particls to help?  Could magnatism
be an emergent effecdt of a field of interacting particles and not
fundamenal some how?

At best, I'll I'm grasping, would be the power of the field to make the
electon vibrate along with all the little orbits, not curve.

So I'm still feeling there needs to be force related to motion of reative
particles to exlain this.

Yes, a perminate magnet can't really magnatise the air to call int into
helping making the electon spin.  As far as I can grasp.  It could,
but air unlike iron, seems to be mostly immune to such help.

Ok, it's a well documented beleif that elctrons have a dipole moment as
an intrisnsic propety -- that they are small point magnets. But my project
here is attempting to explain that using orbits and motion instead.

But, checking the spin experiment, the particles deflect N to S, not
the same as left to right in the field due to charge. They use charge
nutrual atoms, to prevent that -- and see the magenetic effect of the
atom instead!

So if you throw magenets through a magent field, you would expect
deflection up and down! Ok, that makes sense.  But the spin experment
showed quanization effects which is key of course to quantum mechans
notion of quantized spin.

Ok, so charge bending in motion is 90 deg to magnetics bending in magnetic
fields. I get that.

But, a spinning electon, would create a magenetic field alone it's
spin axes, which makes other electons moving perpendicular to the
mag field spin bend into a circult, like the electron is spining!

So a spinning electon makes others spin in parallel planes.  But which
way does it make it spin?  The same, or opposit?  Must be opposit.
If it made it spin the same way, then entire univese would quickly
line up.  And if it makes it spin the opposit way, that justifies
conservation of angular momentum in the system.

Ok, so lets check out right hand rules and current. Postive current in
a right hand rule, creates a N pole at the thumb.

Yes, so an electon spinning one way. tries to force other electon to
spin the opposit.

This is also consistent with the idea that electomagnets, induce currents
in other coils, that counteract the magnetic field.  Though, induction
requires change in fields???

But you know, that doesn't make sense.  A magnet moving in a coil will
induce current will it not?   Isn't it just relative motion that is
important really?

Ah, but motion induces current/force that opposes the magenet and becomes
table. Only changing magnetic field allows oppising force to chnage and
opposing current to change!

So we have a force motion relation that creates all the stuff at work
here?

The motion of one, creates FORCE on the other, but if force is balaced
it won't create motion.

---

Ok, here's an idea.  I can test to see if my code as written works!

I can create a fake electro magnet by forcing a collection of electons
into matching orbits to represent the current of an electro magenet.
No matching postive ion atoms, just the electons. Then I let another
electon fly through the field at the end and see how it acts! maybe.
Could try that.  But if the rason electon magenets work, is the complex
interaction of the atoms not just the moving elctrons, then this wouldn't
work.

But still, something in the magenet or electomagenet has to cause the
electon to cir4ucle in the magenet field.  So we must be able to to set
up something in the "fake" magenet to cause this if we have the right
funamental force laws correct!

In theory, if all we had was one magenet, and only one electon flying
past it the electron (in a true vacuum otherwise) should bend it's
path perpendicular to the motion.  Can the mag force I have coded every
cause that????

Maybe it would need a magenet above and below to create anything close
to a uniform magenet field to create true spinning trapped between
the two magnets?

Or, what about a ring of protons spinning one way and a ring of electons
spinning the other in the same location?  So the toal electomagenetic
force would mostly cancle out?  Leaving only mag force differences?

And a electon moving one way would attract while a proton moving the
other would likeise attract!  Where as moving the same way would
cancel out any magnetic effect.  Making one side of the ring attactice
a moving election and the other repel. Or even one electon standing
still will be attracted to one side and repeled from the other.
But as it starts to move relative to the spinning rings, things could
change.

No, wait, wouldn't both sides equally attract an electon standing still?

NO, my logic is all wrong.  A moving proton pushes an electon away.
Makes no difference which way it moves.  A moving electon attracts an
electon, makes no difference which way it moves.  So opposit spinning
rings will cancel each other out -- relative to an electon standing
still.

But if the electon moves in parallel with the spnning electons, then
the electons will stop having magnetic attraction, and the protons
will have double strong magnetic repeling.

Or, if it moves in parallel with the protons, the electons will attract
while the protons don't.

But, without pure distributed charge, it won't all perfectly cancle out.
As an electon and proton cross below and separate, the moving electon
will attract, and the moving proton will repel.  But they will do
just the oppsit as they apporach ehach oher -- so it will cancele
out.

So what if the protons were standing still, and the electons moving in
a circle?  Would it require the electon to start to circulate at
half the rotationsal speed to balance the effects?  With the electon
standing still, it will have no magnetic foce with the protons, but
the moving electons would attract it.  This is all based on my backwards
sign logic of course -- that electons flying past each other attract
which is the inveser of their static repelling force.


So, if the electon is on the edge, it's attracted to the electons
on the same edge, but also equally to the ectrons on the far edge
so it wwould try to move across the circule to the other side I would
think. But as it pickes up speed, it would interact with the two
sides.  The electons moving with it, the attraction would reduce.
And the increse in relative motion to the prtons, would make it repel.
This moves it away from the side were electons are moving the same
way.  And for the other side, the speed with the electons incrase
and attract more, and the protons increase by same amount and
repel more. Wait this seems to cancel out. Why didn't it cancle out
the same on the other side?

ep speed increases, replels more.  ee speed reduces, attracts less

yes, I think my logic is broken. The motion relative to the sides will
not cause  any movement either way.

Wait, fuck hold it.

ep speed increases, replels more.  ee speed reduces, attracts less repel
more and attract less means repel MORE total.

And on other side.  ep speed increases, repeles more. ee speed INCREAES
so attacts MORE.  That blances out no change on right side, more repel
on the left, so we push to the left.

That means we start to move in the same direction as the electons!
Not the opposit direction!

Ok, so it looks like there is some potenital here for the mageneti field
I've coded to cause some spinning.  But it's all related to the speed
of the atoms relative to each other.  So if the spinning magenet would
have to be moving faster than the electon being pushed around I think.
Maybe. It's too complex.

I think it might be cool to code this test.  Force a collection of e and p
into a circulare spinning path, and let another e free to see what happens
to it!  Becuaes, honestly, I can't figure it out in my head.  But it
looks like there is potential for the magnetic effects to create some
intersting resutls when we cancel out the electostatic with matched paris.

Or, set up a line of protons, with electons orbiting around them, but
fix the protons in place so they can't move.

Ok, but in this, a free electon, next to a "wire" with fixed protons
and current moving electons, will be attracted to the wire! The electro
static will bebalanced but there is no magenetic with the prtons since
they are in the same motion as the electon, and the moving electons will
attract the electron!  That is not correct based on expectations.

---

And, no answer  to why simple current in a wire works to pull two
wires together using my inverted field.  The implications of the lorentz
contration argument is that elctrons flying past protons will attract
more.

Could there be something with holes moving the other way?  Holes move
at the speed of light while electrons move very slow.  Wait, so what if
current flow creates holes?  Making a net postive charge in the wire?
By forcing the electons to spread out?  What if there can't be free
electons in a wire, but only free holes where electons have been pulled
out of an atom?  What if free electons would only free out into space
if they were injected into a netural charge wire?  Ah, but no, electic
charge can build up.  So it is possible...

Doing reserach on ions, -- learn that atoms with new extra slots in
outer shell like to suck up extra electons and form negative ions.
Iron and coper that are good conductors have 2 or 1 electons in outer
shell so they give it up easilly and form ostive ions (holes).

---

Just makes me realize how little I know about chemisty and atoms which is
funny since my work here is trying to argue we need to better understand
atoms! There is much we do know, that I've not learned about.

But, if iron and metals connect by losing electons and forming holes
then current should maybe turn the wire net postive?  but the two net
postive wires would push apart not pull together.  And logically if it
could be seen that way, electrons electons flowing one way and holes
flowing the other, will still repel by my formula.

But electrons taking a fast jump from one atom to another, will move
very fast then slow down.  Maybe.  But no, attraction is qv so the
total attraction is the sum of all qv so the averge speed can be used to
estimate total attraction and small jumps don't creaet some v^2 effect
to create super attraction.

So none of this is making much sense.

All wire effects seem to be backwards from what would make more sense
in nature between particles that the motion woldn't add more attraction,
but would redeuce it or change it's direction.

But, if it's inverted, then not not X creates X.  It has the power to
chain double negatives to create a postive.  In abstract theory. Where
as when forces only add, it can't ever do plus plus to create a negative.

So I can just go back to the the argument that "something unknown that
is complex creates a double negative effect when current flows in wires"
that ends up making the wires attract!

Well, electrons moving past protons will repel in my way, which can
justify skin effect? no...

Ah, skin effect is only caused by alternating currents and is greater
with higher frequency.  Caused by circular edit currents.  It seems.

Which tryes to conteract the current in the middle of the wire but not
the outer edge.

Still can't make any of this add up!

Ugh.

all the web sites talk as if this is all obvious and the formauls are
all well known.  But none talk about trying to simluate electon orbits.

Electron Orbital Motion

https://www.youtube.com/watch?v=HHSluoPqank

The above is trying to visual quantum mechanic orbital models using
density tricks or something...

Certainly, there is a deep knoweledge of atomic orbits.  Why can't they
be calucated as actual classical mecahancs orbits created by particle
interaction!

Is the particle to particle interaction more complex somehow?

================================================================================

2016-09-28 10:52 AM Wednesday

Yesterday

1) Lots of deep thinking about why the equations are how they are.
And trying to understand if the magnatism forumal is correct.

2) Found this fascinating page:

http://newt.phys.unsw.edu.au/einsteinlight/jw/module2_FEB.htm

That shows the connection between u0 and e0 and how similar physical
paramaters define the speed of a wave in strings or air!

In other words, this is eactly why the speed of light is in the
EM equation!  It's the relationship between inerta and force that
defines how fast waves move through the medium (field of particles),
And the reason no other factor is in there is exactly because charge
is defined from current, and current is defined as the amount of moving
charge needed to create 1 N of force!  I'm still a little lost on where
ei-7 comes from, but the fact c^2 is in there is explained now.

The same page implies mag force should be seen as an adjustment to
the E force and SUBTRACTED -- as I have done in my formual but which
is backwards from all others.  And when you do that and simplify, the
Lorenz factor falls out as 1/sqrt(1-v2/c2).

That's two cool clues.  Even if neither are hard facts.

So the implication is that the lorentz factor is how magnatism created by
motion adjusts electrostic force.  But yet, this still doesn't explain
electro magnatism corectly at the atomic level as far as I can grasp.

3) Ended the day feeling lost.  All my thoughts about how these force
equations would actually explain why electomagnatism works in my simuation
seems to fail.  And there is much known about atomic orbitals that I
don't yet understand that is all key to this as well.  The deeper I dig
the more I feel I don't understand.  And nothing seems to be pointing
in the right direction yet to a solution that works.

4) Had the idea of hard coding an electro magnet as roating circles
of electons and protons in the simulation where they moved by defintion
in perfect circles and not under simulation control. Would other free
particles than intract with them to show magnetism like effects?

Today

1) Instead of trying to code particle magnatism from the known laws of
magnatism, I can try to attack from the other direction.  Try to guess
what the particle froce equations need to be, to make macro level
magnatism show up.

2) Oh, and, do my current force equations throw off total energy?  I think
they likely do.  Which means my "energy adusting" code is BREAKING what the
magnatism code is trying to do!  So maybe I just need to turn off the
energy adjusting code and see how the particles interact?

----

So, I tried to code magnetic force of two particles and came up with
a formula that seems mostly consistent with documented formulas of
magnatism.  But I had to play games with frame of inertia when calucating
magenetic field and I choose to invert the sign to make motion reduce
force, instead of increase it.  And try as I might, I can't logically
deduce how electromagnatism could possibly fall out of this atomic
particle behavior.

And fixing the sign, only seems to make it worse.  It causes orbiting
particles to fall into tighter and tighter orbits. I think.  I relly
need to turn the energy adjust off and see what it does.

So, maybe a better approach is to just understand the basic behavior of
electomagnatism and then reverse eneringe and FIND the particle force
laws that make it work!

So, here's a thought I had in bed.  When we have a loop of curent,
is the flux density inside the loop the same everywhere?  Does it work
like gravity inside a body or charge inside a body where the magnetic
field is constant everywhere inside the loop?

It seems to me this needs to be "inside a sphere" with the 1/x2 rule
instead of inside a loop.  But if it's consitent everywher inside a
loop, then that's a simple way to create a constant magenetic field,
and a charged particle inside the loop needs to zip around in a cirle
on it's own!  That would be something to try and duplicate by thinking

I do understand that integrating the current around the loop and intergrating
the flux over the surface does create some identiy in magnatism.  Meaning
the total flux a  related to the current.  But I supsect the flux density
is not constant.

----

http://newt.phys.unsw.edu.au/einsteinlight/jw/module3_Maxwell.htm

Another fun page from this same site.

It points out that if you make e0 and u0 1, and adjust other units like
time to fix that, maxwels qustions become perfectly symmetical.  But
also include magnetic "charge"nd manetic flux densty terms that in
our univese seem to be zero and aren't normally included beaues they
are zero.

Which shows some strange and wonderful symmetry between electic field and
magnetic fields and space and time.  Even if I don't fully understand
what it is.

The same page once again points out that the constants in the qeustion
work out ot be the speed of light.  And seems to say that when the
constants were first worked o ut imperically, it was recognized that they
macthed the speed of light -- which is what lead to the understanding
that light was a wave like radio waves and part of the same effect!

I guess they didn't know the speed of radio waves were the same as the
speed of light at the time?

To me, this says there's something even simpler we are missing
underneith it.

----

Ideas for how magforce might work?

A) Maybe motion just changes the strength of the electostic force and doens't
change it's direction.  So relative motion makes it strong or weaker?
as if it were closer or further away?  Like lorentz contraction?

But the idea that the force might drop to zero at the speed of light bothers
me.  The speed of light falls out of the relationshipo between inerta and
force.  To me that says we should NOT be coding the speed of light into
the fundamental behavior itself -- making it drop to zero at that speed.
If it had some effect that worked like that, what we coded at the fundamental
level would be something else... I would expect.

But if motion made it stronger, then we would not be coding the speed of light.

B) Maybe motion changes the direction of the force but not it's strength?

C) Maybe motion ONLY changes the force perendicular to r?  (not perpendicular
to to direction of motion?)

----

1/2 cup creamer.  35 cal per tablespoon.  8 tablespoons in half cup. 280 cals
in each cup of coffee I drink.  Ouch.  Or 300 ish calories.
I need to learn to use less...  I drink 2 to 4 cups of coffee a day these
days.

----

Why no magnetic monopoless! Becase we have no particle that gives off
a static magnetic field without motion!  And when a particle in motion
gives off a magnetic field, it creates equal and oppost fields!

So, motion creates force -- and we call that force "a magenetic field"
And that motion ties together time and space.

Without motion induced force, all we have is static foce, that ties
together space and force, which ties to inerta and motion.

To mantain conservation of energy and monumtum, we need motion induced
force to be equal and opposit. This seems fundamental. Otherwise, it feels
as if we would he creating a free energy source which electicy is not.

All this feels like it's adding up correctly.  But the only question
remains is how to correctly map motion into force!

Does it matter?  If we map motion to force can any smooth mapping work
as long as we mantain symmetry?

Ah, but the symetry of maxwells equations makes me think the maping is
not arbitary.  That there is some clean and obvious answer to how motion
changes force. If it were arbiaty it would break the symetry.

Maxwells equtions when it shows the symetry between electic and magnetic
is also showing an underlying beauty between force and motion and distance
and time.  How to turn this around to see it?

It's as if there must only be one way to link these together to create
this symmetry.

And I'm not buying this warped space crap. 

--

What if force inverts, if motion is faster than the speed of light?
Through the looking glass sort of interesting.  But WHY would it
inverst AT the speed of light?  I can't justify just hard coding
it that way. I can't justfiy coding force based on v2/c2.  I have
to find a stronger simpler underlying justification to exlain
why it works this way.

--

Without magnatism, we have space defining force defining change
to momentum. Or, space defining change to motion.

What about motion defining change to space as an inversion?
(fuck I didn't want to do that!)

But a more simple view, is motion defining change to motion
as well.

Or, how space changes motion, is a function of what the motion is.

Or, how much one particle can push another is limited by the
motion of the particles.  It can't push as hard on a moving
object?

--

Why does lift work?  That's an effect where moving fluid can't
push as hard.  Why does it work?

---

On a NASA lift descritpion web page.  F=ma is f=m dv/dt.  Which tells us
force creates change in velcoiety, but also reading backwards, chagne in
velocity creates FORCE!  It tells us that the two must match.  It doesn't
really have a direction as much as what it says, say in our model,
is that path of the particles must satisfy the equalty of f=m dv/dt.

So, magnatism tells us this is not the right path, so that's not the
right equalty. Or at least not the full story.

A charged particle circling in a magenetic field tells us there is
acceleration that is 90 deg to the motion.

I feel like the motion of particles in the magenet has to be creating
this magnetic field effect. But the motion of the particles has to be
creating a constant effect unrelated to the speed of motion?  Or the
speed of motion in the magenet defines the strength of the effect?
Yeah, motion creates force. That's consistent.

----

I need to find a few basic math physics answers.  If I have a circule
of particles, what's the static force in the middle?  Constant for all
positions or does it change by position?  Test code time.

----

Why presure drops in fluid when it moves faster!  The answer is that
the drop in pressure is what MAKES it move faster!  Or if there wasn't
a drop it wouldn't be moving faster.  This falls out of a conservation
of energy equation aka Bernoulli's equation.

---

What if there was a conservation equation at work here in particles
related to electromagnatism.  That seems almost like it has to be true.

Ok, but we already have potential + kenetic = constant.  That's because
the force created by distance is traded off for kenetic energy.  So that's
why these define the perserved constant.  But when we add more force
for velcoicty, we mess that up.  Unless we make them all cancle each
othet out!

Force has to be based on relative velcoetiy to make sense.  And look at
what happens over an entire system!  If we start with one particle and
look at the relative velociety of all the others, they must all sum to
zero! This might tell us that the third conserved energy is always zero
over the entire system!  That magnetic monopols don't exist!  Maybe.

Which might tell us that how we are implenting magnet force is already
energy safe?

----

Ok, did test of conservation of total force on a particle inside a circle
of paricles.  It's not constant everywhere!  But if we change force
forumal to 1/r instead of 1/r^2 is does sum to a constant for each point.

1/r^2 makes total force a constant if we sum over an entire closing sphere
(or any enclosing surface I assume).

But what about magnetic field inside a closed circle of current?

wrote magCircleTest() to find out.  The answer is that it's not the same
over the inside of the circle. It's stronger at the edges near the wire
and weakest in the center. Should be infiit at wire I think but I'm not
sure about that.  1/r^2 goes to infinity but dl goes to zero at the same
time so I'm not sure.

But, strength is somewhwt similar over a lot of the circle only getting
to 2.5x stronger at the 20% from edge.  Switching to 1/r doesn't fix it
either though it shows closer to flat values.

----

Ok, turn off energy adjusting and see what happens.  Does magnatism make
it break badly or not change it?

----

[4:27 PM]

Ok, shit.  Tried to turn energyFix on and off to see if magnatism was
really messing up energy but I couldn't see any real difrference from
between mag on and maf off.  The energy gets so far messed up so q2uickly
I can't tell if magnatism is making it worse or not!  But in general it
doesn't seem to be making it worse.

I tried added random to the mag force function but that didn't make it
mess up much either!

I've got an ideas, switch from using relative velociety to absolute!

So when if calculates p1.mag(p2) it will get very different numbners
from p2.mag(p1).  lets see..

----

Ugh.  Even that swapping absolute v for relative didn't have as large
a difference as i expected.

Added relative e % of total KE to see how fast energy error was
accumulating relatove to KE.  Errors accumulate faster with Magnatism
turned on. But not all that much faster and not all the time.

Even with inverted or normal magnatism, it's still not accumulating
errors all that fast.

I really don't know what to make of this.  I'm not sure that magnatism
should balance out.  And I'm not sure it's not.

I was kinda hoping it would be either n obvious problem or an obvious
non problem but I can't say for sure if it's either!

Also, been running a few tests with book version of magnatism sign and
it's not really causing the system to crash with 2p2e tests.

--

Clenaed up some debug printouts.  Added screen size in angstroms.

--

The normal sign magnatism with 2p2e just fell into a state of low
orbit where on electon has e9 speeds and is inside the rLimit of the
simulation!!!!

I don't like the normal sign version due to this tendency to get into
this tight orbit. But, MAYBE, this is actully needed to make neturons?
But it sure make the simlation SUCK big time due to computation disaster
it creates.  No one said the universe was easy to simulate! :)

----

[7:19 PM]

Well, running late.  Not sure which direction to go next.

Ok, todo --- look at the total energy question and see if magnatism ends
up creating a potential energy we aren't accounting for!

Need to understand this better to understand if there's a way to do it
that doesn't throw it off -- of there isn't a way, then this needs to
be dealt with in the total energy calcuations as part of the systems
total energy.

And, so with different ideas for how to code magnatism, I can understand
what it does to the total energy issue.

-- 

idea -- turn off electrostic and only run with magnatism to see what
happens! Might help see/understand energy issues if we don't have
electostic swamping the behavior!

--

Idea, just try testing different made up version of magnatims to understand
what each does!  Might help gain udnerstanding of what is needed to make
m agnatism work!

--

Ok thought from early.  If we create a magnetic field with rotaing current
which way does the electon in the field spin around? With the current
or against it?  I've looked at this mutkple times, but I thought it
was spinning with the current, now I think it's backwards!

Meaning for postive current, protons will spin agaisnt the current.  For
negative current. electon swill spin against the current.

Yeah, seems to have confirmed this again.  When the current flows one way,
the charged particle trying to move through the field spins opposit way!

--

Related thought.  When two parallel wires pull together (due to current
flowing the same way in both), is the motion of the wire attempting to
create a back emf in the wires? (I assume so -- lets try to verify
this..)  Ah, sure it MUST due to how inductors work.  Yes, playing with
right hand rules seems to confirm this.

--

I was trying to justify my backwards sign by saying the reason curent
worked was some complex probprty of current interacting with atoms.

Might still be. But I really feel the need to try and make curren
work as simple moving charged particles and give up on the investion
problem.  But the postive version creates orbit colapse.  So maybe
a slight different version of of the postive version will fix it? 

Like, motion causing the force vector DIRECTION to change but not the
total magnatude?  Might keep potential energy correct if we do that?

If new force is added due to motion, then we have to consider total
motion part of the energy equation.  If motion only BENDS paths, then
it's not part of the energy equation.  But that can't be true or else
motors aould not work.  I don't think.  But if motion points the force
vector a diferent way...????

At speed of light, two electons passing each other, it bends the force
45 deg maybe to make them slow down as well as push away?

But the same for attraction, would force them to pull together faster?

-- Or, what if all motion effects was to make particles slow down?

-- so not negative for different charges.

-- could something like that leak out as gravity?

What if I have the direction of the vector corect, but I just need to
add a reduction in the strength of the ES vector while the motion force
vector grows?

-- Like calling it the motion force instead of magnetic...

ok, I'm liking the idea of giving up on normal magnetic equations and
just looking at what might make sense to implment, and then following
that to see what it causes to emerge.  This might help me find the right
answer even if it turns out the right answer is what the magnetic equtions
were saying!  Or almost what they were saying!

Ah, if the motion force slows down motion, then it seeems like it would
try to kill all motion but I guess it could just be abother part of the
energy balance of the system.

After all, we have captictors and inductors that both hold energy and
release it.   If capacitors hold it with the potential enegy of the ES
field, it feels like we need a way for the motion force to hold it as
potential energy as well!

Ah, fuck.  WTF.  When the current stops in an inductor, there is a
magnetic field holding energy!  The magnetic field is said to colapse
and put energy back into the current flow!  WTF is that energy stored?
In a magent force field?  No fucking way.  It must be in the orbital
energy of the particles in the iron core or the air if that's all you
got or edit currents of the core.

Ok, so the magnetic energy of one moving object must be transfered to
another and back again. Somehow.  Can we make movement force just a way
of kenetic enrgy to move from one object to another instead of trading
off with potential energy?

That sounds cool.  That sounds correct.  I'll try to set it up so movement
force does only that -- transfers kenetic energy from one to another.
It might be that any equal and opposit force between two particles will
always do that anyway!  Need to figure out if that's true.

No, that's NOT true! equal and opposit force on two particles standing
still will ADD kenetic energy to both!  But if one is moving, and we
apply force to slow it down, we can apply force to another to speed it
up to do the transfer! I think.

I'm thinking if motion causes the ES force to point a different way,
that might be harmless?  No, wait, not harmless. It would break the
potential energy vs kentic energy system if it pointed a different way?

Shit, so I think motion force must transfer ke from one to the other at
all times or else brak energy conservation!

This is fun. I'm making progress.  I'll have to pick this up tomorrow.

Need food and tv. And exercise.  Or maybe, not food?  Lost 1 lb this
morning.  Maybe I should just try starving myself to get the weight down?
So hard to not eat...

[8:24 PM]


================================================================================

2016-09-29 4:14 PM Thursday

Yesterday

1) Endless thinking about how to implement magnatism "correctly"

2) Tested 2D circle of particles. What's the total force on the inside
of the circle?  Constant everywhere? Nope.  Weakest in center, strongr
at ecges.  But if force is chanved to 1/r instead of 1/r^2 is it equal
everywhere! And center of spher, is equal everywhere with 1/r^2 force!

3) Same sort of test for magnatism inside 2D current coil.  Again, not
the same everywhere.  Weakest in center, stronger at edges.  However the
change is fairly flat thorugh the middle and only grows strong in the last
20% near edge.  Going to infinity at wire maybe? Gets flatter if 1/r is
used instead of 1/r^2 but not flat.  Because all sides add to the field.
None subtract from it which is what is happening in the force test --
the forces fight each other.

4) Study question of whether magnatism is adding energy or creating a
potential energy vs kentic trade off not correctly being accounted for
by total energy code using only potential energy of ES force vs kentic
energy of motion.

5) Intesting ideas of how to approach it -- by trying to undestand whast
should work!  And The diea if needing to better understand energy vs
force questions.

Today

Very late start.  Rain last night. Dark this morning.  Did get up and
started before 11 but got distracted on long long posts about economics
and basic income to a post by Eray.

Also spent a lot of time thkninig about force effects last night instead
of sleeping. Just excited to see where the thoughts from yesterday might
lead. But not real insights just thinking about it.

Oh, but one.

--

What if magnatism can work as being 90 deg to motion, and that if we
add all these forces up without creating matching opposit forces to
each other particle, that  they will naturalky all balance each other
out?

--

If that is true, then my point about making sure all forces from partical
a to b must balance could be invlaid.

This might work, due to total momentum being zero?

Total monumtum of zero means center of mass is fixed and has a net velocitey
of zero. But total velociety is not zero.    One electon can move fast
to the right, while a proton moves slow to the left, to balance monumum
but not velociety.

If force is a function of velcoiety and charge, but not mass, then total
force won't balance beause total velociety doesn't balance.

But I think force must balance, to mantain conservation of momentum.

Yeah, e and p have equal and opposit force and monentum is perserved.
If the force was any different, monumum would not be perserved.

But not energy!  The equal and opposit force accelerates both and creates
kenetic energy.

But, force at 90 deg to motion, doesn't change velociety and doesn't
change kentic energy!  But WOULd mess up conservation of momentum!
So one sided force used to curve the path, will mess conservation of
moneumtum just as a bounce does, which is why I reset it after a bounce.

Ah, but what if the force causing me to curve, was corrctly balanced by
forces on all other particles that "adjusted" for momeuntum!

Ah, but that is just what I'm doing now really!  Sort of.  Ah, probably
not!

Yes, the balance is preseving conservation of monumtum, BUT the
distribtion of the monentum across all the other particles is not balanced
the same I bet!  Yeah, if one particle made me curve right, and other
curve left, my not change might be zero -- but each of the individual
particles would have been subjected to opposing forces!

---

Fuck, that is an idea.  We know that a single charged particle moving
in a magnetic field only has forces at 90 deg to it's motion.  So how
do we make this 90deg true for all motion of all particles that interct
by magnatism?

My current approach creates 90 deg force for the relative motion of
two particles.  But makes it equal and opposit.  So the actual force on
the particles actual indivdiual motion is not 90 deg.

This was all done to make forces equal and opposit.  But what if they
all balance out equal and opposit on their own somehow if all we do is
add 90 deg force to actualy frame of reference motion to start with?

And can we do this, so it works the same magicaly in all frames of
reference?

Ok, so particle in middle of loop of current is receiivng a little
mag froce from every circulating current charge in theory.  how does
that work?  We have the velociety of the charge and distance contolling
the force.

Ah, in my two particle test I was experimenting with, I didn't force
the total montumum of the system to be zero!  So to get the system
to have equal forces, I had to use relative velocity of the system!
Which forced a sort of equal and opposit balance.

So, I need a force equation that 1) always produces 90 deg to the
velcoiety in the curret frame of reference (that's easy).  But also
always sums to zero over all paritciles in the system!

So the amount of force on A, will be based on distance to B, and cross
product as needed to make 90 deg. But also maybe, on velocilty of B?
But does it maybe only need to be based on velcoiety of A that is running
parallel to me?  So when A is moving straing at me, it creates no force
on me?

I think that's what the equations are already doing with v1xv2xr.

But mayyyyybe, Xr shouldn't be in there?  Maybe the direction of my
magnetic force should only be based on ...

--

But, no. The whole point of my testing was that total force has to balance
no matter what frame of referece we are on.  so adding a constant offset
to all vextors, had to produce a sum total of zero.

Ah, but that's NOT what I tested for!  I tested that the force didn't
change with the frame of reference.  But maybe it's ok and required that
the forces change with the frame of reference as long as the total forces
all add to zero!

Ok, force vs velcoeity vs momenutm again.

Velocities don't sum to zero! So forces based on only velocities, not
going to sum to zero as a result either.

Velociety frame of reference not the same thing as monumtum frame of
reference.

Ah, but it is!  if we add a constant velcoiety to all oobjects, we make
the center of mass movement change by that same amount so we have changed
total monentum by the same v*(total mass).

Well, lets stop guessing. Lets code the damn formula as all the phsics
books tell us to, and see what happesn to TOTAL force in the system!

Ok, right off the bat.   I see an issue.  Two particles, moving in
different odd directions. if the force on one is v X V x R, the direction
of the force vector will be 90 degree to my velocity. But the direction
will be in line with the r plane. It will not balance the vector for
the other particle.

BUT, with this odd v vectors, we don't have zero monumum.  If we adjust
velcoeity to force zero monemtn, does it foce the mag vectors to balance?

Ok, I can't tell by thinking, and I don't want to try and work though
the math.  Time to code and test.

----

[6:25 PM]

Modified mag test code to work with world[] of N particles. Set up for
three now.  Summing total mag force.  Not summing to zero!  But, what
does it take to sum to zero!!! That's what I need to figure out!

================================================================================

2016-09-30 9:55 AM Friday

Yesterday

1) Exploring ideas of how to implement magnatism and how to think
about what it is really doing.  My current code doesn't seem like it
can recreate basic effects correctly so it doesn't feel like it can
be correct.

2) modified the magTest() code to work with N particles instead of just
two and to calucate sum of all mag foces -- mght the sum over a set
zero out even when two particles in the set don't?  That's the question.
And, whether it should sum to zero only in a stable inertia frame or a
stable monumtum frame?  etc.

Today

Not distracted by Facebook yet.  Getting startered sooner.  Yesterday I
wrote a long long set of posts to help Eray understand my position on
economics and Basic Income.  And by god, if he didn't end up agreeing
with the conclusion about labor share being driven down while captial
share rises.  But delayed by start to looking at magnatism forces again
until late afternoon.

Getting an early start today on magnatism!

Had my vitamas already!  Coffee.  Bananna.  Lost 1lb since  yestercay
and down to 222.2 lbs.  Good becaues I worked hard to not eat much the
day before and didn't drop weight! Going to try and keep up the push
until I get below 220.

----

[12:14 PM]

Ok, just wasted a few hours on Basic Income stuff on facebook.  Need more
coffee, and I must dig into this magntism stuff...  Oh, BGOP board
meeting tonight as well.

--

Ok, much complex shit happening in my thinking about magnetic fields.

The problems, is that the system must conserve energy, and conserve
momentum.  It can't be a free energy source.

I thought that meant that the magnetic fields between to particles had
to balance in force.  The force had to be equal and opposit.

But, maybe that's wrong. I need to better understand what's needed to
balance total energy and total monumtum.

For example, it might be ok if two particles don't blance, as long as
a large system of particles do all balance out.

Of course, two particles alone have to balance if they are the entire
system, but that may not translate to using the same math for 10
particles.  Maybe.

So I've had the question as to whather magentic effects create a new
energy store.  So instead of just haing kenetic energy, and potential
enrgy of the ES force to balance, there might be a third factor of the
potential energy of the magnetic force to balance in the equation as well.

We know that capactiors and inductors both store energy. And the magenetic
field is said to story energy even when current is not flowing (at the
electic zero cross over point is maximaul magenetic field strength).

But is that energy IN the magnetic field at the lowest levels I'm tryingt
to code, or is it just the same kenetic and potentical engey we alrady
measure?  Aka, in real coils, might it be stored as kenetic or potential
energy in the atoms of the iron core of the coil?

Part of the issue with tha tquestion, is that I've read many times that
though electic and magnet fields trade off, whether we "see it" as one
or the other, is a frame of referece question. Which seems bogus --
aka, our frame doesn't change the unviese, it only changes what it looks
like to us. Meaning howeer the enrgy is stored, it must the same in all
theortical observational/measurment frames.

If it's not a new source, then we need to think of it as a tweak to the
old source.

But clealry, the old ES field is based on distance alone, not velociety.
What seems to be happening with the magnetic field, is that we are adding
a force based on velociety.

But if it's not creating a new form of potential energy, then all that
must be ahppening is that velociety is shfiting the potential energy
from one place to another.

This implies that looking at potential energy on distance alone gives
us the correct answer to total energy. But that shifting the frame of
reference, gives us what LOOKS in the new frame as different answers
when it's not!

Ok, so here's a clear problem.  Static particles, with no motion, but
lots of potential eneryg.  Then we shift to a new inertial frame of
reference by adding a constant velcoiety factor to everything.  WHAM --
the total kenetic energy changes with no change to potential energy!
Just from a frame shift. That seems wrong.  Seems like me might be able to
"fix" that somehow?

But, this chagne is only a shift into our "percpetion" of total energy.
It doesn't change the behavior of the system at all.  We can clcuate all
the motion under the new frame, using the same math, then translate it
all back to the old frame, and get exactly the same resuls.

The math remains invariant to frame of reference change.

But if the math of magnetic fields is based on velocity, it won't remain
invairant of frame shifts!

This is what led me to use relative velcoiety to do all the calcuations.
Which still seems valid.  Assuming it actualy produces the rigth
answers. But that's the problem. My math didn't produce what seemed to
be valid electomagnatic effects -- like wires pulling together.  Like
electons spinning in a magnetic field created by circulating electons.

If total force on a particle is defined by ES 1/r^2 equation, and all
motion did was change the direction of the force, then we would not
be creating a new energy storage source.  That seems clean.  And seems
like something to check.

if motion changes total force, I think we are scrweed.  Or, better said
motion would be a potential energy store alone with distance.  But
sepawrate from kenetic energy store.  Or- would redefine what the kentic
energy question was.

--

Side thought -- e=mc^2 is not e=1/2mv^2 and this has always bothered me.
Why the missing 1/2? This might be a path to unerstanding the extra
energy story of motion?  1/2 mv^2 is kenetic, and 1/2mc^2 is magenetic
potential energy? So at slow speeds, all the enrgy is kentic, and the
formal is 1/2mv^2. But at high speeds, half is kentic, and half is
magnetic making it e=mv^2?  Just random wild thoughts...

--

Ok, so, what does it TAKE mathematically to conserve energy and momentum?
That's an answer I don't realky know. Not as well as I should.

If motion creates a new additional force in the system, then the new
vector foces on all particles must sum to zero in order to conserve
momentum.  Yes, a constant force adds constant monumtum over time to any
mass particle.  So if we add it one pace, we must blance ith by adding
the invese someone else. So the sum of all force vectors must be zero
to mantain monumtum.  But not constant summed velocity.

But, when there is more than one particle, it could sum over the whole
set. There is no requirement that pars sum to zero.

If pairs all sum to zero force, then we know the set must sum as well.

So yes, it's true that forces on indivudal particles don't need to be
paired as long the total sum to zero.


--

Quick code test.  mag test from yesterday using 3 particles.  Switched
to all three being electons to see if the mass consistenice made total
force zero or constant. It didn't.  That was with a force that was NOT
based on relative velocity, but on actual particle velocity instead.

But going back to the old relative code, total force is zero all the
time of course -- since individual pair forces are the same with that code.

--

Rule 1:  Total sum of force must be zero to mantain constant monentum.

If motion rotated the direction of the es force, but didn't change it's
magnatude, then we will maintain monentum, and not change the potential
vs kenetic energy balance.  I'm pretty sure.  It's like changing veceityh
vectors doesn't change kenetic energy onl paths.

Rotating force vectors will only change paths, not total eneryg. yeah
beuase DF/dt will still be based on distance! Change in force will be
chagne in distance so chagne in kenetic energy will be change in distance
as well. Right?

This path of thought seems very clean with the 1-v^2/c^2 stuff becuaes
it doesn't rquire that we code the balance into the low level force
equation -- we mantain the balance by just rotaing the force vector.

This would require coils to work by transfering kenetic eenrgy from the
current flow to the "magenet" aka kenetic energy stored in electonic
orbits I guess. That seems ok. The kenetic energy of the orbits is
creating the "mageneti field" that is holding the energy when tha the
current crosses the zero voltate point.

Oh, wait, current and volate is 90 deg out of sync in AC capacitor vs
inductgor circuits. That's why they hold energy when there is no current.
sort of.

They are 180 deg out of sync.

So how the fuck would that translate to force rotation in the interaction
between two particles?

Capactors are build up of charge being more 1/d^2 ES push back to the
current.

Coils work by the faster the voltage CHANGE the more push back they create?
So at the zero crossing, they are creating maximal push back?

Intergrate current ot get capacitor push back.

Differentiate current to get coil push back?.

Capacitory behavior is EASILLY expalined by the ES force.

But coil behavior I'm lost to explain with force equations.  This is
what I need to explain.

Capactiros translate energy into distance.  Coils must ranslate it it
into something else. If not a new type of energy store, it must be
a Velociety store.  We call it "magenetic field" but I think that's
BS. I think it's velociety store in the magenet -- is just the motion
of electons in the atoms.

But, that's what it needs to do.  Translate electic current flow in
the coil, into motion in the atoms. To transfer kenetic energy from one
particle in current flow, to another in the atom.

So we talk about magenetic fields acting 90 deg to motion which prevents
KE chagne, but chagnes path. But that's BS.  If all magenetic force
worked that way alone, then magenets and coils and transforms would have
no ability to transfer energy. I don't think.

Ah, but yes, wait.  When we have an electon in oribit, ti's KE is constant
because the distance force is constant.  If magnatism forced the path
to change, then that would force it to slow down or speed up due to the
path change and the ES force.  Changing path, can chagne whethr the ES
force is moving energy.

So, one way to look at this, is that magnatism chagnes paths and the
path change forces the normal ES force to move energ.

Capacitor force is in liine with current flow.  An electon moving
forward into a pile of electons, will be pushed back.

But coil force, is not inline.  It's some sort of side by side effect.
The motion of the electon running parllel to me pushes me back.

So, two electons passing in the night.  I have thought of this is
needing an incrased sidwise force to explain magnatism. But what
we might really need, is increased in axial force.

So, two electons passing in the night:

  V1<----e1
         ^
	 |
     ES FORCE
	 |
  	 v
         e2----->V2

Then need to act against each other, increasing push back/drag!

  V1<----e1 -> Mag
         ^
	 |
     ES FORCE
	 |
  	 v
  Mag <- e2----->V2

But, if we are not adding a new potential energy store, we must reduce
ES force by however much we increase MAG? So total force is still based
on 1/d^2.  The total potential energy can still be calcuated on distance
alone then.

So two electons that pass each other are being pushed aprt by ES force.
But the fast they do, the less it pusshes, and the more the two act to
slow each other down.  Or, if one is going faster, it speeds up the other?
No wait, wouldn't it "win" making the other stop?

Ok, sign might be the wrong way here.  A quick look at parallel currents
in wire seems to suggest tha tmight be true.  The electons in paralel,
have no change so it pushes apart just as hard as normal. But the protons,
that atract, will attract less?  UGH. That doesn't work either.

Ah, maybe this does work with absolute velocities?

OR, the effect is the same regardless of the sign of the charge?
TheY act against each other no matter what the sign, but ES reduces?
Or, they attempt to cross transfer kenetic energy sideways no matter
what the sign of the charges?

The amount of transfer is relative to the angle between r and v?  As we
are doing with cross products?

But no, it must be a function of V as well. Doesn't it?

So, it seems the mag force needs to be a function of velcoeity slowing
the paricles down, based on how near to the speed of light it is.
And the more we add mag force to slow it down, the more we must take
away from the ES force to keep total force the same so as to not create
a new potential store.

Ok, but what if we didn't subtract from ES force.  Then relative
velocities would be a potential energy store.

Oh, fuck.  Orrr, if the total monentum was zero, the store would be zero?
All the relative forces would cancel out in the system?  But if the
system is moving, then it's an energy store -- sucking kenetic energyh
out of the system -- slowing it all down!

So, in a different frame of reference, how much energ ssemed to be stored
in the motion kenetic energy, would be hidden, from the relative numbers.

BUT... That works now already. If we set the system into motion, all the KE
numbers change even though the PE stays the same.  But if we adjusted
by calcuating the motrion of the center of mass of the sytem, that would
give us the missing KE.  I assume. And that's all without magnetic forces
at play.

So what do we add with magnetic forces? ???

--

Ok, so like particles crossin the night get pushed APPART by magnaitm,
not together.  Unlike get pulled TOGETHER.

  V1<----e1 -> Mag
         ^
	 |
     ES FORCE
	 |
  	 v
  Mag <- e2----->V2

So if mag acted in this direction, on the chart.  And ES didn't change
when Mag increased, then the effect would be to make them take a path
that curved apart sooner.  That seems correct.  like partices curve
apart on a tigher path when they pass in the night like this.

But that seems to be true even if ES is reduced in force to make totl
force the same.

At the speed of light, the Mag force wold equal he ES force.  In theoy.

So it would looik like this:



	   total force 45 deg.
           /
          /
  V1<----e1 -> Mag
         ^
	 |
     ES FORCE
	 |
  	 v
  Mag <- e2----->V2
        /
       /
 Total force down to left

[BIG EDIT LATER -- NO NO NO if ES is reduced by what we add to MAG, then
ES is zero and we twist the force to make it 90 deg from normal]

Or, if we think of mag force as CHANGE to ES force needed to do this,
what does it look like?

In the above, ES is x units up for e1.  Then it rotates to 45 to the right
still the same length. So the chnage force is (1-.707, .707-1.0), x, y.

Oh, fuck me.  The length of the change vector becomes sqrt(.5) == .707.
Let me write code to verify this.

Oh, no.  The length is .765367 ish  Which is actually mag deltal to force:
(0.707, 0.707-1.0) I was wrong above. Where .707 is sqrt(.5) aka sqrt(2)/2

For a second, I thought sqrt(5) just showed up.  Which fits in with
sqrt(1e-7) question from the constants of these equations.  But no
it didn't.  Not really.

Ok, so the thought here, is that relative velociety parallel to the
r axes between two particles generates a twist to the ES force to act
to slow down the approach? But weaken the ES force by the same amount.
Or speed up the depature?  or slow down the departurer as well??

Yeah, Relative force has no depature/approach concept. It's just motion.

We can draw it as one paricle standing still, and then a velociety vector
for the other, in any direction.

So we take the part of that "any direction" relative velocity vector, and
find the magnatude parallel to the r vector (vxr).  And we add a new force
vector to slow it down -- take ke out of it in that direction.  And the
magnatude of the KE, is the same as SE, (kcc/r^2), adjusted by speed of
light? sqrt(1-v^2/c^2) sort of thing? So we so we add force to slowdown
the motion parallel to r.  But adjust the ES force to make total force
remain the same.  We twist the ES vector, but don't change it's strength.

This adds a type of headwind to the system to reduce KE buildup. Or,
which means, keep particles from getting too close?  Oh, I wonder.
Could this create an effect that looks like the strong force?  When e
p get close and fast, they repeling force between p and p starts to
vanish letting the ep attaction dominate or something?  Not seeing it,
but could be something there.

Or, depending on frame of reference, does this really just make all
particles want to go the same speed? The fast one try to speed up the
slow ones?

So, what does this make the formula?  Maybe I'll go to the code and try
to code this idea.

[3:07 PM]

---

Relalized while coding.  If two particles pass at speed of light, it
would force a full 90 deg twist to ES using it's full energy to slow down
the formotion and no force to keep them together, or push them apart.
Back to code...

----

[4:19 PM]

getting lost with the math trying to code it.  Having second thoughs about
whether it should always slow down foward motion since this requires a
conditional sign flip for +- vs ++ or --.  That doesn't feel right. Not
elegant enough.

Another note.  Since ES is c^2 vs Mag being v^2, then both compute to
produce the same total foce at the speed of light without having to use
c in the code at all.  We are just looking at the ratio of mag to ES.
Or, we just calcuate the Mag force and subtract that from the ES force
to get the right twising effect in total force!

More thinking takes me back to liking the idea that the mag force is
always acting to move ke from fast to slow -- working to act against
direction of motion. I guess just try to code it and not worry about
elegance of how I code it.  Maybe the math will slimplify later.

---

Oh messy messy.  Subtracting magnetic force from ES vector doesn't keep
the magnatude of ES the same! It doesn't just twist it!  It turns the
rotational circle into a dimaond. Not the same thing.

So now what?  My logic is all broken.

Lets start again...

To make parallel wires attract, the electrons moving together will
have no effect.  But the ep will be moving relative to each other.
relativity says distance shrinks in direction of motion.  So ES pull
gets stronger logically and shifts dirction slightly for each individua
distant particle.  Which doesn't matter for the infinite straight line.

Ah, the relativity view of this argues the appracohing proton will
act as if it's distance is contracted in the direction of appraoch.
So that makes the ES stronger than it would be.  Indicating ES force is
not maintained as a constant in this view.

BUT, relativity also argues for mass and time changes and I don't know
how all these work together in on example like this.

Maybe it all adds up to being the same as my ES vector rotation idea?
Not totally impossible. I guess.

So magnatism would need to rotate the ES vector relative to the ratio
of Mag force vs ES force.  And in this eample of a + appracohing a -,
we are talking about having to rotate forward to increase the parallel
attraction between the paths while approaching, but hum, rotate BACKWARDS
on departure.

That does not feel consistent with the idea of acting to speed up or
slow down the approaching or departing. Or consistent with the idea of
transfering kenetic energy as they pass.

--

This is feeling much like AI work. Around and around in circles with
the same abstract ideas trying to get a combination that sticks and works.
Just sayaing.

--

If current works as it seems to on face value of moving electons relativ
to moving holes, then when e and p pass in space increased velociety has
to make them attact more.  Probably the whole way -- both when approaching
and departing.  Tilting the force vector forward on appraoch, but
backwards on departure.

--

Fuck, I really like the idea of rotation to prevent the creation of
separate potential energy field based on velociety.

And I really  like the diea of magnetic forcing acting 90 deg to all
ES forces.

If velociety alone created change in path that would make an easy to
define potential enrgy story But it would be in direct conflict with
simple kenetic energy.

But if it roates, then the paths are odd, but the energy is still
easy to calcuate.  But of the energy was easy, would we need all the
comoplexity of relativity? Maybe.  No one has suggested relativity
vilolates conservation of energy in any frame.

So particles only interact with other particles.  Seems like a basic
optential valid idea for the univers. It's all particleal realtive.

So using relative velociety to calcuate rotation of ES field seems
plausable.  So lets just use the v^2xr to define the force of the ma
field, and then think of this is rotation between the c^2 vs v^2
directions.  Ah, but them the field only rotates 45 deg as I was
thinking once, not a full 90.  But if "energY" was taken away
from the other direction, it's a full 90 deg.

----

[10:32 PM]

Whent to BGOP board meeting. Just got back.  Will go to forge Sunday
with Phil.

Had an idea in the car.  The diamond pattern of vector "rotation"
I'm thinking might actually be fine.  They are kinda like v^2 terms.
So the sqrt(a+b) is constant when you add to one and and subtract from the
other!  It might be closer to same as 1/sqrt(1-v^2/c^2) than I realize.
The magnatude of sqrt(force) I think is maintained?

f=100

sqrt(f) = 10

fmag = 20

f=100-20=80 vs 20

sqrt(a+b) still equal 10.

But magnatude is sqrt(80^2 + 20^2) = 82.4

ah, gee.  No, this is complex.  It might work, but it would be for
complex reasons if it did.

It's worth coding and playing with I think.

================================================================================

2016-10-01 8:35 AM Saturday

Yesterday

1) Trying to come up with idea of how to code magnatism, but yet make
standard magnetic macro effects show up, but also without breaking
conservation of energy and monumtum.

2) Coded ES force rotation concept.  But turned out to not be rotation
but rather X vs Y shift.  Left it running in a few 2p2e tests.  Three of
the 4 kept the two e together.  But has gone into tight orbit with very
slow simlation on one ep pair.  Second e has flow off.  This is with
energy reduction on bounce as well.  The thrid is about the same but
with about 50% further separation of the ep pair.

Today

Not sure if the code I produced is bug free.  Didn't spend enough time
testing and verifying it and undertanding if it even makes any sense.
Need to double check the code is both doing what I think it's doing and
to verify thta it makes sense to do it that way.

Need to keep thinking on this.

Why don't the published magnetic attaction formuals work when translated
to the particle level?  What am I missing?  Why are they so frame
dependen?

The implication is that changing frame can turn magnetic to electostatic
but still produce the same results. However, electostaict is frame
invariant so it's not doing that.  I'm I missing a matching electostatic
frame being enduced by magnatism?  Is that it?  That does make some sense.
It can be understood as the "adjustment" to the ES field like I just coded
on this last attempt.  So as to keep total force the same?  That would
explain why it looks all wrong but yet must be right since this is well
studied by endless very smart people.

So, so, what is the missing electostatic?  Do I just need to reverse
the electostatic so as to offset the calcuated mag as I was just doing?
Or correctly rotate it?

Again, it doesn't seem valid that the magnetic field can be allowed to
change due to frame of reference.  It makes no sense for the universe
to work like that.

Real world macro level magnatic effects are highly frame invariant at
our macro level.

Another thought.  In line with the parallel wires attracting.  Two magnets
attract.  So if a current loop creates a magent, then two of those
parallel will be two magenets held together S to N and they will attract.

So, P standing still with E moving, must attract!  Which means the
motion causes the attraction to increase.  Which has odd ramifications
for orbits.  At lest this is true if frame of reference invarance holds --
which it MUST damn it -- at least for low speeds....

---

facebook UBI distractions.

---

Thinking about current flow.  The electons won't be constantly moving.
They will be stopping and starting as they jump from atom to atom.
I wonder if this plays some role?  That the velociety has to increase
then slow down?  Or, could be a fast orbit, where it has to speed
up to exit, then slow down when it's recaptured.

My thinking (and the equations) tend to ignore the variation implying
that the total sum is all that is important. Which makes lots of
logical sense.

But, hum, what if magnatism was not as much a true force, but only
an odd side effect of relativity distortion?  That's the entore point
of magnatism was not that it was fundamental as much as the fabric
of reality was not so simple, and the bending of the fabric of reality
was what created the force as a side effect?

This view of course seems to be what people claim to be true. But
I don't think I've allowed myself to accept it.  In fact my entire
approach here was to think reality really is newtonian and that magnatism
is just an emergent property of some fundamental newtonian features.

But in that view, I think I've always seen magnatism as fundamental
anyway, and that reality distortion was just something more complex
added as well.  Because we do have fundamental equations to describe
magnatism without adjustments for relativity.

I'm not giving up on my seerach for a newtonian answer just yet, but
it's good to open my eyes and thing in the other direction!

But "reality distrotion" really just means that from the frame
of refference of a single electon, what does the rest of reality
"look like".

Or, if an electron moving towards other particles means the particles
act as if they are closer than they seem, then why not code magnatism
that way!  So, the approching proton just seems to be a different
location, but the total force on me, is still calcuated using the
same ES equation, with an adjusted location of the particle.

Ok, so if the real distance is X, we can calculate the ES force.

But if the relative distance acts like Y, then the force WON'T just be
rotated! It will be stronger!

So, relative motion, makes the particles act like they are closer
together!  But only in the path of the relative motion.

Which implies that two particles when they are passing side by side, there
is no additional force.  Contraction is only in the direction of motion.
So the sidewise force is created as it approaches, and leaves, but not
when it's side by side!

This seems to be a ver different rule.  And oh, look, such a rule has no
effect on an orbting pair, beuase they are always in constant parallel
motion (well for a circular orbit at least).

Hey, might this force high speed eliptical orbits into circles???
That could be useful.

Ok, this seems possible!  But it seems totally different than the
magnatism code I've been using!  paritcles far off won't have much change,
but the change will get stronger as it gets near, but then fade away,
and be no difference when side by side.  Very odd and different from
what the magnetic code was implying!  Which is good, since the magnetic
formulas weren't working!

So what is the correct forumal for this?

I need relative motion.  Easy.  Got that.

----

Facebook distraction.

-----

[11:08 AM]

Start with relative motion vector, v.

r dot v is the distance to be distorted.

Then shrink that by lorenz contraction. r/sqrt(1-v^2/c^2) maybe?  No,
that's r/0 at the speed of light.  Wrong direction.  I guess r*sqrt(...).
That's r for v = 0, and 0 for v c.  That seems the right direction.

Then find the other axes, which is r cross vHat.

Fuck, what we are looking at is probably just delta r aren't we?  dr/dt is
zero when passing, so no distortion. And dr/dt is small relative to the
r when far away.

Which is just v dot r isn't it!

BUT, the space distortion view tells us the direction of the vector
changes as well as the strength.  But if we think dr/dt just changes
the strength is changing, we won't get the same results.  But relativity
says time changes as well space, and maybe mass.  Maybe all these complex
effects add up to being the same as if ES force changed with dr/dt?

But on the apporach, dr/dt will be negative. And on departrure dr/dt
will be postive.  But yet we need both to make ES stronger?

Actually, to be clear d|r|/dt is what I'm talking about. A magnatude
change.  Not dr/dt the vector change.

----

Oh oh oh.  It's not the change in distance alone. It's the change
in electic field that change in distance is creating!  So dt/dt is
creating de/dt!  And the stregnth of the "magnetic field" is based
on de/dt!  So even when traveling at the speed of light, a very distant
particle is only creating a small de/dt so only a very small magnetic
field distortion!

But as it gets closer the effect gets stronger!

And this is why all the talk about a changing electic field will create a
changing magnetic field and vice versa.  create a changing magnetic field!
I'm guessing...

So, can this way of looking at it help?  What force does the de/dt create!

---- Ah, going back to check Maxwells equations.  finding Ampere's law
that says B(curl) is based on two terms -- which is both the current
density flow AND de/dt.

OK OK OK.  the b = qv/r^2 crap only works for CONSTANT velociety current!
That's all most the sites are talking about. But there's a a dv/dt
term in there that's been left out!  So if the current is changing,
the magnetfic field is different!

Or, translated to particles, if the velociety is changin, it changes
the magnetic field???  Oh, this gets so complex so fast.

But look, a particle standing still in the middle of current loop will
expernce no effects! Only when it moves relative to this "magenet field"
does it have an effect. And only when it moves, will it experinece any
de/dt due to the particles around it -- beuae they are all moving at 90
degrees to it and maintaning a constant distance.

But that's when it's at the center.

When it's at other lcoation, the same should be true -- not total magnetic
effect becuase the particles is not moving relative to the stationary
fixed magnetic field!  But there will be lots of little de/dt effect at
thiat point because all the elecotns are are in offset circular orbit
around this particle.

But they will all be paired and equal and opposit!

Very fucking cool.

So maybe it's possible that total de/dt is the answer here!

Or, for just two particles, de/dt.

But but but.. straight wire, electons approaching and leaving.

De/dt flips.  Ok, so again, a charged particle standing still will see
all the de/dt effects but if they pull stronger on approach and weaker
on departure, it will pull us towards the approaching particles.  But it
should have no effect on us if we we were standing still.

The length contraction concept implies they will pull stronger on approach
but also pull stronger on departure.  But again, that's Broken with
respect to the idea that the particle standing still shouldn't move at
all.  If all the ES fields were canceled out by matching protons that is.

Well, if we find a way to justify thworsing in a (de/dt)^2 or some such in
there we get the abs() function without "cheating" and just coding abs().
Then we get length contraction like effects.

But then that implies a single charged particle next to the wire with
the const flow will be attracted or reveled from the wire.

AH AH AH!  But if it's a wire with matching + and - the attraction and
push will be equal and opposit! So the wire won't be attracted to the
other wire!!!!!!!!!!!!!!!!!!!!!!!!!!!

But once the electons in the second wire move, BAM!, the enetire wire
sees attraction or push.

So, this is suggesting that (de/dt)^2 makes the force increase, whether
it's + or - force.

So, a charge particle inside a current coil, but not at the center,
would be forced to move. If this is true.  But not at the center.
Electons circulating, will make an electon in the center, fly away??
Because they would push harder against it?  But as it starts to move
then.....  Complex shit unfolds?  Ok, wait.  it's near the center
and starts to move away from the center due to the forces.  The ones
that match it's direction don't push as hard anymore and the ones
on the other side push stronger.  Yes, it seems to curve in the correct
direction!  Away from the fast moving and towards the slow moving,
which makes it spin opposit the direction of the circulating electon
flow.

But a nurtral charge particle like atom, won't move in these cases.
The forces will all balance.

But magnetic fields aren't supposed to have any effect on a charged
particle unless it moves! WTF WTF.  Is there no ansewr that works
for all cases? :)

Ah, a changing magnetic field, is like a particle moving through a
static magnetic field. Maybe. The particle is "crosing" lines of flux
logocally? aka lines of flux are sweeping over the particle!  So it
creates a force on the particle and makes the particle move.  Oppoist
the direction of flux changing???? Is that how it's supposed to work?
Or is that logic only applying to electons in wires paried with matching
protons that experence no net force unless the magentic field is changing?
Maybe so?

Maybe this is why it's all been so confusing trying to understand these
questions.  Maybe the whole notion of current is invalid when we are
talking about the intereaction at the particle level becuse current
is a macro effect that ermg3d from large sets of intercint particles
including matching protons in a wire?

Yes, yes, see, current in a wire of matched + and - has no net change
in electical field!  even if at the particle to particle level they are
all changing!

Or maybe the correct undestanding is to ignore the current side of the
equations and look only at the de/dt part, WHICH was totally left out of
many of the simple exmaples that were only describing constant current
flow examples!

So current is spcial, but not in any complex orbital ways, only in the
fact that it's matched sets of + and - particles locked together in
a path.

So, if I can come up with a de/dt equation for magnatism, that correct
explains current in wire behavior of magnatism, them maybe that's the
answer!

So, my current best guess is looking like de^2/dt creates an incrase
in force for whatever direction the force was to start with.

----

Lots of daydreaming..

----

AH.  Inside a current loop, the magnetic field is not constant. It's
got a slope!  A charged particle not moving, should not be effected by
a CONSTANT magenetic field. But the field in this loop is not constant!
It's slooped.  So maybe that counts as a curved hill that the paricle
will roll down!  Making the behavior of the charge particle trying to
roll out of the loop correct!  Or the behavior of a postive particle
rolling to the middle -- like in a bowl.

But if the magenetic field was constant in flux everywere, the particle
wouldn't move!  Maybe that's how it's supposed to work!

And once the particle starts to move, then it all gets more complex!

So maybe the behavior of the single charged particle being moved by the
field is a correct concept?  For the full maxwell equations.  Even if
not correct for the simplified current formulas!

Ok, lets pretend this might be right.

----

Or, a gradient in the magenetic field might be said to create an electic
field!  Forcing the particle to move.

Ah, and a constant flat magenetic field is like what find right at the
center of the loop (at the hill top or hill bottom) and won't make the
charged particle move!

So, all we need to code, if this has any truth, is that the ES force is
increased as a function of de^2/dt.  So, step one, I guess is to figure
out how to calcuate de/dt or de^2/dt.  Then try to figure out the formula
and units for the increase.

You know, if there was a speed of light delay in the information about how
far away a particle was, ah, no that would cause the increase to lag --
for it to be less force. Strike that idea.

So, two particles of the same type comming together has to climb the
energy gradient, but the faster they more, the stronger the energy
gradient! Like climbing a hill with friction sort of? But goign down
the other side we see the same modified hill.

Or rolling down into a valley, the valley gets deeper as we roll, but
climbing out, again, same valley in reverse.

But, no distortion at the top or bottom?  So it's like the sides got
wider but the top is no higher or the bottom no deeper?

This is a very odd and different effect than what I was thinking
magnatism was!

----

Fe = c^2 q1 q2 / r^2
dF = - c^2 q1 q2 / 2 r^3 dr
dr = v dot r dt

dE/dt = (- c^2 q1 q2 / 2 r^3) v dot r
[EDIT ERROR the 2 should be on top not bottom, r should be rHat:]
[And left out 1e-7 but I basically knew that]
[and E is F/q so the above is dF not dE]
[correct is: dF/dt = (-2 c^2 1e-7 q1 q2 / r^3) v dot rHat]

The math seems correct.  But the answer is not what I would expect.

Oh, no, it's correct.  If we have m, them dm/dt becomes m/s.
We had F, which is q2/s2., but our De/dt is now q^2/s^3 which is
consistent with what we should expect!

But, sign, that's a different question.

v dot r is a magnatude, not a true velociety change vector. But
if we make it the correct velocity change vector what does that
do for us?

So, E is postive, if we are --.  So the force on p1, wold need
to use (E rHat) where rHat points from p2 back to us at p1. The
other particle is pushing back at us.

if v dot r is a velociety vector, it would point back at us.
on approach but the - flips it making it backwards from what
I think we need! NO NO, this isn't the magnatism ansewr, this
is only DE!

And E will get stronger as the other particle approaches.
So DE should be postive.  Which seems backwrds.  Well, fuck that,
We can just force the signs to work as we need them to!

And then next, we need to figure out how to map DE to an
increase in force!

So DE is blah, how much force should that create?

F = v^2 q^2 / r^2 == units of(q^2/s^2)

De/dt is now units of q^2/s^3.

So to get force, we need to multipy by time, in some way.  It seems. Or,
integrate.

I have no fucking clue how to move forward from here.

Maxwells equations using a 1/c^2 dE/dT term.

How much extra attraction needs to be added?

I tink I'm stuck in a complex situation of having to do some integrals
to map the equation back to other forms and see what the answer is in the
other forms?  Like the expected force between two wires?  Or something?

But maybe I can just cheat instead and do a numerical approximation to
something the other formulas tell me how to calculate like the current
in a wire force and adjust the formula until I get the right (emergent)
answer.

Is there any way in hell that De/dt is the amount I need to add?
Or maybe just De/dt  Ake, on second worth of change?

I guess a simple two particle system where on is approaching and I can
see the force and De/dt numbers would be a place to start.

I think it's time to a flu shot.  Now.  Will put off that other work
a bit.

[3:07 PM]

================================================================================

2016-10-02 9:51 AM Sunday

Yesterday

1) Lots more thinking about magenetic fields.

2) Start to look at concepts of derivitives.  Changing mangnetic field
is said to produce electic field and changing electric will produce
magnetic.  Though this has meaning at the macro level of electronics and
electromagnets, I've stuggled to translast this down to the fundamental
level of particle interaction rules.

3) Thinking about attacking from the direction of contraction -- where
the approaching particle is nearer than it seems.

4) Noticed Maxwell's equations has curl B rule based on both current flow
and partial dirivative of changing electic field.  But many other rules
only talk about current flow.  It feels like maybe there's a problem
where current flow math is all derived from the average effects of many
moving particles at different distances and fails to correctl capture
individual charge motion.  And that the places where individual charge
motion has been explained in a formual it was derived from the current
flow laws and not from real individual partile interaction!

4) Came up with idea of using dr/dt which produces de/dt to define
magnatic force.  That is, how fast the two prticles are approaching
each other.  Change in relative approach speed.  But them adjusted for
1/r^2 to create chagne in electic vield over time.  That is, the rate
at which the electic field force is changing is what defines how much
magnetic force there is!  And, that the magnetic force is in the same
direction as the ES force.  This is similar to what length contraction
does but not the same -- the angle of force is slightly different.


5) Calucated the basic formula for de/dt as:

dE/dt = (- c^2 q1 q2 / 2 r^3) v dot r

[Later edit, maybe v dot rHat instead?]

But later I learned that E is F/q.  So the above with both charge
values is really DF/dt, and De/dt is the same with one less q.

Signs however are odd and not correct.  The movement needs to always
make the force increase in the same direction it already was in.

But haven't coded it.

And I'm lost as to how to map de/dt to magnetic field increase.

6) On approach is determine the formula numerical so it matches
the current rules.  Second is integrate the above over an
infinite wire to get current laws, then adjust to make it produce
the standard current laws formulas.

7) Cool thing of this formula is that it makes passing ep paris
pull together more, but, there is NO EXTRA PULL when they are
at 90 deg to each other (de/dt = 0 at that point)!  This is
key for making electron orbits work nicely and not circule down
into deadly tight orbits way over the speed of light!  And I
think this way of implementing magnatism might tend to force
eleptical orbits into circular orbits -- which could be very
important for stablity and symmetry effects!

Today

Phil will be at forge with Jeff. Said I'd swing by.  So I need
to go do that soon.  Like now. But wanted to get notes started
for the day.  Wanted to go get a pumkin spice latte as a special
treat for the morning!  Haven't been to starbucks in a while.
Checking app -- sept 25 was last visit.  That was one week ago.
Last Sunday.  Took vitamans.  Had a banana.

My darn scale is acting wierd.  Showed first weight at 217 something.
Even getting off and back on.  But a few tries later, it showing
222 range.  Way inconsistent for some odd reason.  But my weight
is looking good this morning despite the pizza and hot pocket
over consumption last night!

Wanted to do a first quick code of the new magnetism formual
just to get started before leaving to forge.

---

Wrote first verson of Force3() basedon de/dt concept.

No time to test and debug.  Must run to BGOP

[10:40 AM]

idea; need to write esForce() function to caluculate es force
between to particles instead of using addForce.  So as to
help debug this mag force3 idea...

----

[4:33 PM]

Oh god help me. I'm posting too much political and economic comments
on facebook!

Got back from BGOP visit.  Then got lost in facebook.

Got to get back to physics!

Ok, going to look at the numbers the new code is producing to see if it
looks correct.


---

Wrote code to test basic dr/dt and df/dt.  Dr/dt was wrong due to not
using rHat.  dt/dt is just way off for unknown reasons.  Need to check
what I calcuated as the derivitave.

----

[6:45 PM]

Ok, chased down two problems.  I was using r instead of rHat.  In my
paper note scribling I had it right, just not in the document here.

Second, I just left out parentesis on / (2 * r3).

Thrid, the -2 from r^-2 needed to be on top instead of bottom so
this made it off by a factor of 4.

So, dr and dF is now looking correct for value.  Now I need
to check signs in different configurations.

----

BTW, dt/dt / c^2 is producing a value that is in the ballpark of
the ES force.  But too large.

--

Checking with 45 deg motion, and I see dr/dt is all wrong.
It's not just v dot rHat. It's far more complex than that.
Drat.  Gota do some math now...

---

Ok, fuck. Stupid me. dr/dt is JUST v.  Not v dot rHat.  OMG am I stupid!

And it's a fucking vector. So df/dt works ot to be a fector a well,
and DOESN"T POINT THE SAME WAY AS rhat!

So when we base magnetic force of df/dt, it doesn't just making the es
force fector longer or shorter, it makes it MOVE direction as well!

Ok, so I don't have enough experience thinking in vector calculus terms.
I need to work on this some more.

Changing to a vector, I'm getting a little btter answers.  But df/dt
is still a little odd.  when p0 is moving  up and to the the right,
and p1 is to the right, and not moving, then dF/dt needs to point right
and DOWN.  Relative to p0, the next force vector will be longer (beause
we are closer) and slightly down to the right. So the df/dt will be down
and to the right. If this is done correctly!

No, if p and e, then force for p0 will point towards me! So next is up
and to the left for df/dt. My signs seem reversed.

Ok, so I have to look carefuly at signs in both test code and debug to
see if it's all working correctly!

But this is cool.   That the df/dt will be a different dirction than es
so when we add them, it will be more than just a change in magnatude.
Which sounds like what length contrcation was' asking for!  I'll have
to check and see if this is moving the force in the same direction as
length contraction would.

---

Oh, finished watching firefly, and watched the Serienity movie
last night!  Still got a few more episodes to go for Angle.

Then the Josh stuff will be all done.  Got to go finish person
of interst, then there's blacklist.  Which I might not need to
watch.  It's new and I was just watching the first seeason+ I
had missed.

Need food and TV now.  Will get back to vector calculs later..

[7:30 PM]

================================================================================

2016-10-03 10:37 AM Monday

Yesterday

1) Write and debug new df/dt code.  Found many errors.  Including a big
conceptual error in that I thought I was talking about df/dt but was
actually talking about |df/dt| -- just the change in magntude of df.

2) Wrote test code to help debug the df/dt code to do 1)

Today

Finished Angel TV series yesterday.  So that completes the Buffy, Angel
and firefly treo.

Physics

Well, I have to think about what I want to be calucating now.  Is there
a use for df/dt vector?  Or should I go back and implement the change
in magnatude idea?

First though, I think I want to continue to debug the code to make sure
I'm calucating the vector version correctly.  And I want to make sure
the sign is correct.  Then when I feel I understand the vector derative
I can consider the theory of what I want to do.

With the vector, it's no longer zero when the particles pass.  In fact
it might be at it's largest value at that point.  So this is not the
cool idea I was looking at at all.

----

Reviewing http://cds.cern.ch/record/920085/files/0601028.pdf

Whicih is "Relativistic derivations of the electric and magnetic fields
generated by an electric point charge moving with constant velocity"

I learned that Beta is v/c in special relativity.  So 1-b^2 is just
talking about 1-v^2/c^2.  I didn't understand that when looking at
this document before.

And it has a formula for the magnetic field as:

B = V x E / c^2

Which I didn't understant. but seems to be defining what the magnetic
field at any point is, based on what the Electic field at that point
is, where V is the movement of the charged particle that created the
electic field!.  And that this simple formula works under relativity
as well.  But the calcuation of E, is more complex under reltivity due
to contraction.

But I'm still confused about what frame of reference this is all
measured in.

Then, I'm assuming, if we know E and B at that point, then we can
calculate the force on a charged particle at that same point, baesd on
E and B and the motion of that particle.

So, let me look at this formula for B and comprae it to the other ones
I've seen and tried to code!

--

Ugh.  Ok, it all seems to just go back to the same formuals I was already
trying to use before. (but which never included relativity adjustments).
And still, I don't grasp any of it.

If we take the frame of referece to be p1, and p2 is moving, them we
can say p2 is generating an electic field and at p1, there is a magnetic
field from p2 motion. But that since our velociety is zero, the magentic
field creats no motion on us.

Again, this might be me failing to understand how all this is expected
to work. I already decided to assume that if p2 is moving and creating
a magnetic field, it would be valid to think of this as if the magentic
field was moving relative to B1.  And that we were moving though it,
and as such, there would be force.

But damn if none of this makes sense.

But, that new page is making it very clear that relativistic calcuations
would cause E to warp and not remain the same magnatude or direction!
Which makes B warp as well.

And, I still hold on to the idea that all this might only apply to macro
level electomagnatism where the fields are being transmitted from particle
to particle which creates the time delay effects that need the complex
relativistic adjustments.

But the non-relativistic concepts still seem bogus since they apear to
not work when we shift frames of reference.

What I'm I faling to understand????

All the moving point charge crap seems to be extrapulated from current
flow, which is NOT moving point charges.  It's paired charges moving
in opposit directions.  The WIRE with the fixed postive charges sets
the frame of reference by which the electomagnatism equations seem to
be based.

So two parallel electon charges creating current flows each create
magnetic fields, and their wires pull together due to the electons moving
in those cross generated magnetic fields.

---

Ok, well, shit. Looking at signs of df/dt stuff, I see it's wrong.

Which I've traced down to the fact that there is a deeper problem than
just a sign problem.  The true force vector equation is a function of
r/|r|^3.  And I must take the deriviate of that and them multiply by v.
But I have no clue how to do that!  I guess I technically have to (or
could) break it down to three problems for each axis, x,y,z and do three
different partial derivitavge.  Yeah, that should work and I should be
able to figure that out.  The tricky part is that length of r cubed part.
Going to make the derivitvae messy.

Ok, I think I know how to attack it. And I think I don't care.

Because this doesn't look like a valid direction to take anyway.

But my other idea I'd like to still try -- which is to just look at the
d|e|/dt and make the streigth of E increase as some linear function
of d|e|/dt.  I have no real science to justify this approach -- but it
looks like it might be a good fit for creating the emergent properties
that do fit the science.

And if I find evidence that it creates emergent properties that seems
close, I can more carefully analize the math of this as well as the
experimental results of mumerical tests.

----

Oh, crap.  Tried to code Force4 to go back to the previous idea of
calculating d|E|/dt.  But it look like I have the same complex
problem from above of mapping dV as a vector into d|r| as the
derivitave of the change of the length of r.  My r dot V trick
isn't going to cut it.

Ok, I feel like it's time to get out of the house and go do pizza.

Oh, wait, maybe r dot V does cut it!  let me try to code that.

I'll attack this later.

---

Ok, coded using r dot v.  Might be working.

But just had another idea.  Maybe I should just adjust the force of E
by 1-v^2/$^2.  Oh, no, that fails to get the de=0 when side by side.
But, I could just calculate force and direction of E based on length
contraction in the direction of motion and that WOULD create the right
effect and be consistent with ideas I've read.

The Heaviside equation for E from

http://cds.cern.ch/record/920085/files/0601028.pdf

Just seems to adjust the strength of E but not the direction based on
relative speed. It's a little complex due to the use of sin2(theta)
as well.  But it's intersting that it has NO angle adjustment in the
equation.

But, if i'm reading it right, the strength drops to zero as the speed
nears the speed of light impying the B field gets stronger as the E
drops I think.

I'd have to plug in some numbrs to understand what the equation is
really doing.

Maybe the best approach here is just the simple one.  Get the es vector,
and calucate magnetic addition based on v dot r speed!

----

[8:33 PM]

Went to pizza.  Played some pokemon.

Left a few tests running with the new strength adjusting code.

Behavior is hard to understand.  But one test with 2p2e got into a
1e9 velociety tight orbit for one ep pair with the other electon far
away from the proton. And the simulation is so slow at that speed it's
going nowhere.

Nothing immedialy intersting has fallen out of this new type of magentic
logic. I'm not seeing orbots turning circular so much for example.
But more testing is needed for sure.  And I really need to code a magnetic
test that fakes current and see if if a particle moving in the field
acts corrcetly.

================================================================================

2016-10-04 9:47 AM Tuesday

Yesterday

1) Learned that Beta (symbol in equations) in relativity was a reference
to v/c.

2) Studyed the article on deriving E and B using relativity adjustments
and was able to understand a little more.  But I still don't really
understand it and I don't understand how the equations all work together
to explain total force without creating the same imbalance of force I
keep running into.

3) Tried to finish coding Force3 using true df/dt concept and realized it
was far more complex than I realized.  My derative was not fully corect.
Probably need to create a different partial derative for each axes
I'm thinking.  Gave up on that path for now.

4) Codeded Force4 to try and implment orgianl idea of changing strength
of ES only based on relative velcoiety.  But even that ran into some
issues. But I got a verson coded.  And have run some tests.

5) Added max velociety test to see how fast paritcles were really moving.
In one overnight test of the code, an electon got into a super fast orbit
of 13c.  Not limiting to the speed of light clealry.  Maybe that's not
bad? But it certaonly does create real compuational problems since it
slows down the simuation to a stand sill.

Today

[1:25 PM]

Got serisoly distracted posting to facebook again about the future. That'a
where there are 3 1/2 hour gap beoteen starting Yesterday secion of
notes and finsihing them just now!

So, this cversion of Froce4 isn't doing anythihg intersting in the
simulation.  But I want to look at it and test it to see what it's
really doing.  How much froce increase is it creating?  is it really
zero at 90 deg Like I think?  Maybe a simpler alogirhtm would be better?
What did I code yesterday?  I don't even remember! :)

---

Ok, found out what I coded.  Mag force was just es force times the
velocity beta factor squared v^2/c^.  So when the relative motion was
equal to the speed of light, it was as strong as as the ES -- aka the
ES force would double.  But for less speeds the ES force was changed less.

But, there was no limit to this -- so when v as greater than c the force
just keept getting larger.  The hope was that the dynamics of the system
would prevent that.  But that didn't work since I saw a 31c number or
wathever it was...

Nice and simple code.  But I'm not seeing electipcal orbits turn into
circles. Maybe my thinkihg is just wrong on that. Maybe all they do
is turn into a different (fater) SHAPE, but not progress to more and
more circular?

Other issue, this seems to be changing the very nature of the ES field
so my energy conservation is probably broken.  Which could mean by
energy adjusting code is preventing this from doing what it should
be doing.

Let me set up a fixed static test of an electron in orbit and trest
with energ adjusting on and off!

----

[2:30 PM]

Ok, test with on electon orbiting one proton.  With energy adjust
and without.  Without magnatism, and without energ adjustments,
the system was losing energy and the orbit was getting smaller
over time. It also drifted around the proton like a sprigraph.

With Magnatism, it was ganining extra energy and the orbit was getting
larger and larger.

There was no tendency of the orbit to become more circular that I
could spot with magnatism. Just accumlated simulation errors.

I could change how energy adjusting is working.  I can look at
starting and ending force of each particle, and adjust it's velocity
to match it's ending force. Instead of calucating total potential
energy and total kenetic and spreading the kentic energy over all
particles.  In fact, it feels to me that doing it particle by
paritcle is the correct appaoch anyway.

Aren't I already doing that?

Ah, yes, I think I am, but what I'm doing is assuming the path of
the particle causes it to see linear change in force from the
starting to the ending velociety.  So I calucate a new end based
on the last ending force.  But I never then close the circle
by just setting the velcoiety to fix the ke difference.  I don't
think. So I should try that.  It should allow the system to not
lose enrgy but yet not assume that total PE+KE is a constant!

----

Checked energy code.  Realized the problem.  Energy change is the line
inergal of df * dl over the path the particle takes.  You can't calculate
true energy change by looking at force difference between start and
end alone.  You have to look at distance as well, but you can't assume
straight line path.

In the case of 1/x^2 the path isn't important but when the magnetic
effect cuases the force to varry in more complex ways, the striaght line
path doesn't work correctly.  So knowing the ending force and location
is not enough!

BUT, I already had code to calculate ending velocity based on current
estimated ending position and ending force.  But I wasn't calling it
one last time. I would update velcoiety, then update psotion.

So I added one extra call after the itteration loop to update velociety
based on new force to reduce energy error.  And that works pretty good --
not perfect, but pretty good.

Wait, why isn't it perfect for non magnatism runs????  Shouldn't it be?

Now I'm confused again.  But, oh, wait, we are dealing with the sum of
many different forces from many different moving particles.  Oh, yeah
the potential energy is based on distance, but forces cancel out when
distance doesn't.  So it's all very complex I think.

Maybe.  Or maybe I'n not really calculating ending energy as accuratly
as I should be????  Potential energy is just kqq/d.  And force is kqq/d^2.
Why can't I just look at the change in force and deduce what the change
in energy is assuming conservation of energy?

Ok, lets look at this..

Assume force has chagned by df.  So assume distance chagned by dr.
If starting d was ds, then ending d can be calculated can it not?

df = kqq/de^2 - kqq/ds^2

df + kqq/ds^2 = kqq/de^2
de^2 = kqq/(df + kqq/ds^2)
de = sqrt(kqq/(df + kqq/ds^2))
de = sqrt(kqq/(df*ds^2/ds^2 + kqq/ds^2))
de = sqrt(kqq/((df*ds^2 + kqq)/ds^2))
de = sqrt(kqq*ds^2/(df*ds^2 + kqq))

Oh, gee I'm lost. what am I trying to find?

Ok, given ds, de, and df, can we calculate dE (chagne in energy)
and will the ds and df drop out making it only a function of
df? That's what I'm trying to figure out...
this is working.

A change in force just fails to define true potential energy change
becaue it's related to distance as well.

Oh, fuck, so lets drop out ds and de and just ue change in distance
dd?

Pig wnats a package to post office. Got to run now.

----

Also, the new "fix ending velociety" code has blow up multiple times
cauusing out of control flying paritcles!  That's not going to work,
though I don't know why it happaned.

----

OH, fixed max energy error! It was using the max error seen ebfore
adjusting dt and redooing it!  Fixed it to show the max error actually
USED in a move!  It's much better now!

I had always been surprised by how high that got!

----

[8:36 PM]

Mailed package for Pig. Salid at pizza.  Pokemon!  Installed myself at
the leader on the freeman house as the 10th player!  It seems the blue
have controlled this gym for weeks.  It will be intersting to if it stays
up and for how long -- might actually get to collect real coin for it!

Then played some with emulator and enrgy conservation.

SOmething odd is at work. It blows up with velociety hitting something
absurd like 83489348394 c..  So huge numer like like that.

This is with energyFix turned off. But with energyFix2 on.  Or off.
With magntic on, or off.  Wihtout energy fix, it seems to randomly
blow up.  I have no idea what sort of condition is causing it.

I've turned erngyFix back on, and so far no blow  up. But max speed
is showing 22c.  Which indicates something is sort of odd.  And inside
RLimit is 0.  So it didn't fall into the middle.

In fact, I don't know howit could reach the higher speeds and not mess
up the energy totals.  Maybe 22c is not too high to do that?

I do wonder if I've added a bug somewhere today. Did a quick look
at the diff from yesterday and nothing stood out.

I guess I should check out an older verion and test to see if it's
acting the same -- like does it blow up with energyFix turned off.

But later. Time for snacks and TV now.

================================================================================

2016-10-05 6:33 PM Wednesday

Yesterday

1) Stuff

Today

Spent most the day posting to a physics post by Eray Ozkural about quntum
mechanics and sharing my ieas form this work.  And being told by a Robert
Brown who knows phsics (taches it I think he said) that I was crazy. Just
normal stuff.

But on this page:

http://academic.mu.edu/phys/matthysd/web004/l0220.htm

That is "Magnetic Field of a Moving Charge" it comes up with the answer:

FB/Fe = v^2/c^2

.. of two protons moving in parallel at the same speed.

Which is cool just in the fact that it supports this idea that the
magnetic field force is the same as Fe except based on V instead of C.
So that the strength of the magenetic field, will just increase with
force as velcoiety approaches C.

But as is true eleswhere, the formulas from this page make no sense for
free particles because the frame of reference defines whether there is
a magnetic field or not.

But, if we assume this is correct but that the real magnetic effect
is crated in a wire by the ions not moving, vs the elctons that are
moving, then the attraction between the moving elctons in one wire
and the stationary ions on the other, is what causes the wires to pull
toether. We have to assume that the magnetic force is causing the pull
to get stronger.

================================================================================

2016-10-06 1:10 PM Thursday

Yesterday

1) Not much. Long post to facebook about physics that did nothing but
get me laughed at.  :) Typical.

Today

Thoughts.  This morning in the shower, I was thinking about the fact that
a changing magnetic field will produce a current.  And that I haven't
spent as much time thinking about that as I should.  And they way to think
about that is that the expanding magnetic field of the increasing current
is making the line of flux "move" outward through the parallel wire next
to it.  It's as if the wire is moving THOUGH a fixed magenetic field.
Which induces a current.

So a (logically) postive curent that is increasing in one wire, will cause
an increasing magnetic field moving outward.  If the postive flow is to
the right, then the lines of fux are up in the wire above the first one.
And when the lines move outwar, that's as if the postive charge carriers
are moving in towards us.  Which with the right hand rule, means it
induces a current to the left -- opposit of the direction of flow --
all as expected.

Changing current induces a reverse flow in the other wires.

What particle to particle interaction is needed to explain that???

I'm feeling as if NO interaction at the particle level can explain that?
Am I crazy?

So we can't just look at the current velociety of a particle, to explain
a different behavor for. We must look at the acceleration of the particle.

So two wires side by side. One with constant current, the other with
none.  There is no net force at work on the wire with no current in
this exmaple.  So even though the electons are moving in the one wire,
relative to both the electons and protons of the other wire, there is
no net forced induced.  But we can write this off by saying there is net
force induced on both the electgons and protons, but in opposit directions
so they cancel out and cause no current or no movement of the wire.

But, if we add acceleartion to the current flow, then it all changes!
Acceleration to the current in one wire, causes inverse accelerataion
in the other wire?  Is that what has to happen?

I've seen talk that implies a changing E field causes a chaning B field.
But in this case of accelearting current in one wire, there will be
no change to the E field in the other.  Ugh.

Well, unless in order to accelerate the current, the "pressure" has
to increase, meaning the charge density of electons vers protons
has to change?

Well, shit.  So, constant difference in parallel motion, seems to indicate
that opposit paticles must pull together (aka increase E force).  But
accelerating difference, does something else?

Somehow, the magnetic field is building "energy" as it expands. So when
the current is first started, the magnetic field is fighting against
ifself in a coil and it takes time to expand and build up current. This
implies a potential kenetic link where the potental energy making the
curent movie is being transformed into kenetic energy where it is "stored".
And if we take the potental away, the magentic field will "colapse" and
produce back EMF.  Where the fuck is that energy actually stored? This
feels to me like the atoms are tricking us and strong the energy for us
somehow.  And the ability of the changing current to create energy flow
in the other wire, is something like a difference between the the current
flow and the magnetic energy store.

So this becomes a very fundamental qustion. Where the FUCK is the energy
of magnatism really being stored?  If motion creates a greater force in
the E field, we can say motion CREATES energy???  Wouldn't that be the
case if motion created an increase in force and as a result act as an
increas in potential energy store?  So then that increase in protental
enegy could work it's way to an increase in kenetic energy somewhere in
the atoms to be the real magnetic energy store?  Real energy Store of
the system?

But how, where?  How can the atoms store this energy?

Maybe the answer lies in the alignment of orbits? (instead of the kenetic
energy of the orbits?)

Like little magenets all ligned up? The orbits all fight each other
and one alightment will store more energy than another, so when the
magenetic field "buidls up" we are twisting orbits to store the nergy?
As if each is a little spring that we add energy to?

We know the orbits have some sort of quantum level desire to lock into
different energy stable configuations.  And the transfer of these stable
energy states is done by "photons" which I don't think is considered
what is at work with magnatism.

It seems as if magnatism is understood as a magical "field" that exists
in free space. Orhangnal to E.

This implies that B is just a new force like E. But that it is created
90 deg to E, based on motion.  So just back to it being a force, that
changes with velociety, instead of with distance.

But there must be something odd at work here. If the potential energy
of the E field is a force in line with the two particle, the potental
energy of the B field must be something like 90 deg out of line with the
E field. But it must somehow translate to a different form of motion?
Or something? Or what?

If sidwise motion is what defines the E field, then something really
odd happens with orbits.  The potential energy of the E field createes
an orbit, which is nothing but sidewise motion.  We can't use that
latteral relative motion to add more potential energy, or else it will
only cause the orbit to get faster and tigher won't it?  OR WILL it?
If sidewase motion causes the E field to get stronger in the direction
of the E field, then it will push the orbit OUT.

OH, WRONG. The ep pair is pulling together!  So if an increase in lateral
motion causes more pull, it only causes the orbit to spiral down out of
control.

Oh, but what if sidewise motion causes an increase in lateral force
at 90 deg to E field? So that a circular orbit will cause the e to
try and spin faster -- throwing it out of obrit???

Somehow, the effect of the B field, has to be a separate energy store
from the normal E field.  So hum, if the e field is creating a
circular orbit, what if the B field is trying to twist that orbit
sideways?  Making the particle turn in it's orbit? 

AH AH AH AH....

If the e field is making it rorate around one path -- say around the
equator, the B field could be trying to make it roate on a polar orbit
(so to say) 90 deg out of line.  Then these might somehow act if they
are two spearat orbital kenetic energy stores?

And this could create maybe something like stable figure 8 sorts of
orbital patterns as the two orbital forces lock into even mutlipels of
each others frequency?

Or maybe just a wobble back and forth in an orbit, or a spiriling pattern?

This is REALLY soundling like it's heading somewhere!

SO if it's in an orbit, left to the right as we draw it, then the B
field is working to make it spin up or down in the obrit?  90 deg to
the direction of travel!s

Ok so in a stable orbit, the E field is not changing, it's constant.

But the B field needs to be added to make it turn.  The current rules
for EM don't seem to work in that dimention.

Ah, but wait, what if it made it dive in and out instead to create the
extra dimenstion?  Might that work?  I feel like there has to be an
effect here at work that stores kenetic energy at 90 degs to the normal
orbial kenetic energy.  if that is in and out, then the orbits get all
fucked up.  I feel that can't work. I feel it must be up and down intead
turn left and right.

So, this implies, that latteral relative velociety, needs to make the
pair TURN!  Just like the direction of the Magnetic field, but not like
the force it is SUPPOSED to be creating!  Something very different from
anything I've seen described, but yet a close cousin to the same ideas.

Ok, so if in a stable circulare orbit created by E, then B will make the
particles turn, but not change the distance away from each other, so it
will not change the kenetic energy of the orbit. It wil "spin" in orbit.
Or something cool like that hopefully.

But when it's not a circulaure orbit and not perfectly 90 deg to the
direction of travel, then what?

I guess making the B force 90 deg to V and 90 deg to R is the right
answer.  But how strong?  Should the velociety be reduced by the
angle of R?  (sin theta, vxr)?  It feels to me maybe it shouldn't be.
That would create a force that changes in what intutively feels like
odd ways as it orbits in an electicpal orbit. An eleptical orbit
will change due to velcoeity, but if you add vxr then it's max at
the two ends, but weaker on the sides.

Could try it either and and just ee what appehs.

So, attempt one.  Just add a force that is vxrhat in direction, but the
strength is proportial to v^2/c^2 relative to E.

----

Ok, wrote a first cut at the new idea.  magneticForceTotal5().  coded
vxr becuase it was eaiswer then coding v^2 * vHat x rHat.

But, ran into problem.  The direction of the force is not equal
and opposit!  So the force for an e and p in orbit. both push in
the same direction!  I can invert based on the sign of the charge
of "my" side.  That will fix the issue for e p pairs but it will
stll be broken for ee and PP pairs!

Or will it?  That's how mag fields work is it not?  an - particle
spins left, and + will spin right in the same "field"?  If the
other particle is a different sign, the field inverts.  So - +
will spin down, -- up,  +1 spins up, ++ down.

BUT, this means not equal and opposit and that seems impossibly wrong.

Well, fuck code and test anyway..

---

Ok, flip baesd on one side.  That doesn't have the effect I expected
it to.  The electron is pushed one way, the proton the other, and the
net effect is still a stable orbit, with a slight offset to the centr of
the orbit I think. As if the sun was high or low in the center slightly.

THe B field is not creating or changing the orbit.  The concept seems
spot on. The implmentation fails to implment the concept however.
And totally fails on ee and pp.

So, here's another idea.  The B field just pushes the orbit from one
orientation to another.  So on a single orbiting ep pair, it will have
no effect. But two pairs will effect each other. Like magnets and cause
the orbital rotation to align.

So, maybe.  Two ep pairs, in parallel syncronied orbits are locked in and
have no B force changing them. But if they are at 90 deg to each other,
they try to flip each other?

So this goes back to normal b formula.  The e in the orbit is creating a
magnetic field just like current in an electomagnet. The opposit polls
attract making them line up in syncronized pariall orbits sharing a
common axes.

Hum, but if b is a fuction of V, and electons move faster, they would
be creating more B effect than protons?

This concept is lost when we use the relative velociety becaue we
aren't assigning one velociet to one and not the other. But we could
take relative revelcoity and assign to both to create equal and opposit
monumum to get a velcoeity spread across the two -- making the faster
moving one the electron dominate the force?  Seems ver made up instead
of simply jusified, but it's a thought.

But, in truth, the protons will have lower relative velcoeity to each
other and the electons higher relative to each other on average. So
the system will pick up that fact even with only relative velocties
being used.

----

Some thnking with my feet up on the desk (waiting for pig to get
ready for 4 pm noodles were going to do at noddles and comapny
where t's not 5:30 and we still haven't left).

If to approaching particles causes B force in one direction, and
to retreating paricles cause B force in the opposit, might
that cause the orbit to spin? So b force is more like b dot r
instead of b cross r?  Well time to eat..

================================================================================

2016-10-07 2:46 PM Friday

Yesterday

1) Thinking about the fact that changing current induces voltage force
in wire and whether my thinking included this in the past?

2) Idea of how magnatism could cause orbits to turn without changing
kenetic energy of normal orbit.  That is, an electron in an equator orbit,
could turn left and go into a polar orbit.  Or if it KEPT turning, would
be in a new orbit in an entire new dimenstion from the normal orbot. And
this "new dimension" could be seen as a serpate store of kenetic energy
than the defult orbit!  Just thinking about how magnetic force translates
to kenetic energy that can in any way be seen as a "different" kenetic
energy story than the E field.

3) Tried coding magneticFoceTotal5() to play with the ideas above.
Turned out it didn't work at all as I was hoping.  vxy creates a force
on the electon and the proton in the SAME direction so they don't blance.
Violation of conservation of energy.  When I cheated and flipped based on
sign of charge, it still didn't create a complex orbit as I expected --
because the force stayed in the same direction the whole time pushing
the entire orbit to the side, not turning it into a spinning orbit.

4) Robort answered my questions on facebook about EM.  He was well aware
of how EM seems to violate newtonian physics.  his answer seems to be
that yes, you need relativity to understand it.  It doesn't work under
newtonian physics.  He recogmended a text book, which I found on Amazon
for cheap and ordered.   I'm just going to have to work my way though
all the math and learn it!

Today

Went to forge this morning with Phil.  Tried a quick loop forge weld
and cut end of a fire poker.

================================================================================

2016-10-08 1:48 PM Saturday

Yesterday

1) Chatting of Facebook about phsyics with Robert who is trying to nicely
point out how stupid I am and how totally wrong my theories are.

2) Found software to make a movie of my simulation running and post it
to facebook!  That's cool.  Made a movie of how one electon can make
two protons hold together.

3) Had to pull out an older verison of physics.ph to make the movie because
the new one seems broken.  Even turning magnatism off, particles are flying
around in strange ways at times.  Clearly, I've broken something.  I suspect
in my attempt to chagne the energyFix code. But maybe just a simple typo
or accidently deleted line. Need to track THAT down now.

Today

Been posting more to this same facebook thread on a Eray post that Robert
Brown from Duke has been sharing with me.  Just trying to justify my
reserach direction with them -- Robert is not buying it (he has tons of
real phsics experence and teaches it at Duke).  Eray, well, not convienced
or interested, but not as negative.

I've had some new insights that are highly helpful from these debates.

One, to really understand electomagnatism, we must understand the
relativity implications.  I was thinking that since magnatism is a
macro level obvious force, that we should be able to understand it for
low speeds, using newtonian ideas of magnatism.  But that wasn't work.
Then I had a personal moment of enlightenment.

The electrostatic force is HIGHLY powerful.  It's HUGE.  And the only
reason we don't see it much at our macro level, is because all the
electons and protons are balancing each other out.  So when we see
magnatism show up, it's NOT a large force relative to electrostaic
force -- it's just a very small amount of force that is "leaking out"
of the sytem due to space time distortions of relativity!  You can't
explain it, without the space time distritions of relativity (at least
in simple terms of magnatism).

So that helps me understand why the formulas of magnatism don't translate
so easity to particles. It doesnt' invalid the idea that there ARE
formuals at the particle level to explain it, but that what we see at
the marco level we describe as magnatism, is not the exact same thing
we see at the macro level of particles.

So, I've ordered the text book, and oepleully, it will be at a level
that I'll be able to understand and work through.

But, in the meant time, I think it is still 100% valid to keep seraching
for an alternate implemention of particle to partical magnatism.

In fact, it could well be that magantism as we know it, when created
by current flow, is really a complex manipulation to the stable state
of atoms -- twisting all the orbtial patterns in a way that create the
combined total effect we know of as "magnatism" external, even though
how the current flow interacts wtih the probits, is a more complex issue
(on total detail, maybe easy in abstract ideas one understood).

---

So, I need to hunt down the bug in the code and fix that.  But first
some abstract thinking about particle level magnatism.

So, I really really like this idea that magnatism is a force that is
working on particles in a way that creates a duality of potential enery
effects on orbits. So instead of the 1 dimention effect of classical
gravity or it's parallel here, the electic force, magnatism addes a
second dimenstion.  Likely, at 90 degs to the ES force.

And the two working together created indepdenent orbital frequencies.
Like one is creating an equator robit around the earth, but the other
force is forcing it to spin in a circle doing a constant left turn,
creating a looping pattern.  Which then will stabalize into a minimum
energey configtion of either a perfect circulure orbit for the simplest
case, or a figure 8 orbit for the next case.  (maybe never a circle
actually).  Or a triple figure 8 for the next.

If magnetic force is v^2/c^c relative to electrostatic, then it could
well explain that the two orbits fall into a 2:1 orbital frequency
at a fixed velcoiety. The velociety needed to make the frequencies in
that 2:1 ratio.  And that fixed velcoeity, defines a very hard fixed
kenetic energy level that defines the quantilization of reality for us.
So, I don't know the relationsihp between velociety and orbital period
(aka frequency).  Let me go try to find that.

A = v^2/r  Where A is the acceleration of circular motion, v the
velcoiety, and r the radius of the circle.

So f = ma = ccqq/r^2 for electostatic or vvqq/r^2 for magnetic.

Hum, wait. What.  Am I doing?

Right, so the velcoetiy will be shared by both orbits.  But one aspect
of the orbit will be half as long as the other aspect.

And one aspect is contrlled by A = v^2/r in one direction, and the other
is controlled by A = V^2/r in the other.  So v has to be the same for
both, but v and and r is different in the two dimentions.

So since they share the same v, we know that one path has to be half as
long as the other, for the orbital frequencies to match up.

If the path is 2pir so the path length is in direct proportion with r.

So r has to be half the size for one.  So A has to be doube the size of
one than the other.

And since we are talking abot one electon with a fixed mass, then the
mass is the same for both, so the force has to be double.

So to create a 2:1 ratio of orbtial frequency we just need a 2:1 relation
of the forces.  So 1/2 = v^2/c^2 seems to be the answer.

1/2 = v^2/c^2

v12 = sqrt(c^2/2)

1/3 = v^2/c^2

v13 = sart(c^2/3)

V23 = sqrt(2c^2/3)

The kenetic energy of this electron is then 1/2mv^2 so K12 = 1/2mc^2/2
= mc^2/4

The quantum energy transfer however would be the difference in energy
between these levels.  So I need to calculate the absolute energy in
these levels and look at the differnces.

And, there is the issue that the proton we orbit around will be moving
and carrying kenetic energy.  And when we change v and our orbit, it
will ause the proton to change as well. How does that all work out? Where
does that energy go or not go?

I need to look deeper into this, come up with real numbers of what the
quantization of the atom might beusing this logic, and see if maps to
other phsics constants like planks constants or something???

Ok, but now, put that on hold, and fix the bug in the code...

---

[3:30 PM]

Ha, nice Trump supportor came by.  Chatted with him for 15 minutes or so.
Didn't chnage his mind, but at least I slowed him down. :)

Pig wants to go to Outback and use a coupon.  She was talking 2pm
It's 3:30.  I'm getting hungry. :)

---

[3:53 PM]

Fixed bug in code.  Wasn't a bug after all.  I had just changed the
default monumutm setting in the Random code to be 1/100 of the speed
of light for electons, from what it was of 1/1000. This made all the
paricles move faster, and "look" crazy, just because there was more
kenetic energy in the system, not becuae the code was broken.

So, a far as I can tell, all is fine there.

---

Food? Pig, where are you?

Ok, so move on to more fun ideas of magnatism.

So, what I'm thinking of, seems to imply there will be two forms of
potential enrgy in the system.  E and B.  So the B, will, like with single
particles in a magnetic field be confgiured to always add force at 90
deg to the E field. So it's potential enegy will always be a separate
dimention in the sytem to the E energy.

And it will need to be equal and opposit on both particles. Which is
different than what the rules of magntism were trying to create.

With the standard rules, movement created a magenetic force that would
worgk against the E field, or add to it, depending on how it I intrepreted
it, but never did it work at a stricly 90 deg to it.

Ok, but another possiblity is that the magnetic field, will replace the
E field, So as magnatism grows, E shinks.  That might keep total force,
and total potential energy in the system contant as it is now?  But just
point the force in a different direction, creating complex paths.  Maybe.

The other option, is that magnetic force grows with velcoeity, creating
odd distortions of energy.  But that seems consistent with how the idea
of magnatism is told.

So lets look at what we can implement.

First, review the last attempt - I don't remember what I did. in Total5().

---

Ok, total5() did the same idea I'm saying above. Sort of. vxr so the
magnetic field was 90 deg to r and to v.

But this ran into the problem that it created the same directional force
on both particles.  So that messed up equal and opposit.

So then I cheated and flipped based on sign of charge on each side,
and that made equal and opposot for EP paris, but still equal for ee
and ep paris whic again, can't work.  And even the equal and opposot
of ep pairs failed to create complex orbits.  It just pushed the whole
eplictial orbit sidewise as far as I could tell.

So, I need something more complex or different that follows this idea, but
yet works differently.  how do we make the electron "spin" in it's orbit!

One idea from before, was to use change in velocity, or chagne in E,
to drive it. So in a circular obit, there is no magenetic field. But
in an eliptical orbit, there is a magenetic field as we approach and
speed up, but as we leave and slow down, the sign of the magnetif field
flips. So up force on one half of the orbit, down force on the other.
That sure seems like it would create a complex orbit.  Or at a minimum,
make the orbit spin or something?

But, would a circular orbit be stable and be allowed to hold any
energy? Seems like it would. Feels like a hydrogen atom would be allowed
to have any energy level if that was true.  Which doesn't work.

Somehow, the non circular orbit would have to be a lower energy one,
or a more stable one.

It's possible that the stability of a hydogen atom dpends on the
enveioment it's part of (backgound noise readiation at the fixed
frequences from more complex atoms),  But it would be a lot simpler
if the answer allowed us to explain why even a lone hydrogen with one
electron and one proton had quantied orbits.  But you know, the entire
earth is filled with black body radiation from the sun and from all the
matter of the earth, so if all atoms other than hydrogen were quntized,
maybe it would totaly explain why hydrogen acts as a quantizsed atom
even when it's not on it's own?

Ok, but why not something like a complex figure 8 sort of pattern due
to magnatism force being half that of E force.  Making the lowest and
fastest orbit possible, locked into this pattern maybe? (1/2 c?)

Ok, here's an idea, fake magnatism by hrad coding a mag force that is
1/2 of E.  And see what it does to te orbit...

----

a = v^2/r for a circulare orbit. Should be able to use this to calcuate
the perfect V to force a perfect circular orbit.  f = ma, so a = f/m.  so

a = f/m

v^2/r = f/m

v^2/r = (1e-7 c^2 qq / r^2) / m

v^2/r = 1e-7 c^2 qq / r^2 m

v^2 = 1e-7 c^2 qq / mr

v = c * sqrt(1e-7 qq/rm)

Almost worked. But was a little off.  I get it. I need to calcuate r
to be the distance to the center of mass of the pair for the circle,
but r to be the distance between the two, for the force!

v^2/rCenter = 1e-7 c^2 qq / rTotal^2 m

v^2/rCenter = 1e-7 c^2 qq / rTotal^2 m

rCenter1 = r * m2 / (m1 + m2)

rCenter2 = r * m1 / (m1 + m2)

v = c * sqrt(1e-7 qq rCenter/rTotal^2 m)

ok, that seems to get it perfect but the error quickly accumulates so
the energy is bouncing around slowly.  Thought maybe the math was wrong
but it looks correct.

----

Ok, cool.  Set an e into perfect orbit around a p. Then added a bxv force
equal to the E.  It caused it to spin in an orbit that was exactly 1/4
of the radius of the big orbit!  Or it looked that way. Conical like.
That is, from the side, it started straight out out to the right, and
orbited down and to the left at 45, until is reached stragith below.
This was looking at the obit from the side so it was a strignth line
back and forth before I turned on the sidewase force.

Reducting to 1/4 just made the orbit larger, moving to the half way
between the bottom and left if you get my point.  So the spin is working
perfectly to simply cause it to drive in a circule, but staying at the
same distance from the proton.  No figure 8 effect at all.  Just two
orbits combined, which just makes the smaller circcle act like it's
offset from the proton.

SO, to create something like a figure 8, I am very much going to have
to create sytem that swiches direction of force in pattern.

Let me test this with a different stating condition and see what happens!

----

Ok, changing starting V to somthing about 1.3 slower changes the orbit
into something strange that I can't reall "see" the pattern of.  Sort of
doing a figure 8-is like of zipping in and out the proton in a slow and
strange way. This use of the extra force is causing the simation to run
really slow for some reason I don't udnerstand. But clearly something
odd about the interaction with the auto speed adjust.

So, constant force turning to one side, will cause little circular orbits
that don't circcle the whole globe, and maybe, sort of a spirograph like
effect moving around the globe if the the speed changes but force doesn't.

No figure 8 trade off at work here.  No ossolations between the two
forces at all. They are perfectly separate and they create two totally
independent orbits as I was thinking of.

But now, I'm wondering of that much independence is right!

----

Heading to outback.  But, idea.  two atoms aroudn one nucles where
each were seeking these little orbits would be intesting to see the
internaciton.

I could fake an nucleues by giving a single proton a double or triple
charge to experment with orbits of multiple electons!

----

[7:21 PM]

Back from outback.

Just had somethoughts.

Equal and opposit forces are required to maintin conservation of moneumtum
in the system.  So that's just required.

If the B force is perpendicular to motion, then it has no effect on
energy.  It only chanages direction.

So if the B force is going to to have energy effects, it's going to HAVE
TO not be perpenduclar to E force all the time!  Or, it has to change
E as well as be perpendicular to it!  Oh, wait.  Wrong.  It's not an
issue of whether it's perpendicular to E, the qustion of whether it's
perpendicular to V.

Force perpendicular to V are the ones that have no energy effects.
But if we make B perpendicualr to E but not always perpendicular to V,
then it will have energy effects!  So that might be a path to explore.

TV now..

================================================================================

2016-10-09 2:44 PM Sunday

Yesterday

1) Tracked down what I thought was a bug in code which wasn't a but.
Just turned out ot be me setting the starting monemtum of the system too
high in Random mode and causing stuff to fly around with too much energy.

2) Serach for ideas of how to implement magnatism based on trying to
create complex but stable orbits -- like figure 8 -- by having electic
field trade of against magnetic field.

3) Worked out formula and code to calculate orbital velcoeity prefectly
as part of my experments.

4) Hard coded constant Left turn magnatism to undertand what sort of
orbit it would create -- nice little circle followign constant distance
from proton.

Today

Went to forge again this morning.  Phil, Jeff, and Sal were there.
Did second loop weld cut version of fire poker.

Ideas to explore.

First, on calculating the orbital velociety, I tried ignoring the motion
of the protron and only calucated the orbit of the electon as if the
proton ws standing still.  Then decided I had to adjust to get it right
by calucating the orbits of both around the center of mass instead.
With the two moving in opposit directions.  BUT.  Maybe it produces
the same result both ways?  What's the difference between the two?
does it produce the same answer?

OK -- the answer is that it's NOT the same.  To get the velociety correct
you must actually use the distance to the center of mass and the distance
to the other particle to calcuate the ES force. And do the same for both.
It's very close -- the dfiference shows up in the 4th place, but it's
not the same.

I suspect, that maybe, this is true because the mass of each are not the
same.  If the two were the same mass, you might be able to pretend one
is standing still and the other is moving.  But then again, maybe not.
Because there's an r^2 term in there and r^2 will never be the same
as 2*(r/2)^2

But, if the mass was way different as in the earth, the difference between
the center of mass and the center of earth would be so insignificant
that the error would be hidd3en down in many significant digits.

----

Next.  Ok,

So, back to theory.  How to make E and B trade off against each other
so that it creates a complex orbit.

I sense the need for a sin vs cos effect created by differential effects
for E and B.  So, keeping r the same, worked with a B force that didn't
change the kenetic enrgy, but just made the orbit circle in fun ways
around the surface of the spher.  If we changed B over time we could
make it take a complex figure 8 or anything we wanted the same way.
Any complex path driving over the surface of the spher is possible if
we just kept B at 90 deg to R and V.  But it would have no energy effects.

We need energy to trade off.  Or distance and direction to trade off
or something.  And to hold the damn nucleus together, we need a figure
8 pattern where the electons can pass through the center!  So drawing a
figure 8 on the surface of the sphere will not hold the nucelus together.

Ok, so, yeah, when apporaching the center, we are curving left. When
running away from the center, it has to curve right! So we need to base
the direction of B on whether r is getting smaller or larger.  It needs
to be based on dr!  Or dE.  NO NO NO. That's. wrong.  When it leaves the
center, it can be curing left, but then after reaching the max outward
point, and it starts to head back, it would chagne to curving right.
That won't produce a figure 8!

Maybe a 3D figure 8 with the two ends at 90 deg to each other is a path
that can be made to work instead of a 2D figure 8?  Or something complex
along those lines?  It doesnt' feel like a 2D figure 8 is possible and
doesn't explain the trade off between E and B being 90 deg out of phase
in any way.  I'm thiking direcgtion of travel (v) is one axes, and the
other two axes are E and B effects for turning left right, and serving
up and down (ish).

With E alone, r and Velociety are trading off as it goes though an
elpitical orbit.  For a circuuare orbit, V and r are constant.

Well, if we make dE define B, were B is 90 deg to r, then we have
forces trading off in a differential oscillator.  But B needs to be
in the same plane with V, or else all it's doing it changing the path,
but not the energy.

Changing the path (but not enregy), could create complex effects where
two atoms are interacting with each other, but if all we have is one
hydrogen atom with proton and electon, there would be no real interaction
between the two.  But wait, maybe it's not that simple.  Changing the
path of an eleptical orbit, would change how fast it was diving into or
away from the atom, and that would have real effects on the E part of
the ossilation.  So, ok, maybe having B perpendicular to both V and R
might be important. I'm going to have to try this.

And then try having B perpendicular to R, but in the plane of r&v.

If B is rxv, then sign doesn't seem important. It would make the whole
system left or right handed, but produce the same overall results either
way I think. But if it's in the plane with the V, then sign has very
different effects.  Like in one, B slows down the particle making it dive
into the center I think.  And in the other, B makes V speed up, causing it
to fly outward.  So that's three basic ideas that will produce different
effects to try by driving B with dE/dt but keeping B 90 deg to E.

And I don't know how to set the strength of B relative to dE/dt.  But I
think that will work itself out in time.


----

[5:37 PM]

Got grass cut.  Need shower.  Cats begging for food.  Not their 6pm
dinner time yet!

Problem with the above Df/dt approach.  Which was the same the last
time I experimented with this. I have no clue how to map measures of dt
or DF into F for B.  I have to look more carefully at the formuals and
see if somethning insightful pops out.  Probably related to c as being
a constant.  Will do that later.  Need to shower and feed cats and think
about food first.

================================================================================

2016-10-10 8:15 AM Monday

Yesterday

1) Figured out: When calculating orbital velocity, you can't pretend
one is standing still in an e p pair, and the other moves.  Does't get
the right number.  You must use the distance to the center of the mass
for the radius of rotation, and distance to the other prticle for the
force. Then apply that to the a = v^2/r for circular acceleration.

2) Coded an idea based on Df/dt again.  Didn't know how to map Df/dt
to F.  Or Dr/dt to force.

Today

Had a few cool thoughts in bed.

First, I dod know how to map dr/dt to Force!

r dot v or r cross v produce numbers of dr/dt in different axies.
But this is not hard to map to force.  This is just v still.  It's not
a complex dr/dt.  V IS dr/dt.  And we DO knonw how to map V to f.  AKA,
1e-7ccqq/r^2 or 1e-7vvqq/r^2!  So we do know what to use!

--

Second, and more important.  If the B vector is in the rxv direction we
can't be equal and oppposit for ee and pp pairs!  There's no way with
two pairs to define which is up and which is down?  And we can't allow
them both to be up or both down becuase it violates conservation of
energy and monemtum!  But, if we do it 90 degrees to that so it's in the
same plane with r and v, then we can make it equal and opposit for all
mixes of e and p pairs!  So this forces us to keep in in the same plane!
Which means sadly, we lose the possiblity of complex 3D orbits.  But it
opens the door for more complex 2D orbits other than simple circles
and eliplses!  Could one version maybe create figure 8?

So I was thinking of multiple options to check out yesterday of how
to make B be always perpendicular to R, and decided making it in the
direction of rxv was one, or in the plane where others.  But we can no
just mostly throw out rxv direction, and just look at the options of
making it in the plane.

Wasn't I playing with this appraoch in one of my past versions and
just decided all I did was chagne the shape of the orbit to fatter.
I'll have to check what I was doing before.

--

Next new thoughts -- idea that can from the calucations of orbital
velociety.  Instead of using relative V we can pick the frame of reference
where V is distributed across the two particles, such that we are in
the frame where momentum of the two particles is zero! So we make both
particles move.  Maybe that would have helped in the application of
mangatism rules?  Giving us a velocity for both particles to define the
magnetic force on the other side?

Something to think about. Adjusting V to a zero momentum frame of
reference before applying a formula is another way to make the formula
frame of reference indepdendent.

--

Going to Deep Water Horizon movie this morning with Pig.  Maybe 9:20
showtime. So don't have time to do much this motning.  Need to get
coffee and banana and bitemens next.

----

Looking at pictures to think through vectors and signs.

v dot r, seems like it should be the key magnatude.  So this is all
90 deg to what magnatism was telling us if we do it this way!`
And it means when passing side by side, no magnetic force at all.
As in circular orbits are stable -- no magnatism.  But when
heading straight towards each other, it's maximum.

--

Oh, but crap.  We can't do that.  Because there's no way to define
direction of the sidewase force in that case! Oh, but wait, maybe
it's not that bad.  Beause this system would always be pushing
the paths away from that!   Oh, and gee, it's no different than
the problem of two particles on top of each other where we don't
know which way to push them away! It's the same problem in fact!
So, ok, not an oh crap.  Just a singularity we can't compute that
for the most part won't happen and if it does it will ony happen
for a single instant in time.  I think.  Ok not s show stopper.

--

So v dot R looks ok as the magnatude.  Which means the more they
are headed towards each other, the more the system trys to push them
apart!

Assume B sign flips when E flips.

----

[2:23 PM]

Back from movie. Wasted a few hours posting about Basic Income and the like.

Now time to get back into this coding of magnatism.

What did I implement in the past again?  Need to double check.

Ok, Total5 (A) -- add B 90 deg to velocietyt so it doesn't effect the velcoety
created by E.  It only makes it turn.

Total5 (B) -- make B 90 deg to E, but up or down, as in rxv.

Total4 -- use De/dt but make the strength of E force grow or shrink.  Not 90 deg to R.

Total3 -- first try at using DF/dt where I learned I was really thinking about d|f|/dt
but thought it was dF/dt. Unsure what direction I was trying to point B.

Total2 -- oh, gee, it's so complex and hacked I can't tell.  But one version
was ((v x rhat) x rhat) that would have made B perpendicular to r. But
the magnatude was the cross, so maximum strenth at perpendicular. I'm now
talking the same, but 90 degres different, with minimum B at V perpendicular to r.
Total2 also was attempting to reduce Fe as Fb grew larger.

Total1 -- V x (V x rHat)  Normal mag rules. Sort of. 90 deg to V with maximum
force when V perpendicular to R -- when the two pass in the night.

Ok, so it looks like Total2 is close to what I'm going to try now. But
yet vxr instead of v dot r for magnatude.  SO this does seem like yet
another variation on the same ideas. So it is something new.

So, this is force related to change in E.  Maximal B force, when the two
particles are headed straight together, or straight away from each other.
And B perpendicular to R, not to V. Sign, unclear. Will try all options.
But B in the same plane as r and V.

This is total5 (C) (another if clause in total5)

---

[4:44 PM]

Got a version of Total5(C) cvoded and running. Mangnatude of F seems
resonable.  and F dot R is zero ish.  Looks right. But I don't know what
the sign is like in different situations. That needs tobe checked and
thought about.

The orbits it is creating is somewhat normal, but also oddsly crazy
looking at the same time.  So I don't really know whta I'm looking at.

I have to carefully check signs and calucations to see if what it's
doing and if it's doing what seems to be resonable in all different cases.

And if all cases seem to be even have a resonable answer!

---

Tried inverting force.  The inverted version got locked into 32c speeds
at times so that doesn't look so good.  Back to running a random test
with the non-inverted verson. Eeven though I have no clue what direction
the current sytem is pointing!

================================================================================

2016-10-11 2:22 PM Tuesday

Yesterday

1) Coded Toral5(C) Idea.  B perpenducular to r, but in the same plane
with r and v.  Magnatude determined bx V projected onto r. (v dot r).
Have no carefully tested signs to see if they are logically constant.

Today

Need to look closerly at Total5(C) version to see how the vectors point
under all conditions of which way V points and combinations of + and -
particles.  See if it seems logically valid.

So far, running informal tests are not showing anything particular
different or existing with this appraoch.

I'm beginning to feel like I'm running out of ideas of how to implment
B without breaking basic laws of conservation.  And none of them are
so far showing signs of intersting complex orbits that could explain
how 1) the nucleus is holding together, and 2) how it's creating quantized
orbits.

But, I've not run out yet!

So, on this lastest idea of keeping B at 90 deg to E, I still need to
verifty directions and signs.  And another idea is have E reduce
as B grows.  I've not tested that either.  

Having E reduce as B grows would put a cap on orbits for sure.  I
guess.  Orbital velociety can't get too large because the E holding it
in orbit will vanish forcing it to fl out to a higher orbit.  Might
it find a stable balance point? 

Well, lets look closer at direction of vectors first.

--- 

Working through vector checking.  Got bHat wrong.  It's magnatude was
not 1.0.

--

But an idea.  If B and E interacted in a way that would cause an electipical
orbit to rotate like a spirograph at different ratios, of B and E,
cuased by different valuds of V and Kenetic Energy, then this roating
might stablize when interacting with other atoms, and that stable point
could be the cause of the spectrum quantization.

--

ugh. This simulation is doing weird stuff I don't understand.  Electons
look like they are sort of in a normal orbit, then they fly off for no
reason at all.  After getting too close to the other particle I think.
Maybe. I'm not sure.  Looks like mayne these rules are constantly adding
energy to the system? Or something.  The forces are equal and opposit. Can
it add energy in that case?

---

Looking at the behavior of this new algorithm I'm just  lost. Don't
understand what it's doing. Or not doing.

--

A thought.  We could think of B as a force which tries to balance energy
in the sytem by taking it away from the faster moving particles and
giving it to the slower moving particles.  So the effect of B is to
spread energy thoughout the sytem evenly.  Maybe?

That could maybe create some balance points?  Maybe?

================================================================================

2016-10-12 12:45 PM Wednesday

Yesterday

1) More careful checking of total5(C) version for signs and values.
Found one bug where rHatxvHat did not produce a vector of length 1 as
I falsely thought.  So I fixed that.  Make B stronger. But didn't have
much real affect on anything.

2) Spent more time thinking...

Today

Feeling a bit lost.  This last attempt didn't produce any instant insight.
It's acting in strange ways I don't understand.  And I'm left with no
rational understanding of why one direction of the B arrorw is better
or worse than another for this approach.

I still like the abstract idea of the B and E energy effects trading
off against each other.   But just not sure how to make it work.

If E expresses itself as elptical orbit of close and far from r, and
low and high V, how might B express itself as well?  AkA, E is trade off
between r and V when we take the concept of the field out of the picture.

What can B be a trade off between when we look only at the two articles
in issolaiton?  It would be very nice if a simple two particle system
of + and - would have anorbital trade off that allows even the simplest
form of hydrogen (not nutron) to have a quantum enrgy states.

So, if we assume equal and opposit forces to always maintain constant
energy and constant monentum, then we must find a more complex orbital
pattern other than a circle or elipse as the foundation of the system.

I was thikning figure 8, but I don't really understand how to make
that happen.  I was thinking polar vs equator orbit, but that doesn't
work for only one pair.  There is no orrientation of orbit for one pair.

With this last version of B, being in the plane with r and V, all we
can do is change the shape of the orbit in the plane.  That really feels
lacking when we have another whole dimention to work with that we aren't
using.  Which is why I was attracted to the idea of B being opposit of
that plane.  But then ran into the problem of having no way to define
the sign of that B when we hae two -- or two ++.  I onky works with +-.
But here's a thought.  Maybe B only works between protons and electons,
and has no effect electon to electon, or proton to proton!  Maybe the B
force needs to be understood as a different force, like gravity, is a
different force rather than just a modification to E?

Protons and Electons are fundamentally different things and there is no
requirement they always interact based only on their charge and mass.
Maybe the B effect only happens between P and E?  Doesn't feel all that
good conceptually, but doesn't feel impossible either.  Maybe there would
be a way to define B such that between an ++ or -- it just canceles out
and has no effect? That would be cleaner solution if that could happen.

Ok, but lets put that idea on hold. and go back to looking more carefully
wat what we are doing in 5(C).

---

So, if B is only acting in the r v plane, what can it do to the orbit?

Could it create a sin wave like ossolation to the orbit? That in it's
furtherest extent, turned into a figure 8 maybe even?

So if we want the orbit to have a sin wave wobble to it, that means as
the the E increases, it must shift the force from inward to outward.

Using this logic, it feels like we need to do what magntism was
telling us to do!  That is, make an increase in velcoeity, reduce
the strength of the E field.  So make dE reduce E.  Which means
make V reduce E.  And when V is c, it's reduced to zero.

Didn't I try coding this?  I keep going in circles with the ideas
so much I can't remember.  Maybe I didn't code that exactly.
So, shit, I guess I should try coding it.  And what about sign?
If E is decreasing, then the pull of ep should increase I think
to make it turn into a sin wave orbit.


@


1.36
log
@checkpoint by ciall
@
text
@d10603 2
a10604 2
Looking at the behavior of this new algorithm I'm just  lost. Don't understand
what it's doing. Or not doing.
d10613 1
a10613 1
That could maybe create some blance points?  Maybe?
d10615 1
d10617 79
@


1.35
log
@checkpoint by ciall
@
text
@d10540 78
@


1.34
log
@checkpoint by ciall
@
text
@d9985 3
a9987 3
And one aspect is contrlled by A = v^2/r in one direction, and the
other is controlled by A = V^2/r in the other.  So v has to be
the same for both, but v and and r is different in the two dimentions.
d9989 2
a9990 2
So since they share the same v, we know that one path has to be half
as long as the other, for the orbital frequencies to match up.
d9994 2
a9995 2
So r has to be half the size for one.  So A has to be doube the size
of one than the other.
d10035 2
a10036 2
Ha, nice Trump supportor came by.  Chatted with him for 15 minutes
or so.  Didn't chnage his mind, but at least I slowed him down. :)
d10047 3
a10049 3
of light for electons, from what it was of 1/1000. This made all
the paricles move faster, and "look" crazy, just because there was
more kenetic energy in the system, not becuae the code was broken.
d10065 2
a10066 2
And it will need to be equal and opposit on both particles. Which
is different than what the rules of magntism were trying to create.
d10122 4
a10125 4
earth is filled with black body radiation from the sun and from
all the matter of the earth, so if all atoms other than hydrogen were
quntized, maybe it would totaly explain why hydrogen acts as
a quantizsed atom even when it's not on it's own?
d10128 2
a10129 2
to magnatism force being half that of E force.  Making the lowest
and fastest orbit possible, locked into this pattern maybe? (1/2 c?)
d10131 2
a10132 2
Ok, here's an idea, fake magnatism by hrad coding a mag force that
is 1/2 of E.  And see what it does to te orbit... 
d10136 2
a10137 3
a = v^2/r for a circulare orbit. Should be able to use this to
calcuate the perfect V to force a perfect circular orbit.
f = ma, so a = f/m.  so
d10151 3
a10153 3
Almost worked. But was a little off.  I get it. I need to calcuate
r to be the distance to the center of mass of the pair for the
circle, but r to be the distance between the two, for the force!
d10156 1
d10160 1
d10165 3
a10167 3
ok, that seems to get it perfect but the error quickly accumulates
so the energy is bouncing around slowly.  Thought maybe the math
was wrong but it looks correct.
d10189 1
a10189 2
Let me test this with a different stating condition and see what
happens!
d10204 3
a10206 3
No figure 8 trade off at work here.  No ossolations between the two forces
at all. They are perfectly separate and they create two totally independent
orbits as I was thinking of.
d10212 3
a10214 2
Heading to outback.  But, idea.  two atoms aroudn one nucles where each
were seeking these little orbits would be intesting to see the internaciton.
d10230 2
a10231 2
If the B force is perpendicular to motion, then it has no effect on energy.
It only chanages direction.
d10233 9
a10241 8
So if the B force is going to to have energy effects, it's going to HAVE TO
not be perpenduclar to E force all the time!  Or, it has to change E as well
as be perpendicular to it!  Oh, wait.  Wrong.  It's not an issue of whether
it's perpendicular to E, the qustion of whether it's perpendicular to V.

Force perpendicular to V are the ones that have no energy effects.  But
if we make B perpendicualr to E but not always perpendicular to V, then
it will have energy effects!  So that might be a path to explore.
d10251 7
a10257 7
1) Tracked down what I thought was a bug in code which wasn't a but.  Just
turned out ot be me setting the starting monemtum of the system too high
in Random mode and causing stuff to fly around with too much energy.

2) Serach for ideas of how to implement magnatism based on trying to create
complex but stable orbits -- like figure 8 -- by having electic field trade
of against magnetic field.
d10262 3
a10264 3
4) Hard coded constant Left turn magnatism to undertand what sort of orbit
it would create -- nice little circle followign constant distance from
proton.
d10268 2
a10269 2
Went to forge again this morning.  Phil, Jeff, and Sal were there.  Did
second loop weld cut version of fire poker.
d10271 1
a10271 1
Ideas to exploare.
d10273 7
a10279 6
First, on calculating the orbital velociety, I tried ignoreing the motion
of the protron and only calucated the orbit of the electon as if the proton
ws standing still.  Then decided I had to adjust to get it right by calucating
the orbits of both around the center of mass instead.  With the two moving
in opposit directions.  BUT.  Maybe it produces the same result both ways?
What's the difference between the two?  does it produce the same answer?
d10282 4
a10285 4
you must actually use the distance to the center of mass and the distance to
the other particle to calcuate the ES force. And do the same for both.
It's very close -- the dfiference shows up in the 4th place, but it's not
the same.
d10290 2
a10291 2
Because there's an r^2 term in there and r^2 will never be the same as
2*(r/2)^2
d10294 2
a10295 2
the center of mass and the center of earth would be so insignificant that
the error would be hidd3en down in many significant digits.
d10299 1
a10299 1
Next.  Ok, 
d10301 2
a10302 2
So, back to theory.  How to make E and B trade off against each other so
that it creates a complex orbit.
d10307 9
a10315 9
around the surface of the spher.  If we changed B over time we could make
it take a complex figure 8 or anything we wanted the same way.  Any complex
path driving over the surface of the spher is possible if we just kept
B at 90 deg to R and V.  But it would have no energy effects.

We need energy to trade off.  Or distance and direction to trade off or
something.  And to hold the damn nucleus together, we need a figure 8 pattern
where the electons can pass through the center!  So drawing a figure 8
on the surface of the sphere will not hold the nucelus together.
d10337 12
a10348 12
forces trading off in a differential oscillator.  But B needs
to be in the same plane with V, or else all it's doing it changing
the path, but not the energy.

Changing the path (but not enregy), could create coplex effexts where
two atoms are interacting with each other, but if all we have is
one hydrogen atom with proton and electon, there would be no real
interaction between the two.  But wait, maybe it's not that simple.
Changing the path of an eleptical orbit, would change how fast it was
diving into or away from the atom, and that would have real effects
on the E part of the ossilation.  So, ok, maybe having B perpendicular
to both V and R might be important. I'm going to have to try this.
d10360 2
a10361 2
And I don't know how to set the strength of B relative to dE/dt.  But
I think that will work itself out in time.
d10377 163
@


1.33
log
@checkpoint by ciall
@
text
@d9887 1
a9887 1
2016-10-08 1:48 PM Friday
d10229 2
a10230 2
If the B force is perpendicular to motion, then it has on effect on energy.
It only chanage direction.
d10234 1
a10234 1
as be perpendicular to it!  Oh, wait.  WRong.  It's not an issue of whether
d10243 131
@


1.32
log
@checkpoint by ciall
@
text
@d10043 199
@


1.31
log
@checkpoint by ciall
@
text
@d9884 160
@


1.30
log
@checkpoint by ciall
@
text
@d9846 38
@


1.29
log
@checkpoint by ciall
@
text
@d9572 8
a9579 8
Which is cool just in the fact that it supports this idea that the magnetic
field force is the same as Fe except based on V instead of C.  So that
the strength of the magenetic field, will just increase with force
as velcoiety approaches C.

But as is true eleswhere, the formulas from this page make no sense
for free particles because the frame of reference defines whether there
is a magnetic field or not.
d9582 259
a9840 5
is crated in a wire by the ions not moving, vs the elctons that
are moving, then the attraction between the moving elctons in one
wire and the stationary ions on the other, is what causes the wires
to pull toether. We have to assume that the magnetic force is
causing the pull to get stronger.
d9842 4
@


1.28
log
@checkpoint by ciall
@
text
@d9546 42
@


1.27
log
@checkpoint by ciall
@
text
@d9337 1
d9339 207
@


1.26
log
@checkpoint by ciall
@
text
@d8954 1
a8954 1
2016-10-02 9:51 AM Saturday
d9111 1
a9111 1
and DOWN.  relative to p0, the next force vector will be longer (beause
d9140 198
@


1.25
log
@checkpoint by ciall
@
text
@d8884 4
d8947 2
a8948 2
I think it's time to a flu shot.  Now.  Will put off that other
work a bit.
d8951 189
@


1.24
log
@checkpoint by ciall
@
text
@d8514 433
@


1.23
log
@checkpoint by ciall
@
text
@d7950 1
a7950 1
Ok, I cna't tell by thinking, and I don't want to try and work though
d7957 557
a8513 4
Modified mag test code to work with world[] of N particles. Set up
for three now.  Summing total mag force.  Not summing to zero!
But, what does it take to sum to zero!!! That's what I need to
figure out!
@


1.22
log
@checkpoint by ciall
@
text
@d7769 1
a7769 2
potential energy vs kentic energy system if it pointed a different
way?
d7777 2
a7778 2
morning.  Maybe I should just try starving myself to get the weight
down?  So hard to not eat...
d7783 178
@


1.21
log
@checkpoint by ciall
@
text
@d4344 1
a4344 1
Reading about space time and lorentx contraction.  These things are
d5808 1
a5808 1
2016-09-24 8:25 AM Sunday
d6279 1
a6279 1
2016-09-24 11:06 AM Monday
d6755 1
a6755 1
2016-09-25 10:03 AM Tuesday
d7216 1
a7216 1
wires together using my inverted field.  The implications of the lorenz
d7289 3
a7291 3
Certainly, there is a deep knoweledge of atomic orbits.  Why can't
they be calucated as actual classical mecahancs orbits created by
particle interaction!
d7294 489
@


1.20
log
@checkpoint by ciall
@
text
@d6753 542
@


1.19
log
@checkpoint by ciall
@
text
@d6277 1
d6279 1
d6281 471
@


1.18
log
@checkpoint by ciall
@
text
@d5805 475
@


1.17
log
@checkpoint by ciall
@
text
@d5364 1
a5364 1
c = 1/sqrt(u0 E0)
d5469 336
@


1.16
log
@checkpoint by ciall
@
text
@d5138 1
a5138 1
Ok, toher point, magenetic constant is DEFINED as 4PIe-7 so above, B
d5285 184
@


1.15
log
@checkpoint by ciall
@
text
@d5071 214
@


1.14
log
@check in from yesteday edits lost in swap file...
@
text
@d4688 1
a4688 1
2016-09-20 10:47 AM Truesday
d4723 348
@


1.13
log
@checkpoint by ciall
@
text
@d4686 37
@


1.12
log
@checkpoint by ciall
@
text
@d4272 413
a4684 4
Been studying electomagnatism to try and gain a deeper and more fundamental
understanding of it all.  I thought I actually sort of understood this from
work with electoncis, but I see at this fundamental level, I have lot to
still learn.
@


1.11
log
@checkpoint
@
text
@d4248 28
@


1.10
log
@checkpoint
@
text
@d2992 1257
@


1.9
log
@checkpoint
@
text
@d2783 2
a2784 2
So without checking the math and thinking carefully, these particles are
scale invariant.  Relative to each other, they can expand or shirnk
d2806 2
a2807 2
starting energy.  Won't make it exactly the same, but puts it in
the ball park I'm thinking.  And easy to code maybe.
d2830 2
a2831 2
fun being around nerds talking nerdy stuff.  5 of us total.  The other 4
for regulars of the meetings as far as I know.  They only get together
d2848 2
a2849 2
oddly time and force and distance are relative, mass is relative, but there is no
absolute at work in these equatons.
d2857 135
a2991 4
If we change relative mass the behavior most certainly changes -- like making
both particles the same mass instead of the 2000:1 ratio.  But what if
we just change the mass by multiplying each by 10000?  Does the behavior stay
the same but the speed just changes?
@


1.8
log
@checkpoint
@
text
@d2241 7
a2247 8
1) Got RLimit code working to cap force in a small circle around
the particles to cap the inifity problem.  Had to adjust force
and PE equations to match each other.  Sort of a mess how I coded
it still.  But as long as DT can reduce enough, this allows the
system to track the force curve without making errors that the
energy fix can't fix.  Which means fall-though falls through every
time now and doesn't bounce back.  But I have real issues with
the accuracy of the simulation unresolved.
d2253 7
a2259 7
By using a very small RLimit (0.001 Ang)  the sytem works better
with fewer questionable simuaiton behaviors.  Basically, make it
so small that it's almost never used.

Still, however, I think when it is used, we got a problem. I guess
I could track the number of times the particles get that close
together, to track how questionable a simlation is.
d2263 583
a2845 9
from the edge (20 pixels maybe -- one A) and electons bounce at 5
I think. This keeps them away from the edge and allows the elctons
to nothave their orbits reduced to near zero just beause the proton
it's orbiting around does a bounce. Sort of bhelps with simlation
speed By keeping the elctons futher away from the proton.

Added code to start a random field of electons and protons.
Fun watching all electrons run to the edges and get pinneed
but keep all evenly spaced out!
d2847 3
a2849 2
Added code to put center of mass in the middle of the screen
at start.
d2851 3
a2853 2
Changed screen coordinate mapping to put zero in the upper left
now that we have auto center code.  Makes code simpler.
d2855 1
d2857 4
@


1.7
log
@checkpoint
@
text
@d2056 1
a2056 1
2016-08-26 8:46 AM Saturday
d2216 1
a2216 1
got the LRadius working I think.  Fall though works just fine
d2220 1
a2220 1
Even for a very small LRadius of 0.01A (vs 2A that I was testing
d2229 1
a2229 1
I just need a really really small LRadious so it only hits it
d2234 47
@


1.6
log
@checkpoint
@
text
@d2125 109
@


1.5
log
@checkpoint
@
text
@d1533 591
@


1.4
log
@checkpoint
@
text
@d1402 132
@


1.3
log
@checkpoint
@
text
@d602 1
a602 1
Leaving to take Julie back to school in Greensborough tomorrow.  Pig and
d636 1
a636 1
that I got from somewhere.:f
d638 764
@


1.2
log
@checkpoint
@
text
@d1 1
a1 1
notes -- Physics -- running lab notes from work on Physics simulation
d6 215
d334 12
a345 12
Some facebook debates got me to dig into physics and QM deeper than I have
in the past that got me a better understanding of the actualy tests that
have been done for things like the double slit.  This got me back to my
old ideas that the physics has failed to understand the inner workings
of the atom, and that all the complexity they document in the QM and
other formulas really are emergent propeties of a far simpler system
they have missed.  That wave particle duality is an illuion created by
the lack of measurement tools -- we have reached the bottom of the ladder
in terms of measurement -- and a they try to understand the measurements
they are getting they are modeling the measument system interaction with
atoms instead of modling atoms.  And I think the atom is far simpler than
what they assume.
d372 7
a378 6
And one option, is that the only real particles are electons and protrons
and the only real field is the EM field.  Nutrons might must be a thighly
coupled proton and electon in a tight high speed orbit.  Photons are just
a measurement illusion.  Nutrons might tend to all self syncronize so that
they all end up in the same frquency orbit creating a resonating vibration
thoughout the entire univese of all the nutrons being syncronized.
d380 1
a380 1
And, I have to wonder if synchronized nutrons might attract each other.
d476 5
a480 5
incorrectly.  Forgot to add no-water.  Been playing on facebook like normal.
Wrote suggestion to Starbucks about being able to pick from past order
menu when doing add item.  Hope they add this some day.
Julie is almost ready to go to Best Buy to hunt for TV.  I'll try to add
momentum check...
d524 3
a526 2
power.  Fun to watch the electons fly around now and try to get back in
orbit.  But every time it gets too close it will sling shot out to the wall.
d607 1
a607 1
Need to go get haircut.  I guess pokemon when I do that?
d610 27
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
notes -- renamed from dnet.notes 2016-0204 4:17 PM Thursday Curt
d3 3
a5 20929
Lab notes and brainstorming and history of work on AI. I've been doing
a lot of the same running dialog using lab notbooks for years, but for
this code I've started using this on line log whic his really far better
execpt I can't draw pictures. I Just keep adding to the end and timestamping
everything.

dnet.notes  created 2014-01-03 from dnet.py comments
	Felt it would be better to put my rambling thoughts
	and ideas here in a notes file instead of in the source code.

    dnet.py - Dynamic Network - AGI ideas
    2012-10-14 Started this file  Beem meaning to do this for a few weeks
               wrote lots of nodts, and got some basic code creating a
               network with no weights yet - that's a start.
    2012-10-15 Got basic pulse sorting working last night.
               states are updated, but not weights
               Wrote code to update weights. Keeping 3 weights
               normalized to sum of .3 for now.
               Watching sigle input fan-out over time. Very slow
               going becuase pulse density is very slow on edges
               after a few layers. Takes long time to spread pulses
               out across row to edges. A better mix-up toplogy would
               basically fix this instead of the simple flat one I'm using
               rigt now. Will let run all night to see what it looks
               like in morning. Yeah, thinking more, the overlaping fan
               outs is a real slow down. Need to forking binary tree I
               used in he past to make it fan out exponentially instead
               of linearly. Tomorrow - lots to test.
    2012-10-16 In morning, see it did not fan out much even running all night.
                print out weights of column 3 and see middle is .1,.1,.1
                but rest are near 0 0 3 or 3 0 0. But yet, even with these
                "max" weights, the next layer had weak low value edge
                states.
                Tested edge effecs by making smaller net. Behavior at
                edge looks fine.
                Playing with alternating set of fast and slow pulses
                makes it seem clear this can't do shit with that
                type of signal! But, maybe, if there were other "reference"
                signals in another path that would make all the difference.
    2013-04-13
		Moved source from Windows box to new ubuntu Skinner box.

		Haven't looked at this for months. Forgot just wnat logic I was using.
		But thinking about these same ideas, I created another way to do the
		adjusting without remembering what I had done in the past.

		First new understanding - this is not a probablity distribution. All
		the node values are independent features where the node value represents
		the strength of the feature - which can be close in concept to a probablity
		of the feature existing. In fact, it might be possible to represent it is
		a probablity - but each feature has it's own, they are not a
		distribuiton.

		2. Much better way to do weight updates. In this old code, weights are adjusted
		based on what path was selected. But the new better way, is to set a desired
		averge target value for each downstream feature value. Each time
		an update is done, if the
		node value is greater than the target, reduce the weight. If it's less than
		the target, increase the weight. Then pick a path randomly, based on the
		node values for each path. This fixes some problems. First, since paths
		are randomly selected, we might pick the lowest path, but still do the updates
		as if that path was "too strong". It adds lots of noise and causes the weights
		to fluculate a lot more than need be. Also, with the old technique, I would
		balance the weights, which capped them to a max sum of 3. By doing updates
		with no regard for actual node values, but only based on sort - which is
		a function of _relative_ node values, node values could drift up or down
                over time.

		In other words, when we adjust based on path, node values become unregulated
		and can fluxuate in odd ways. But when we adjust weights baesd on node values
		node values should be very stable. Or that is, averge node values should
		be very stable. Or, the mean in the distribution will be the target, such
		as 1.0. The mean of all node values in the network will be 1.0 all the time
		if 1.0 is the default target.

		3. I don't know if I had figured out how to do RL with the old design. But
		with target based weight updates, RL becomes obvious. Each link between
		nodes has both it's current w value, but also a target value for it's
		downstream node. RL learning is applied to the target values! A path that
		is rewarded, gets it's target increased. Huh, or wait, maybe just the opposit.
		Maybe it should be decreased to balance the value across all nodes?
		This is the tricky question of how to do these things. If a node/path is
		getting lots of rewards when it's used, we might want to reduce it's activity
		so that activity times value is equlaized across the net. This would cause
		the nodes with higher values, to end up with less traffic, which is another
		way of saying "higher resolution". The idea is that for percpetion, we want
		higher resolution for high value signals, and lower resolution for low value
		signals. We want to lump all the bad stuff together, and separate the good
		signals into separate nodes. I think.

		But for action, we want to send more of a good signal, to the right place,
		and less of the good signal, to the wrong place - which is just the opposit
		of what we do for the percpetion problem I think. A path with higher rewareds
		should get more signal, not less. Ugh. Well, that is just something we
		have to cope with later based on experiments.

		Oh, wait, I have another idea on that. The half life of the decay might come
		into play on this issue. To increaswe the resolution of good signals, we might
		decrease it in the temporal domain, by reducing the half life of the decay!
		The half life of the decay, ends up controlling the standard deviation of the
		feature signal in the temporal domain. So we could increase the target value
		which will raise the mean value of the feature, and cause it to be selected
		more often on average - routing more pulses that way, but reduce the standard
		deviation. Or the other way, will figure out later - but it gives another
		dimension to play with. So the target value ends up setting the mean, where
		as the half life decay sets the SD. Or does it? Or is it more a frequency
		response thing? So much to think about and experiment with. None the less
		I have simple factors to use for LR to play with on this approach.

		4. Other issue I believe I had with this design months ago, was the whole
		problem that a single input signal, would spread through a network, but that
		it would not "decode" the signal automatically. The temporal domain info
		would not cause the signal to switch to different parts of the network.
		My cheap answer is that I would need to inject other signals as "reference"
		signals to compare to for it to work. Which might be true. Even if the
		other signal was nothing more than a constant noise signal, that will
		give the system something to compare to and cause signals to be steered.
		But, new idea, is realizing that the half life decay could be different in
		each downstream node, and that would give the network the power to steer
		even a single signal to different places based on temporal data alone.
		Combining this with 3, maybe the net should use identical decay profiles
		at first, and with RL, adjust them. So signals are randomly spread over
		the net at first, but get steered to the right places by RL later - which
		of course is what we need. Ah, but no, it must decode first, and steer
		second - decode by unsupervised.

		TEST REPORT

		Well it seems to generally work with the new weight update technique of using
		targets for each link.

		But it brings up many questions to think about. A long half life, causes
		this small net to stay mostly active with values between say 80 and 120 ish
		ranges. So the path choice ends up being highly random (high exploration).
		Though I tested
		only with a single (and then dual) constant pulse source, It seems to be that
		more patterned pulse streams would still fill the net with mostly random
		spray of routes and not do a very good job of percpetion - that is activating
		different areas based on pulse patterns. Adding RL should force more
		specific routes to form, but the problem without RL is a question of how well
		the percpetion concept is working.

		Shorter half lives make for larger value variation, and tends to cause the pulses
		to form streams that sway back and forth in the net. That seems beter. sort
		of. But maybe with more complex data, what we need to see is streams merging
		togeher and staying merged. At least this net doesn't have that odd asymetry
		that the gap sorter did - this one seems to sway left and right equally far
		more like real water.

		State updates are still value+weight which might be too simplistic.

		State values don't represent real activity levels since they re-scale to be
		an avergae of 100. So a pulse that fires only half as much as another one
		can still have a state of 100. That might be fine. Might even be good, but
		it surprised me at first. Might have implications in RL global value assignemnt.
		or not??

		Of course, this is chicken wire like net instead of more advanced. But with
		3 inputs per node, it could "skip" signals over each other. But that won't
		be tested until RL is added.
		

MyNetwork 21 x 15  time: 4000004.0
weights in column 3 for testing rows 5 to 17
 0.010  0.010  0.010
 0.891  0.483  0.242
 0.507  0.279  0.190
 0.291  0.194  0.173
 0.202  0.169  0.203
 0.172  0.194  0.289
 0.189  0.279  0.453
 0.249  0.440  0.417
 0.416  0.434  0.251
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  0   0   0   0   0   0   0   0   0   1   0   0   9   2   2
  0   0   0   0   0   0   2   2  33  46  26  70  12  36  36
  0   0   0   0   1 109 106  33  95  38  98  54  38  31  82
  0   0   0 104  99  95 131 122  74  76 107  75  83  97 124
  0   0  72 115 132 102 102 132 130  96 101 103 127 131  99
  0  93  94 108 123 103  96 133  92 135 112 100 125 117 117
  0  93  93  85  87  90  75  79  99  96  84 102  79  84  60
  0  93  89  63  51  74  67  50  48  61  72  55  40  33  63
  0   0  64  36  25  24  34  17  43  26  13  56  34  43   9
  0   0   0   1  46  29  16  51  33  35  46  32  39  33  28
  0   0   0  68  71  83  74  90  70  85  76  52  74  89 112
  0   0  97 101  89  86  78  93  83  95 110  79 122 127 124
  0 100  99 106  81  94 117  73 109  82  74 114 107 114 118
  0 100 100  97  82  79  90  73  75  74  69  90  99  63  54
  0 100  84  81  88  93  86  76  65  42  59  80  38  59  48
  0   0  71  56  77  75  55  83  59  82  80  58  61  56  61
  0   0   0  26  71  56  49  56  69  63  75  73  44  75  54


"""
"""
    2014-01-03 Picked this up again -- cleaned out old comments
		Been thinking more. Google has been buying robotics
		Just a few weeks ago, DARPA robotics Challence has first test
		Need to get job. But deciced I need to make this AI work first!
		Still really excited about this general direciton. Need too keep
		focused on issues and make this work. Have goal to build
		robot this year (2014) that is life like with this code. Must
		get code working first of course!
		A few new ideas that came from thinking about this code, after
		I forgot how this was implemented.
		* Thought to make state capped at 0 to 1 value instead of uncapped like now.
		* Set target to .5 maybe? instead current 1.0
		  Could make pulse distribution more logical than the current approach.
		* Could make percpetion learning tweak half life, instead of weight

		Questions
		given different pulse patterns, what SHOULD the state value seek?
		Current test code, runs constant pulse into row 10. And then constant
		pulse into row 5 or 12 alternating after 1000's of steps. This
		makes the overlap cell at row 11 seek a value of 97, when 12 is off, but seek a value
		of 100 when 12 is on. (based on current settings)  12 has a pulse frequency
		of half of 10,  What should that overlap cell really be? Why?
		Right now, the overlap cell has a SW of <0 from the slow input and .007 from
		the fast, which has .007 for all. The fast input, is working to take total
		control of that overlap cell. That's wrong. If second input is half the
		pulse rate of first, second input should have 1/3 control of cell. 
		that is what would happen, if we didn't adjust weights, but instead controlled
		activity by adjusting half life.
		Then, RL could adjust weights in links to stear pulses to the right places.
		But, the pulse would not really stear fast if adjacent cells has energy from other
		sources? Because adjacent activited cells would stay activated and suck pulses.
		But, if that was bad, and we could make the link learn to extinquish the adjecent
		cells, that would quickly fix it! So weights need to work in a way to force
		cell to be extinquished faster than normal decay! The idea of a propablity update
		using a multiple might be the answer. s *= w instead of s+= w. If so, a w of 1.0
		has no effect. 1.01 increases it towads 1.0. .99 decrases it towards 0. W needs
		to represet rewards which need to have relative effects but absolute values to
		filter back through net.

		Or, we could have targets as well that the weights updated against. And RL could
		adjust the targets. So PL (percpetion learning) could update half lifes so as to
		regulate average long term value of node to .5 so all nodes ended up with same
		average activity level (area under curve was normalized). But, we might need to
		normalize SD of value so the swing about .5 was roughly the same in all nodes.

		Yet another thoughts, should the node value really be an elgiblity trace updated based
		on how nodes were actually routed instead so when system is rewarded, due to randomness
		caused by routes, the system would use that seeint as the new target?

		But, if RL was target based, the rewards could cause the target to seek the current
		node value. That seems right. Reward should make it seek what has just been hapening
		again. Node value represet what has been hapening (both in environment and action
		spaces), so rewards indicate what we want to happen again.

		Lots of otions to explore on these updates and process. First step for next
		time is to code 0 to 1 node values, and half life updates to see how that works.


2014-01-03  11:36 AM

	More thinking last night.
	The code I'm running has a repeating pulse pattern like this:

	05 ||||||||            |||||||||||||          ||||||||||
	10 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||
	12         ||||||||||||             ||||||||||

	Except the repeat pattern is much longer (thosands of pulses)
	The network output I'm thinking should cycle in a predictible way to
	represent where in this cycle the envrionment is in  something like:

	| ||  |
	  | || |
	    | || |  |
	      | ||| 
	        | | ||| |   | 
	          | || |  |
	      | ||| | 
	    | || |  |
	| ||  |

	Except this wave of outputs should cycle for the full 2000 steps or whatever.

	A more tight version with less randomness might be:

	||||||| |
	       | ||||||| |
	                | ||||||||
	                          |||||||| |

	This is apposed to the idea that the pulses should fan out and form random noise in
	a deep enough net. Random noise outputs certainly do not well represent the state of
	the envrionment so that doesn't seem right. I think this means that temporal
	overlaping activity should tend to merge together, and non overlaping activity
	should stay seperated.

	Thoughts on node values. Watching net run.
	should regulate both mean value and standard deviation.
	If we switch from un limited high value as now, to cap to 0 to 1 range
	the mean could be .5. But could set a standard deviation of .25 to .75.
	So when pulse was inside .25 to .75 range, expand SD, outside, shrink.
	Likewise, mean is tracked by greater than .5 reduce, less than .5 raise.
	Pulses raise value, time lowers it. If we only adjust when there's a pulse
	then we have two things to work with. The value before we apply the pulse,
	and the value after the pulse. Maybe value before pulse should target .25,
	and value after pulse should target .75? I was thinking this a few days ago.
	Adjust half life based on value before pulse. Adjust weight on value after
	pulse? If v too low, increase half life to slow decay. If v too high,
	decrease half life. Constant frequence stream of pulses should target
	values that jump from .25 to .75 with ever pulse in this case.

	But, there are multiple weights coming into a node and that's a problem. The
	different weights being adjusted "fight" each other, and "fight" with the half
	life. So we have 4 different variables fighting to determine the behavior of
	the node. Not right. There needs to be a pulse strength factor in the node
	that all the incoming weights share. Only that one factor then is adjusted
	to control the "after pulse" value.

	The weights in the links, chould be controlled by RL. They should distribute
	"effect" across down stream nodes equaly by default, but allow RL to shape that
	so some nodes get more strength than others. That sounds right. RL controls
	the links between nodes. PL controls the behavior of the nodes themself.

	So with capped 0 to 1 node values, a pulse will move towards 1, and time will
	move towards 0. The speed of move towards 1 will be a combined factor of the
	links strength, and the node pulse strength. Nodes will always try to self
	regulate to a normalized activity pattern that is centered around .5 and that
	bounces up and down with a certain standard deviation. The speed at which
	it bounces up and down however will be a function of how much variablity exists
	in the data stream itself. high frequency noise in the data, will cuase the
	value to bounce a lot. But the deeper we go though the net, the nodes should
	stabalize and slow down with their changing. I would think. That sounds
	like what is should do.

	Need to try and recode everything to follow these ideas.

2014-01-04  11:02 PM
	
	haven't recoded. Added histogram output and pulse traces to better see what
	was happening. In this 3x chicken wire net paths straight through the net
	have a statistical advantage. That leads to the data flowing mostly straight
	through with a bit of spread. I thought a longer net would make it spread out
	futher. But it doesn't. It even seems to encourage it to clump more! As
	seen in a 15x30 deep net. When net switches from 10,12 input to 5,10 input
	the data is at first sucked to the same outputs around row 11. Then when
	switched back to 10,12, it again is sucked to the same oututs for an extended
	timje.

	Paths "burn" into the network, and then when inputs switch, those paths are
	very slow the switch. The new data falls into the paths just bruned in. The
	deeper the net, the more this effect shows up as the new inputs have to
	burn new paths starting with the intput side and working to new outputs.

	Neat to see this "life like" dynamic effect, but not so sure this is good.

	BUT BUT -- if I switch to the better binary spreading net, this statistical
	bias to burn a specific path I think will vanish. And, burning new paths
	in such a net, I wouold also expect to be faster. Just more to keep an eye
	on and understand. Negative weights that force a node to damp quicker than
	normal might be required to force the net to quickly flip from one state
	to another without this slow burn though effect I'm seeing now.

MyNetwork 15 x 30  time: 900122.0
weights in column 0 for testing rows 5 to 12
 5    0.014  0.014  0.014
10    0.014  0.014  0.011
12    0.009  0.014  0.014
weights in column 5 for testing rows 5 to 12
 5    0.012  0.012  0.012
10    0.013  0.011  0.011
12    0.010  0.011  0.012
  .   .   .   .   .   .   .   .   .   .   .   0   0   0   1   1   1   0   1   0   0   1   1   0   .   .   1   .   .   .  
  .   .   .   .   0   .   0   0   0   0   0   0   1   1   2   1   2   1   1   2   1   0   .   .   .   .   .   .   .   .  
  .   .   .   2   2   4   3   3   2   3   3   3   5   5   3   4   4   3   3   2   3   1   1   1   0   .   .   .   .   .  
  .   .  11   8  11  12  14  13  12  12  11  14  12  10  12  11  11  11   8   7   5   4   3   4   3   2   2   0   1   0  
  .  44  25  26  27  25  26  25  23  23  26  24  24  24  21  21  19  20  19  15  13  16  16  13  10   8   8   7   6   8  
  .  44  38  35  32  30  30  30  31  32  34  31  27  29  31  29  28  28  27  26  26  29  32  31  30  27  24  28  30  31       1 =
  .  44  27  30  27  26  25  28  30  31  30  27  30  33  36  35  32  35  42  46  46  56  53  59  60  61  61  66  66  74      18 ===========
  .   .  13  13  15  24  27  29  33  34  29  33  40  43  49  51  57  63  68  78  89  77  85  88  94  93  91  94  98-100      27 ================
  .   .  33  23  38  43  53  51  56  51  61  65  75  74  81  91- 96  95 108 109 111 115 118-113 113-114-121 115-113 110-     33 ====================
  .  99  64  81  78  89  90  86- 91  90- 95 100 102 111 106-112-111-116-115-116 118-111-112-109-108-107-111-106-105 100      27 ================
  .- 99  99-106 104  96- 92- 92  91- 95- 97- 98- 92- 91- 89- 86  84  78- 78- 69- 62  72  64  61  57  59  61- 57  52- 48-     10 ======
  . 116- 83  94- 84- 80- 72  72- 71- 72  63  62- 54  55- 49  42  38  40  29  29  26  21  15  19  20  20  15  16  15  16       2 =
  .- 56- 62  59- 55- 47  42  46  39  39  36  32  29  19  20  15  18  12   8   6   4   2   5   3   4   5   2   3   4   2  
  .  56  21- 24  16  21  18  17  17  10  11   8   7   5   4   5   3   3   2   2   2   2   2   2   2   1   1   1   1   .  
  .   .   7   4   3   3   4   4   0   2   1   1   1   0   0   0   0   0   0   1   2   2   2   2   1   1   1   1   .   .  
pps  993

2014-01-06  9:50 PM

Wrote the code to limit values in nodes from 0 to 1.
Made each node have it's own half life and pulse strength with the idea of having the node
adjust both the half life and pulse strength to noramlize it's long term activity. Thinking I could
make it adjust it's mean and standard deviation values. Couldn't get good results when trying to
adjust both at the same time since they tend to offset each other. Longer half life has the same
sort of effect as stronger pulse updates. In the end, I think it's best for now just to adjust
one of those two paramaters and leave the other alone.

Net seems to be a good random number genertor. Produces different bell curve pulse probablity distribution
outputs. But if I switch to binary try toplogy, that should fanish but also make it far to hard to grasp
what the net is doing.

Need to add reinforment learning and see if I can make the net learn something. See how it constraints
wild paths through the net.

Don't know if this design is realy "learning" to build a good state represenation of the intputs. Mostly
it just seems to be a random  behavior generator. I could work on feeding it more complex types of
input data patterns and see if there is any indication that it's creating good state representations
of the inputs. I guess I could argue that the output propabiity distribtion certainly does change in relation
to the state of the system. That's a good sign I guess.

If I make RL work at all, I could then test to see what sort of input state I could get the RL learning
to recognize. That also seems like an approach. If it can learn to connect input A to output X but not
get it to recognize anything more complex like A and -B vs A and B, then I know the net is lacking in
pattern discrimination power. Or, lacking in the ability to learn to assocate the pattern with the
behavior.

Yes, adding RL seems like a good next step, if not for testing, also just for thinking though how it will
work in this design!

2014-01-07  10:10 AM

Added a reward() function that adjustred link weights towards the current ratio of downstream nodes.
Didn't work at all on a simple test to reward whenever a given input went to a fixed output.

Issues -- Node values are not a real indication of actual pulse paths. When pulse output path is
picked randomly, there is no modification of the three downstream nodes to indicate which way it
went. Only less direct next level nodes are modified. This creates a path 3 nodes wide that gets
updated due to the node's path. This does not seem right in terms of using this to feedback.

Part of how it should work is that random paths that "work" better should be rewarded. So I feel
we need to add a real eligiblity trace to the nodes to do RL with. Could the current node values
be changed to a real eligilibity trace? Instead of updating all paths, and then picking a route,
a route could be picked using current node values, and then update the node only for the route picked.

But, how to make that work? If a node becomes active, and only that node grows in value, it will
only attract more and more pulses to it. How will activity get regulated if this is how paths
are picked? It seems unstable. Negative activity? When node becomes active because of a neighbor,
it causes pulses to go away from it instead of towards it??? So the more active a node is, the
less ikely it is for a pulse to be routed that way? So, link weights set desired node activity
targerts, and the node that is too active, is less likely to get a pulse.

This seems to be missing something important. For pattern learning, the network must remember
assocations somewhere in weight values. But this approach doesn't do that. The link weights
are RL values. And the correlations are not set anywhere. A node that has three inputs,
has no weights that control which input is more important relative to data correlations. That
doesn't seem right. Maybe each link needs two weights. One for data correlation learning
and the other for RL learning.

Or maybe, the one link weight can do both RL and data learning???? That would be cool if it
were possible. I've felt at times that these two learning systems should be merged into a
single optimizaion problem. But I've not seen how that would be possible yet.

After much thought, I've concluded that the self adjusting nodes are in fact doing the
pattern learning. If we had self adjusting weights on the links, we would have three
weights controlling the activity of every node. If we have one weight per node as currently
coded, then each node output drives three downstream nodes with different sensitivites. So
each output fan out does have vairation in what it drives that way. And that fan out
power is controlled by average long term correlations in the signals, which seems to fit
the bill.

But does this work correctly? Does RL learning, just cause the nodes to then un-learn the
attempt to bias? No I guess not. Maybe. Hum

But, What controls pulse path? Weighted random based on Combined node strenth and link
strength? Not node value anymore? So node value is used for tracking long term activity
for doing activity normailzing in the nodes? And for doing RL learning? No, a network
that does nothing more than send pulses down random paths can't work. There's no way
for one stream to regulate the flow of another in such a network. No amplificaiton.

Ok, so we must use node value, of some type, to make pulse routing decisions as well.
But if we update values only on sort, we must then not just keep sorting more pulses
down the path with the most activity.

For RL learning, it seems to me that the node value should accurately track the
true activity of the node which will represent the true state of the network.
So, when the node regulates, it doesn't "fake" the activity level, as it's doing
now, it ends up regulating how many pulses are being sent to the node. The goal
for state recognition, is to make all nodes have the same long term average
activity. But maybe, also try to maximize deviation to maximize information
content?

And that link weights, should bias the flow of pulses out of the node. Ah, but that
doesn't need to match node activity. Flow needs to be a function of link weights
that bias based on RL, and node weights that bias for PL, and node activity
that creates amplification effects. If given weight combinations could predict
expected averge node values, then RL can be based on how node values are currently
different from expected average.

A thought. If even after RL training, the entire network remained balanced so that
all nodes had equal long term activity, and all behavior was created not by offseting
the equalty of activity, but only by controling when given nodes were high and low
then RL learning would be a lot simpler. This sounds like a very good approach!

Ah, so, I need to remember that when seeting link values for RL learning, I can
look at node values on both ends of the links. Small output node value means it
has small effect on nodes, and as such, RL traning should be slower. Zero
output node value means it has no recent activity and could have nothing to do
with reward. Yes, so consider output the power that drives the behaviior, and
make learning faster based on how much "power" it has. Seems right.

So, nodes self regulate to equalize activity in all nodes all the time, even
after RL training. It does it by a paramater in the node, that controls
how much puless get attrcted to the node. RL learning happens in the link
which bias flow of puless from one node to next.

But, I still need to figure out how amplification happens. One signal, flowing
into a node, must have the power to instantly, attract or rebel flows from
other nodes. Somehow. Amplification where a high node attracts more pulses
seems totally unstable. But maybe not if the effect only applies a small bias
and not enough to caues a run away snowball effect. Negative amplification
seems like it would cause entire net to create total equalty of output producing
not real information content in outputs. Just lots of random pules all coming
out at the same rate all over the net. Maybe the default then needs to
be amplification, but given the correct RL seetings, it might produce
negative effects in some nodes?

So lets think. No RL would be equal weights and equal RL bias. Net should become
a percpetion network in that case. Nodes that have strong correlations, will need
to reduce pulse flow to shared outputs to reduce signal strength. Node will
start by random equal distribtion of pulses ot either side of 2 output node.
If it gets interference with one side, that will bias the common node to
reduce in strength and push more nodes to other side. So when second signal
was not there. we would have unbalanced flow of pulses to right side (not the
commond sid). Left side is the common side. When second stream starts,
it raises left to above average. But I want it to not just raise that
second stream, but suck more pulses there as well? So node values will
slightly bias flow as well? Ah, but the node value bias will be fighting
against the node weight bias. So if flow out of first is 1/3 2/3 due to
node bias then that creates stronger right node, which sucks bias to
say 1/4 3/4. Creating a "locked in" effect. Hum. But then second flow
starts, bosting 1/4 to 1/4 + 1/3 say, 7/12, .58 vs .75, and that shifts
the lock-in bias which reduces how much flows to the right, which then will
snowball at some strength level and flip output to being much stronger
to the left. Cool. So having a slight postive feedback bias in the pulse
routing creates a "locked" in amplification effect.

But for an equal balanced node, the locked in effect might not be strong enough
to keep it locked in. So a few random pulses to the low side, could flip it.
Or else, the random distribution of pulses could in general just outweight
the lock in effect in general. So this creates a hysteresis effect in the network.
Making it a strong effect will create the high-contest network I've thought
about (conservtive - strong J) and low contrast, (liberial P).

seems really tricky to balance. Need to experiment. Time for another rewirte
of the code.

Ok, so thinking. If I try to reward the net to connect input A to output X
then if the net tries to remain balanced, there must be other inputs
that can be routed to the other outputs, so all outputs stay balanced. The
ability to route ALL A to X is depent on other inputs being avaiable to route
to all the other inputs to other outputs in the same volume!

So I think I need to rethink what is really resonable in a net that remains
balanced. As intput A goes up and down in intensity, output X should go
up and own in intensity to match it! But actual pulses might not be
directly routed form A to X and pulse volume might also not match the input
and the outputs. For exmaple, if there are few inputs as in my typical
testing, they should spread to all the outputs evenely. So pulse volume
might be far less on the outputs than the inputs in that case.

2014-01-12  3:50 PM Sunday

Need to get back to this. Been distrcated by feeling bad about not doing BGOP web site
for the entire last week!!!! Still need to get the web site done.

But, for this. To apply rewards, we must be able to predict expected averge
node activity based on current reward link weights (and other factors as needed).
Then, the difference between expected activity, and actual activity, is used
to adjust the weights in the direction of what has actually happaned. The adjustment
should be proportional to how different the results are.

A key point is that random rewards, uncorrelated with the signal controlled by
the link weight, should not in the long term, cause the weight to drift.

So all the math of how nodes are updated and how pulses are routed needs to be
done in a way that makes it easy to predict expected averge results.

If link weights directly controlls probablity of pulse routing, and it's too hard
to estimate downstream node values from only the one link weight, then I might
consider adding a link value as well that is a long term exponential decay measure
of only the link activity. That might more easilly track to link weights, and
might be more appropriate to use as the judge for rewards. But if that is all
that is controlling routing, then percpetion probably won't work correctly.

Ah, but if link weight times node weight is the key, then we can use that to
predict volume of pulses routed to each link, and then measure that against
the actual volume in the link weight to drive RL learning.

2016-01-24  2:31 PM Sunday

Don't remember where these notes came from. Did I take them out of the
source? I've been working on xnet.py the past few days and adding notes
there. Thought I would do some thinking here now. Just had a huge snow storm
Friday and Sat after a very warm winter so far. over 2 ft. Still haven't
measured the actualy depth.

xnet a year ago I worked on RL stuff and made some progress. I have
focused so much on the percpetion side I had not yet bothered to even
try RL learning in a long time. Added qValues to each link (not node).
Callucated running averge reward estimate by doing a weigted decaying
average of qValues. When real rewards were received the difference
between the current estimate and the reward, was then back probigated
to all links in the network. Links maintan an elgiblity trace as it's
activity level so the amount of adjustment to each qValue was based
on the strength of the eligbility trace. Each required thus requires
an entire scan of the entire network (expensive) but seems to work
well. All added in other optimization terms in the error equaltiohn
to reduce the mean square qValues to force the system to keep qValues
near zero in size and pentialize large qValues. That helps prevent
the sytem from having qValues fight each other wth one martching
towards infinity where others martch towards negative infinity.

qVluaes then were used to bias flow of pulses towards the links with
the larger qValues.

That all seemed to work just fine. But what wasn't working, was the long
standing problem of how to correctly do percpetion learning, so as to
make the network automatically transofrm sensory data into a high quality
state signal. The same things I've been struggling with for years.

With the above work, I had tried to just not use w learning at all
and just use the qvalues to drive behaviors with stocastic sorting.
That worked fine for learning simple routing, but I realized now it had
no hope for more complex issues beuaes it lacked any rational form of
automatic concept/percpetion learning.

So, just this week or so, I went back to work on xnet, and added a form
of w learning back in. Needed better and easier test environment so
I've spent some time writting a Simulation class to do the main loop,
and process claseses to do event triggered work like pulse generation, and
debug displing. Still needs more work learning how to strcgue things but
in general it's a far better way to do testing and gives me lots of power
to make changes and experiment. I'll even be able to wire in simulations
like the beasty in the maze stuff I've done in the past so that's cool.

But today, the reason for all these notes, is to do more brainstroming
on pnet algorithms.

ALGORITHM BRAINSTROMING

So, the one I just thought up and coded a few days ago, was based
on correlation learning. Tghe idea is that correlated signals must
wire together. I think conceptuaally this is a very strong and sound
abstracgtion as the key to pnet. But implementing a working version
has proven frustating.

So we have a three algorithms that will work together. There is the
algorithm for picking sort paths, using data like the activity levels of
the links and the nodes, and link weights. There is link weight learning
(which in the past has been for the purpose of pnet learning), and there
is qValue learning.

A key idea used in the past, was that default link weight learning would
balance long term pulse flow evenly out all paths. And qLeanring would
bias that balance. And I still like the sound of this..

But this leaves unswered what is the correctg pulse sorting alogirthm,
and the matching version of w learning to go with it?

What I just tried to code, was to create a probablity distribution
using w*Oa for each path where Oa is the activity level of the output
Node (not link). And then adjust w to make it balance. The idea is
that correclated signals would make the output node activity larger.
Which would then actually force w to be smaller, to make sure an even
flow was still maintained. Sounded good.

But when I coded it, it has a nasity postive feedback effect, where
randomly, one path would get used more, then, that would bias all pulses
to follow. It was a run away postive feedback effect that forced most
pulses to keep following one path. And one which no amount of w learning
could undo.

But then I had the novel idea of using a NEGATIVE bias against the flow
path, and I coded w/Oa instead of w*Oa so when activity increases, the
probablity of sorting decreased. This was an approach to bias sort to low
activity nodes! It turned the postive feedback into negative feedback.
That made it act as I wanted. With pulses tending to spread out all over
the network instead of clumping into one path. Great. But w learning as
coded is balancing path flow balancing, not node activity balancing. And
when combined with sorting towards inactive nodes, leaves strange results
of unbalanced long term signals. The two balancing effects seemed to
fight each other. This doesn't seem right.

What to do?

Still need to make correlation sorting happen somehow.I

RECENT INSIGHT

Frequency sorting is not needed. The net should not be expected to
work with a single high speed high frequencey input, such as an audio
signal, and be able to correct decode it down to low speed human beahvior.
Inputs should be bandwidth matched, with output requiremnts! Which means
all that is needed, is mixing and matching the changing inputs to create
the right changing outputs! This is big -- it writes off the need to
do a lot of things I thought manybe such a net would be able to do!

So...

Postive feedback on node activity, runs away, and locks activity to
random paths.

Negative feedback seems to just scatter pulses randomly everywhere with
no real point.

A key idea is that the pulse sorting algorithm, has to be a useful primary
code for building complexity out of. Like NOR gates can be the single
primary tool for creating any binary logic. The othe rlearned w and q
values just write the "code" using the primary tool to create any and
all algoritims needed.

So what might that primary function be? Amplification and control is KEY.
So one small signal needs to have the power to regulate the flow of one
larger signal. A small signal going from 5 pps to 10 pps back to 5 pps,
needs the power to produce a signal that is much stronger in volume.
Which them, might get sent to 10 other places to do more regulation for
example. Things like w values should bias that amplification and control,
but the underlying algoirthm must be able to implement amplification.

The a*w algorithm seemed to do that. Multiplication is amplification.
There's a * in that. Seems possible. :)

Maybe a way to tackle this is focus just on two raw signals binary sorting
to one common node and and separate outside nodes. What is possible here?
Seems like a solid test bed to think about.

It needs to produce 3 somewhat balanced outputs. Or at least, more
balacned than the two inputs. Flow balancing the two sorts, is a
good start. But the middle common shared output  would obviusly by
much larger than desired. Seems to me, that long term slow w learning,
could fix that. But if sorting was based on long ter activity values,
w learning would need to be slower, and longer term than the activity
averages. But that seems valid. The key is that w would not jump up
and down causing active flow shaping but would instead be very long term
slow learning to bias the flows towards balance. Or (aka), balancing it
towards what qLearning "wanted". That sounds good. w learning right
now, is doing sort balancing, instead of output activity balancing.
Might be part of the problem... The negative sorting is doing activity
balancing but fighting with the w tryhing to do different balancing.

Not sure if just "balancing" is valid, but lets code that and look at it.

2016-01-24  3:31 PM Sunday

2016-01-24  6:57 PM Sunday

got lost messing with auto scaling of histograms in net.show(). Ugh.

But, switched to output activity balancing of w weights and it works
really well to do just as desired -- outputs get roughly balanced. Looks
good.

But, on simple two inputs of changing frequency, the common node remains
slightly higher output and is not adjustted for. That might be becuase
I adjust based on outputs BEFORE the current pulse is sorted. Need to
try using values after sort, and see what that does.

Tested updating activity of output node before doing w adjusting.
Made it slightly worse instead of better. I suspect that this is due
to the odd way the activity tracking works and how it's not linear so
there can remain a bias for one of activity under or over the mean and
how it fades in time.

Ah, but I don't adjust w baesd on how FAR over or under the mean the
current activity is. So I'm adjusting to the median, not the mean.
That might be why it's not flater. Will have to think about that and
test. But for now, out to get some food, and ingress for for the hack
a day badge which I'm only a little over a month from finishing the 360
day badge. So lost in this work I almost forgot about it!

2016-01-24  10:42 PM Sunday

Adjusting by distance away from mean made no difference. Played with
other things. Nothing works.

I think the real trouble maker here is the exponential decay of activity
values. it spends more time in it's cycle in lower values than higher
values. Random sorts will more likely find the activity level low,
causing it to raise more than lower. But this should balance across all
paths. But pulses thave have low w values, will tend to sort less often,
find itself in lower part of decay more often, causing it to raise w
instead of lowering it more often. So the futher the w values are apart,
the more this error I think will show up. Here's an idea for testing.
Replace the exponetial dcay, with a linear approximation, so for the
first half life period, it's a linear decay, towards 1/2. I think
in my test here, most all pulses will show up in that lenear section,
so this makes it operation in a linear section instead of exponeital.
Lets see if that forces the system to remove the error!

Nope. Changing to linear decay for first halflife had no real effect.

Then I tried, if w was < 1.0 and being decreased, increase learning
speed by a factor of 10, and reduced learned weight from about .7
to .6 but that still wasn't enough to totally erease the size of the
common overlap. It did bias it to fix offset, but a 10x increase in
learning was a major push and the system still refused to warp much.
The system is fighting being "fixed" like this. Which is sort of cool
that it's so resistant to do things "incorrectly."

Now I'm wondering if this is fighting against what the probablity based
pulse sort system is doing. Oh, yeah. yeah, probablity of sort is 1/a.
So the sort bias is not linear. If activity is .1 high probablity
is 1/(x+.1) and if it's .1 low, probablity is 1/(x-.1).  1/x is not a
linear function. The more the activity tends to bounce around, the less
linear it will be. Which means the longer the gaps, the more distorted
the the system will become. Increasing half life should reduce variance
lets see if the result is less variation even though that's not a valid
solution to this bias.

BINGO. Increasing the half life from 10,000 to 100,000 made the bias
too small to see if it was still there. Also, slowed down w learing
speed by a factor of 10 to make sure it was still very slow relative to
activity and it still hid any bias. w as 63 range vs 70

So, if this is a problem with non-linear issues in the sorting algorithm
I should pause and think about that now. Can I for one, make it linear
to prove this is the problem? Will put activity half life back to
what it was (it was calibrated to measure pulses per second at a half
life of 10,000 ms which is 10 virtual seconds). Left learning slow,
and sure enough bias is still strong so slowing learning didn't help.
lower w was like 70 71 again.

SO lets think about the 1/a pulse routing algorithm.

I picked that as a random idea of how to create a negative feedback
to sort towards least active node. But it's no linear effects I think
might be fighting w learning preventing w learning from totally offseting
uneven sort. So either I should think of changing how pulse routing
is working OR MAYBE, change how w learning works, to mirror what pulse
routing is doing?

Lets think of the mirror issue. A change in w changes w/a probablity of
the sort. But that then changes the a. So then w has to change more.
But it doesn't converge. Ah, but you know, a longer halflife fixes this.
Which means I think that the problem really is an interction of how
much the a value bounces up and down. Ah, yes, since w/a is non linear
due to 1/a if a doesn't change much, the non linear error is reduced.
So that could be what's at work. In the test case, I see the A value
bounce as much as from 8 to 40. So that's major non linear issues.

When I make the network longer and add more signals, the total amount
of noise makes it far more stable and more balanced. So in real life,
this for one, might not be an issue. But if I can make it perfect even
with wide variation in activity, that would be much better. I feel I
need to understand how to fix this, before I can move on.

Lets try a quick experment and remove the 1/a from the rounting and just
sort randomly using w values. See if the system can balance in that case!

Well, gee, it can't! Same issue. w is a little better, at 68 and 69
instad of 70, but still not down to 63 range where it needs to be to
balance. So this is NOT an issue caused by the 1/a sort! Guess again
time!

In other words, this is a straight up problem with how I'm doing w
learning. I adjust based on what path was picked to sort. But I don't
know if that's even a valid idea. If a path is not picked very often,
(beause w is low), then it won't get adjusted as often as the other paths.
That might be the core problem that causes the bias. Is there a way to
not care what path was picked, but to just adjust all w values every
time, based on all activity levels? Or does the speed of adjustment,
just need to be biased by the probablity of a path being selected?
Bias learning speed by 1/w? Not sure, will try it. No, on second
thought that doesn't make sense.

Have used pulse sorted in the past as the key to which way to adjust
based on pulse volume adjusting. But if we are not doing pulse volume
sorting (which we are not) then what pulse we pick each time really
isn't important. I don't think. The idea would be to write this learning
without any clue as to how w values actually bias sorting. Just assume
that higher w values increas activity and lower decrease it. So, if we
just look at all the current activity levels, and calucate error values
of each activity away from the activity mean, could we use all three
errors to adjust all three of the w values in the directions indicated by
the errors? We are currently keeping the product of all three w values
equal to 1.0, so we can't apply three adjustments at once. Can we?

If activity errors are +1 +2 -4, how to do a small adjustment to
all three w baesd on that? Adjust and then re-normalize should work.
If it's error around a mean, then the errors will sum to zero. I could
normalize the SD of the errors as well to cope with scale issues, and
then adjust SD to control speed of learning. Let me go do some coding
and testing using this approach.

Wrote that. Has exact same problem as before! Adjusting all nodes at
once using all Activity level errros from mean produces same result,
as adjusting w, based on only the pulse sorted and the activity error
from meme of that one route. Has same bias problem where strong signals
don't get cut back. w is seeking to 68 instead of 63. Might be a little
lower, but still not low enough.

So, the probnlem seems to be a side effect of activity levels jumping
up and down too much causing non linear learning effects.

In my test, the two inputs are changing rates from 100 to 500 gap
sizes, which is 2 and 10 pps. Call that average total volume of 12
pps combined. They fan out to 5 outputs. So 12 pps / 5 is 2.4 pps on
average per output path. With a half life of 10 seconds. Or, low end
4 pps total creates .8 pps average outputs. And high end is 20 pps,
creating average output per path of 4 pps. Which is what I'm seeing.
Highest output levels are 43 which is 4.3 pps.

The problem with my test, is that it jumps up and down in frequency at
a very slow rate! 100 seconds to change frequency for one input, and
140 seconds for the other. And the half life of this system is only
10 seconds! So it starts to adjust w for one set, then it changes,
then adjusts w for the next leve, then changes. Even though w speed is
slow enough, there is a fade up and fade down effect due to half life I
bet where it drifts one way faster than the other when the inputs change.

The half life is the short term memory of the system. Basically it
doesn't remeber was happaned before. Might look into this some more but
it's not w learning that is the problem here. It's the fact that the
envionemnt is shifting and total average pulses is not the same thing
as total time spent in different environments due to how fast the system
shifts from one to the other.

By making frequencey shift once a second for one, and once ever 1.4
seconds for the other, w learning has no problem at all creating a flat
output. So long term pulses sorted, that was measured by the histogram,
was not the same thing as setting w correctly to operate in all those
different enviornments with a single layer network! Or, we could say the
histogram, when used with that sort of test, wasn't showing a problem
with w learning. It was showing a problem of a network that was too
small, learning to cope with too complex an enviornment.

So I think we can call this case closed for now. The network can't
balance w values, if the network is too small to cope with a complex
enviornment that changes beyond it's half life short term memory.

So, as we say in testing, each layer in that ase is better balanced then
the one before it, but it would take mutliple layers to do a good job
of balancing in such a complex enviornment.

It also shows the problem of what can happen if the inputs aren't somewhat
well balanced to beging with in total volume. In this test, the total
voume was jumping up and down drastically, from 4 ppps total volume to
20 pps total volume and staying at those different volume levels for
way more than the half life of the activity traces.

So, it's late. Tomorrow, I'll clean up code and move on.

End result, w learning to balance output volume seems to be working well.

Next I'll have to start thinking about whther this w/a PD pulse routing
alogirhtm is the right one, or if something else would be better. But
a working volume sort, does some cool stuff for learning correlations
I think!

LIFE NOTES:  Newsreader shut down this past Novemeber so I'm no longer
tied to that at all. Money is running short, so it's time to make this
AI work, or go get a real job. Avi has tried to talk me into working
with his new San Fran networking company. The idea of working with
DeepMind on RL reserach would be cool, but a big step to move to London.
And if I can make this work first that would be better.

2016-01-25  2:31 AM Monday

2016-01-25  12:04 PM Monday

Got to Starbucks for the first time since Friday. Roads are starting to
show pavement, snow is piled high everywhere and many lots are no cleared.

So, what to researech today? Quick testing just now showed the network is
balabncing very nicly, but yet also maintins lots of noise and variablity
on the outputs even in a deeper net. Tried binary tree net as well and
same is true.

Yes, binary net blances very quickly on it's own for a deeper net even
before w learning is finished. Deeper parts of net then show random flow
with all w values around 100 like 99 vs 101. But yet, output is still
highly chaotic even though it's well balanced in the long term volume.
But is the chaos useful chaos or just the result of all the random
distributions?

Quick testing with 10x larger halfLife. Variance of chaotic behavior
seems to increase as a result of slower response times of ActivitTrace.
More data flows the same path for longer. Increase is only small but
the bell curve seems to have longer tails if I were to create a bell
curve for the varations. Good thing is that it doesn't fade out and
become white noise with longer half life.

This is all looks very good on the surface. But is this a chotic system
that can be harnsed to create something good.

---

Ok, got side tracked, and decided to add Q learning to w learning
algoirthm that is working so well. Uses qValues from links to calucate
a targer output balance, and then adjusts weights to hit that blance.

But, ok fuck. Problem. Different nodes will have different balance
targets, and with a complex enough net, it will all create a feedback
problem.

So X feeds to output nodes  A, B, C, and  and Y feeds to output nodes
B, C and D. But if X has one target ratio for B and C and Y has a
different one, they will fight each other to try and dominate the other.
This is not good. This is why flow balancing made sense. the nodes could
balance their own flow and not "fight" with others trying to set diferent
balance targets!

But pulse sorting MUST be related to interactions between different paths.
w learning, must learn correlations in some way. I guess if pulse
routing changes by shared interaction, then learning w to blance flow,
will learn corrleations. Maybe.

So to make this work this way, we have to calculate flow targets per
node somehow, so that the two upstream nodes are trying to set the same
relative volume targerts for the nodes! Which I think means, we have
to use node based qValues instead of link based qValues. And a flow
rate caluation that will always come up with the same relative answers
for nodes.  (aka A 2x B, will be true from both feed paths).

It really seems better to me to think of the links as the "behaviors"
and track Q values per links. But I guess in fact, once a link has
selected, the "firing" of the output node, is the behavior we have forced.
But no, the linking of the two nodes is what's more important. To
not use that, would reduce the "inteligence" of the system.

X->B might be very important, but Y->B might be very bad If we only
masure the "value" of B, and not the links, it wold make it impossible
for the net to learn X->B separte from Y->B. Shit, I have to
redue the learning code I was just falling in love. But, it's still
activity balancing. Just need to use the link activity instead
of the output node activity. Or, should it volume balancing? Volume
balancing is easy to code by adjusting w for each pulse path and adjusting
the size of the adjustment by the volume ratios.

Why did I decide to do output balancing anyway? Becuase it better
reprensted corelation learning? If I do flow balancing, it's not
representng correlations. Is it? Yes, that was the issue. The two
shared nodes, ended up getting a larger w, instead of a lower one when
I was trying to do path balancing.

So, lets think this through. Shared node gets 2x the pulss, so it's
volume is higher. Volume balancing doesn't care. Volume is balanced.
But higher activity gets less pulses sorted that way. Then volume
balancing sarts to care. And tries to push more to the shared path which
cuases probablity to lower. So they fight each other, stabalize at some
point, with w high for the shared path, and with the volume of the shared
path, higher than the others.

So it feels like when we do flow balancing, with a pulse sorting system
based on node balance, the two fight and end up in a stange configuration.
Which is why we switched to activty balance, and foundit worled well
with an activity balancing pulse sorting.

So, if we have to switch to flow balance for RL learning, can we flow
balance the pulse sorting? Or make it "fight" with the w learning in
an inteligent way?

What if the qLearning happens not as part of flow balancing, but instead
as part of the pulse routing? So w learning for output balance can
stay the same? So the goal is to keep the outputs of the entire network
balanced, no matter matter what RL learning does. RL learning determins
What gets sent where, but not the total volume. That might work. So if
RL qValues cause more flow from X->B, then w learning will cause less
flow from Y->B to compensate. That sounds good, becuase X->B might have
high qValue, but if Y is a useless signal, then it won't learn to bias
away from B on it's own.

Ok, so we can think of w values as long term statistial percpetion
learning.

And qValues then become direct routing programming. So the "program"
that controls the behavior of the network is in both the w values and
the qValues. And reacts to current flow rates.

I'm also thinking that the a fundamental way two signals get combined
should act like multiplication (amplificaiton). Is that possible with
this idea to put qValues into routing algorithm?

Playing with routing with q*w/a as the probablity distribution. See
if the network can learn doing this!

2016-01-25  7:28 PM Monday

Went out for food at Noddles with Pig.

Testing shows odd results for trying to get network to route pulses
from input to a given output. Not really working. But was showing 2x
output from the correct network for a while. Then later, it was all
balanced out.

q learning seems to be fighting aginast w learning. I think maybe I'm
trying to train the network to do something it can't do -- which is my old
standby test of forcing pulses from one input to route to one output.
But in a network that must remain balanced, this is not possible.
All pulses from the input would be too much output. So either this
approach just isn't working. Or, it can work, but not for the specific
sort of testing I'm trying.

Instead of trying to make the network cut a direct path from an input
to an output, what I need to do is try to get an output controlled
by an input. Meaning when the input is high, the output goes high,
and when the input is low, the output goes low. Using differential
control, maybe use two outputs to drive a pulse generator? Or use the
activity volume of an output and try to train the net to control it
from an alternating input? This mjight be a good aprpaoch, but I have
to limit the systme to doing the thigns it's able to do.

2016-01-26 1:09 AM Tuesday

Time for bed. Just wanted to add some notes. Need to more careful
playing with this q*w/a distribution sorting to understand what it can
do or can't do combined with activity balancing.

With activity balancing, we can't expect this net to send different
signals to different places directly. It must be baesd on relative
activity levels shifting so I need to think up some simple experiments
to see what it can learn and think though if this is working or not.

The use of negative feedback by activity along with w learning creates a
negtive effect and I want to see if that can be used to get this system
to learn a NOT X type of signal effect. The final solution must have
that power. And test for amplificaiton ability can one small input
signal, be used to control many outputs?

And, I'm not sure how RL is actually coded at this point. Need to look
at that and make sure it's valid.

================================================================================

2016-01-26 2:08 PM Tuesday

Getting started. Was doing some testing. Not learning RL at all in
this configurtion. Maybe I need to just put RL aside for the moment,
and focuse on whether this current approach does percpetion learning
like I want and whether this is a good appraoch.

2016-01-26 10:02 PM Tuesday

Wrote Hitogram class and added it to PulseGenerator so I could track and
print output histogram by input pulses. Changed Network to use the new
class. Set up rewards to reward P1 to output 4, and punish P2 to output
4, but did nothing to pulses sorted elsehwere. This basically worked.
It distored the pulse historgam so that output 4, is made up of 90%
P1 and 10% P2. Total output is still balanced as expected. This might
be significant in that this is more of what this design network is able
to do. Not just sort all pulses one way, but rather, adjust ratios.

Also, before rewards, the histograms per pulse were nicely variant with
each output being different levels in different outputs. But this is
the 3 way chick wire topology. The binary net might mix it all better
and show little bias? Need to test. Letting the last test run for
a long time to see if things shift.

2016-01-27 11:26 PM Wednesday

No real work today. Got distracted on facebook. DeepMind news about
their go program! Didn't know they were working on go!

Left program running doing RL learning of routing pulses to one output.
works well. But interesting that one run got up to a 99.86 type of
level of pulses to the one output from P1 source but other run was a
little lower. The nets seemed to lock into slightly different random
configurations where one worked a little better than the other.

This was done by rewarding all pulses to output 4, with a 1 if it came
from the P1 source and 0 if it came from the P2 soruce. No rewards were
given to pulses sent elsewhere.  

MyNetwork 10 x 6  time: 61607572458
  . 100 100 100|  . 100 100 100|  . 100 100 100|  . 100 100 100| 11o  .   . ***| 13o|   1222 ==========
  . 100 100 100|  . 100 100 100|  . 100 100 100| 12o*** ***   .| 13o***   .   .| 12o|   1193 ==========
  . 100 100 100|  . 100 100 100| 16o***   . ***| 13o*** ***   .| 12o*** ***   .| 12o|   1247 ==========
  . 100 100 100| 22o*** ***   .| 18o*** ***   .| 13o*** ***   .| 13o182  36 151| 12o|   1238 ==========
 60o579   6 296| 27o*** ***   .| 19o*** ***   .| 13o  . *** ***| 13o***   . ***| 12o|   1223 ==========
  . 100 100 100| 24o262  14 276| 18o  1 666 ***| 14o  . *** ***| 12o  .   . ***| 12o|   1268 ===========
 60o***  26  30| 19o  . *** ***| 18o***   . ***| 13o  . *** ***| 12o***   .   .| 12o|   1153 ==========
  . 100 100 100| 28o***   . ***| 14o***   . ***| 13o***   . ***| 13o  . ***   .| 13o|   1206 ==========
  . 100 100 100|  . 100 100 100| 18o***   . ***| 12o  . *** ***|  9o  . ***   .|  9o|   1164 ==========
  . 100 100 100|  . 100 100 100|  . 100 100 100| 15o***   . ***| 11o  . ***   .| 11o|   1181 ==========
p1 percent for output 4 is:  95.91
P1
 0   1222 ==========
 1    270 ===
 2   1243 ===========
 3   1069 =========
 4   1173 ==========
 5    208 ==
 6    364 ===
 7    440 ====
 8     44 =
 9      .
P2
 0      .
 1    923 ========
 2      4 =
 3    169 ==
 4     50 =
 5   1060 =========
 6    789 =======
 7    766 =======
 8   1120 ==========
 9   1181 ==========
P1 sorts to output 4
--------------
avgRewards   0.9591
qMean:      20.4039
qVar*F:     -19.5345  qVar:  19.5345  qVarFactor:  -1.0
qSkew*F:    -0.0000  qSkew: -0.1822  qSkewFactor: 0.0
qKurt*F:     0.0000  qKurt:  1.9109  qKurtFactor: 0.0
estimated:   0.8694
reward:      1.0000  TD error: 0.1306   MSE: 0.0376
P2 sorts to output 4
--------------
avgRewards   0.9629
qMean:      20.3437
qVar*F:     -19.8088  qVar:  19.8088  qVarFactor:  -1.0
qSkew*F:    -0.0000  qSkew: -0.1981  qSkewFactor: 0.0
qKurt*F:     0.0000  qKurt:  1.7813  qKurtFactor: 0.0
estimated:   0.5349
reward:      0.0000  TD error:-0.5349   MSE: 0.0367

I still don't have a clue what else this network can do or can't do. Need
to do more careful testing and thinking about what this algoirthm is
capable of.

================================================================================

2016-01-28 10:51 AM Thursday

Left a 8 layer deep binary tree wire net running last night with the
same reward for sorting only P1 pulses to output 4 and the output ratio
this morning is bouncing beween 100% and 99.92%  MSE 0.0019 to 0.0008.

On last checking the network was unable to predict the 0 rewards and since
it's getting the behvior nearly perfect the 1 rewards are dominating
the system and preventing it from learning how to predict 0 rewards.
Also, due to the fact that the path the p1 nodes must take to reach the
one output will have all qvalues of 1.0, so they dominate the qvalues of
the paths in the network. The few times that P2 pulses make it to the
same output, they are probably using paths that that the P1 pulses use
to get there, so can't learn that as a bad path. Then again, everywhere
a P2 pulse can cross into the P0 path should become a bad path with a
highly negative q value to allow the system to predict a zero reward.
Maybe the issue is simply that it doesn't happen enough to learn that?
Or maybe, the odds of the P2 making a mistake, is aboujt the same as
a P1 pulse accidently getting into the P2 path, but then accidently
finding it's way back to the correct P1 path, and being rewaarded as good?

I guess the ultimate issue is that it can't learn valid qValue for
something you are asking it never to do. It can only learn to estimate
rewards correctly for things you want it to do sometimes.

================================================================================

2016-01-30 10:54 AM Saturday

Wrote HallTest last night. This new set of Simulation objects are
making this so much easer to code this stuff now which is very good.
I've needed easyer ways to test for a long time now. For 15 years or
so I've had this insight to use global feedback allowing the learning
network to see what behaviors it's generatin so it can learn sequences
of behavior, but in all this time I've never actually coded it, becaue
I've done nothing but focuse on making the learning network work and it's
never worked as well as it needs to. But with these new Simlation class
objects it will be trivally easy to code this for the first time! And it
feels like this version of the learning system is finally getting close
to working well enough that I will need to code the full fedbback loop.
But with these objects, it will be trivall easy to create output pulse
generators that are controlled by the network to create outputs, but
also feedback to the net!

For example, in the alternating hall problem I just coded, if the Beasty
doesn't have sensors to tell it which way to go to find food, it has
to learn the beahvior "keep walking left" and "keep walking right" to
solve the problem with a long hall. If it has hall bump sensors, it
need to know bump right triggers the start of walk left, and bump left,
tirggers the start of walk right. But the even harder version of the
problem is with no bump sensors, where it has to learn walk left x steps,
walk right x steps and repeat forever -- which is behavior driven 100%
off the output feedback signal.

But last night, I coded the easy version. It's got smell sensors that
tell it which direction the food is.

So to explain the hall problem for future readers... It's an agent I
call a Beasty in a 1D grid world. Just a simple hall of fixed length. And
the goal of the Beasty is to find food in the grid world. Food is palced
at different squares in the grid and when the beasty steps on that grid,
he gets a reward for finding and eating the food. In this hall, food is
placed at one end of the hall on the last grid. When the Beasty finds it,
another piece of food magically shows up at the other end. The Beasty
has to march in the other direction now to get the food. This repeats
forever. So the solution to maximising food, is to just keep marching back
and forth. But it's a good simple test, becuase the beasty can't just
learn "walking left is good", or "walking right is good". It has to have
the power to learn the more complex sequence of walking back and forth.
But how hard th eproblem is, is also controled by what sensors the Beasty
is given. The hardest version, is the Blind Alternating Hall problem
where the Beasty has no external sensors at all, and only behavior
feedback, and reward inputs and must learn to walk back and forth blindly.

But last night, I coded the simplest version possible. It's got smell
sensors telling it which way the food is. Two pulse inputs for each side.
The right sensor sends out high frequency pulses when the food is on the
right, and low frequency when on the left. The left smell sensor does
the opposit. The beasty is rewarded when it randomly finds food. But I
also made it even easier, and reward it with +1 when it takes a step in
the right direction, and -1 when it steps in the wrong direction. Left
it running all night, and it's showing no signs of learning even this
most simple problem.

I set it up so pulses out the top half of the net walk left, and pulses
out the bottom half, step right. Steps are actulaly coded as .1 and
the grid is displayed as ints so it takes 10 steps to get though a grid
as displayed. Which is probably stupid now that I think more, because
all I've done is created a longer hall problem and displaing it with
ASCII graphics as a small one. Why not just display it as it is. :)

World display looks like this now:

+-----+
|.B..:|
+-----+
Beasty location: 1.4
Food Count: 1

Ok, with all the above as background documention, I need to brainstorm
about what's happening and get to the botom of why it's not working.

Without careful thought, the first fear is that I've again asked the
learning system to learn something it can't, due to the balancing
requirements and with asking it to use all outputs to control
behavior. So, let me fix the .1 step problem, and display it as it
really is, and then change the control to only use two outputs to
move the beasty and allow the system to ignore the others.

Then, I can leave that running while I go get my morning Starbucks!

2016-01-30 4:08 PM Saturday

Did that. Didn't look much better. But I've added more careful measures
of food per seconds and I've been letting it run while geting distrated
facbgooking about Basic Income. Now time for lunch

Thought: System needs to learn from secondary reinforment. And if it's
not being rewarded, it's not learning. So a stream of 0 rewards, could be
a way to force that secondary reinforment learning. But is that right?
Could it be that the the system should maintain something like a total
average flat qValue in the system as the default "no reward" value, and
then measure the difference between current estmated reward, and that
"mean reward" as the the value to back propigate as secondary reiforment?
Shit, I thik that's it. The system needs to mantain long term "average
reward" and then use that value as the regular "do nothing" reward once
a second or so to back prop secondary reinforcment!

This gets back to old ideas I had about having to "balance" rewards by
making the postive rewards balance with the negative rewards.

Yes, this is good. Becuse if I do random reward(0), but the average reward
of the system is actully say 20 then those random rewards are really
acting as random punishments that would really destort behavior (aka
don't do what you are doing to get those 20 average rewareds). Rewarding
with the average rewards, I think jsut says, "this is what you expect for
doing what you are doing". So when it does something better than that
it back props a +error, and when it does somethin gwoerse than averet,
a real punishment is back propped.

After lunch, I'll experiment...

Oh, and for the record, the run I just left running, started at Average
Seconds Per Food of 43.0 after a minture or two. It's now down to 36.2.
I'll check to see what it is when I get back.

2016-01-30 6:40 PM Saturday

Done with food. Modified food averge to do full avergae of first 100
then start the exponential average. Started high, but dropped down
to around 40 by the time I got back. The older one is around 37.8.
Don't think they will get much better. Want to play with this idea
of using average rewards for idle updates. Have to think a bit
to try and figure out what is the right average, and how to mix
with more more complex formula using variance etc.

2016-01-31 1:50 AM Sunday

Looked at adding average reward updates say once a second to create
secondary reinforcment, and got lost in the complexity of how RL is
working here or what is "right" for this. I've got code calcuating running
avergae of qValues to use as the key reward estimator. But since we
use Activty Tackers to distribute error, the discounting of the avrege I
think should follow the exact same algorithm, and it doesn't. The decay
is pulse count based vs simualtion time based. And I have no idea if
the two decay factors come anywhere near close to creating the same
average effect. And if they don't, then error updates are likely bogus.
So that needs to be fixed. Or reserached, to see how much of a problem
it is if it's not correct? The adaptive nature of the system might adjust
for that error making it not important. But I dounbt that is really true.
So I need more careful testing of RL of how it's calculated.

The hall problem even in it's simple form, really isn't working.
The net seems able to improve it's food finding a little bit, but never
masters it. But it's all way to complex for me to figure out why it'
snot working better. So I need to return to basics, of what this net
should be able to learn, and make very simple tests, to show how it
learns verious simple things, to see if it can and if not, why not.

Such as, one input, sorting to one output -- but adjusted for the
balancing effect of this current net design. I'm begining to question
this logic of the blancing effect, that is NOT backed up by RL shifting
the balancing ratio. So I have to think on that.

Other back to basics, is can the net learn to route one signal to two
different places.

Can it learn to adjust the power of the signal in an output, such as if
a signal rounted to one place had to be twice the strength of the same
signal, routed to a different output?

Can one signal act as a gate for another? and how is that learned?

Can two signals be combined as a sum to produce an output?

Can two signals be combined as a difference, to produce an output?
This might be the same qustion as gate, or control?

I think if I did into questions like these, and try to make the smallest
simplest net and eviornment to test these sorts of things, I can both
figure out if RL is coded correctly, and if the net, as it works, even
has a chance to learn these sorts of things.

It's complex and confusing, but I need to go back to basics, and see
what works and what doesn't.

Oh, and in the confusion of RL, I'm using a qMean varance term in the
error equation, which means I'm asking the network to learn to minimize
the variance of qValues over the network. And the factor is set to -1
(as strong a tgerm as qMean). Darn that seems agressively wrong. This
is a feature to minimize the net from overfitting the solution with
wildly different qValues (like -1 10^20 vs +10^20 for different values.
Which seems fine but the size of the term the error equation seems
too agreesive which means qMean values don't just measure rewards, but
also measure their own varriance? I'll have to think about what this
measns mores. Bit it's left me verry confused about what the correct
"averge" should be, to implement as secondary reinforcers.

I'm thinking however, I need to set up an ActivityTrace for all rewards,
and calculate a discounted average using the time based discount for
that Activity Trace to calcuate the correct average reward. And, should
that reward be averaged in as well Seems like maybe it should? Got to
look at all this under some simple tests to prove what the net can and
can't learn, and to lead me to making a net that can learn evettyhing. :)

2016-02-01 12:32 AM Monday

Ok, I've been looking at RL and rewards and getting confused and feeling
lost, but I'v just had some clearity that feels good.

Wrote and rewrote ActiveAvg to calcuate qMean using correct discounting
to match ActivityTrace discounting. Use it for qVar and avgRewards as
well, so I have a secondaryReinforcment() that does average rewards for
the system. Still need to play more with that. Right now I've got that
turned off.

Deleted all the skew and kurt statistcs that I was playing with. I see
no value in them anymore so that cleans things up.

Now, I've been looking more careful at what I'm doing and how it works.
Set up a simple test that had two signals in a network only one layer
deep, faning out to 3, but not even interacting. Every pluse for one
output from the first pulse generator, was rewarded -1 uncondionally.
And every pulse out another output, that only the second input could
reach was rewarded +1.

Played with halfLife and learning speeds a little to see what was
happening. It was learning qValues very slowely at first, so I speed
it up so I could see what was happening.

Came to realize something important! When the qMean value tries to
estimate a reward, it fails badly! It can't ever learn to output a
1 for the uncondional 1 reward, and 0 for the unconditonal 0 reward!
And THAT"S NOT A PROBLEM! Beucase all that it is important, is that the
qVaues get CALCULATED CORRECTLY! We never ever give a shit, about what
the system is estimating with the qMean calculation.

What the qValues values converge on, is the discounted future expected
rewards! (discounted by the ActivityTrace discount function). So even
though a link leads directly to a +1 reward all the time, it's qValue
can't reach a value, that would allow it to force the qMean to jump to +1.
And we don't want it to! We want it to reach a value that tells us what
all future rewards will be, discounted back to us, with the ActiveTrace
discount fucntion. And becuse, with this forward looking view, the next
reward to show up, will be that +1 pulse, we can be sure to know this
qValue will refelct that!

So this is what the system is learning at th emoment:

yNetwork 7 x 2  time: 224:43:20.0000
 39o 21  23 ***   196   170  -690  14  11  14| 11o|    175 ==============
  . 100 100 100     .     .     .   .   .   .| 14o|    193 ================
  . 100 100 100     .     .     .   .   .   .|  . | 
  . 100 100 100     .     .     .   .   .   .| 22o|    181 ===============
 68o464   5 465  -185   690  -182  22  24  22| 24o|    193 ================
  . 100 100 100     .     .     .   .   .   .| 22o|    176 ==============
  . 100 100 100     .     .     .   .   .   .| 14o|    182 ===============

That -690 is -6.90 qValue which leads to the -1 reward out that path.
The +690, is 6.9 qValue which leads to the 1 reward out that path.

So even though we have strong qValues indicating which path to take,
the w learning, has offset the qValues, and is forcing an even output.
Intersting, the qValues were learning faster than the qValues at a point,
so that second output (-1 reward), was near zero for a long time and the
5th output (1 reward) was about twice the output if it's two neighbors.

But in time, the qValues have started to converte on a "correct" forward
discounted qValue, and the wlearning was able to catch up and flatten
out the output again.

I'm thinking this flat output (that can override RL) is bullshit again.
If those qValues are correct, then we SHOULD NOT send putpts out that
reward(0) path. But yet, this output blance is most certaonly required
to crate preveptoin learning, so I just have to think though the correct
way to do it. Probably go back to somethign where the percpetio elarning
is once again, adjusting outputs to MATCH what the qValues suggest.
Later, for that.

Other disovery. The qVar is bull shit. I turned it off by setting
the qVarFactor to zero. It measures how much qMean bounces up and down
for each qValue update. And I was using it tin the error function to
make the system minimize how much qMean bounced up and down. It was
from the concept of trying to prevent the system from overfitting the
data. It would limit how large the qValues could become. But with how it
currently works, there's no danger of that. The qValues simply mindicate
discountged future reards and those won't martch towards infinity.
No need to pull them back towards zero that I can see. We want them to
tell us the systems best understanding of true estimated future rewards
and this code as written, without the qVar does that. Currently, I see
no overfitting issues that need to be fixed with this approach.

Next, I'm using MSE to estimate the varriance of qValues. That's sort of
correct, because it's the measure of variance of the error value, which
is what is being propigated to change qValues. But each qValue also has
other factors effecting it's true variance, such as it's average activity
level and I'll have to look at the update formulat again to know for sure.
But I'm thinking I will either need to calcuate a true variance of each
link qValue independely, or figure out a correct formular for using MES
to calcuate what the true valuraince is.

Then, knowing the true variance of each qValue, I hope I can use some
logic to answer the question of what the ideal perfect explore/explot
approach is, using the overlapping bell curve of each qValue in a link,
to map those, to the correct probablity distribution for path choices!
Instead of the estimated guess formula I'm using now. I should start
by calcuating true variance just becuse I can.

Becuase the outputs are balanced, the probalites become fixed, and allow
the qValues to adjust to their correct values FOR THAT policy. Which
makes the whole system very stable, even if in this exmpale, the syste
is unable to make any use of the known and correctly calcuated qValues!

It's nice that this simple approach shows stablity. But when the changing
qValues cause behavior changes, there is a problem if qValues are changing
faster than behaivor changes. The behavior lags behind, and a rising
qValue cuases rising probablity, but if it leads to worse performace,
the qValue drops, and then later use drops. If there is some staple
point, where a given probablity distrbution cuases maximal qValues,
the system must not ossolate up and down around that point, it must
somehow work so that it converges to that point, instead of issolating.
Will have to remeber to keep an eye out for that and try to understand
what causes it and make sure the sytem is balanced correctly to prevent
it. I Would assume, the main approch, is to make qLearning slower to
give the agent plenty of time to test the new values. Or maybe the
problem doesn't even exist and I'm making up nonsense?

TODO:  Calculate individual qValue variances (as it changes from
visit to vist of pulses that need to make decisions). See what
that tells us and think of how to better use that in the mapping
from qValues to path probablity distributions.

TODO: See if this test left running, is in the same place in the morning.
Is it stable? And if so, try running it again, with slower learning
and longer halfLife?

TODO: secondarReinforcement? Turn it on, and see what it does in this
simple one layer test.

So psyched to understand what it's doing and to understand that qValues
are actually being calcuated exactly correctly now an to know I can
ignore the fact that qMean failes to estimate rewards.

TODO: there's a speed factor in terms of how fast TD error is updating
qValues. What is "correct" for that? FIgure that out. What is the ideal
correct value? Does it need to automatically adjust to slow learning
and speed learning based on whether qValues are converging? It doesn't
seem to me that it is something that should be picked to match the
environment. Or at least, whatever it is that I would use to set it
to match the enviornment, should be possible to automate in the system.

================================================================================

2016-02-01 1:36 PM Monday

Changed halfLife and speed, and let the simple test run again
overnight. Surprised to see it converge to very different qValues.
But now, a few hours later, I realizse it's ont done converging
yet. Want to do a quick experiment to run +1 and -1 sequence
through Activity Trace and see what values it cycles through
between the +1 and -1. Shouldn't it coverge to what the
qValues are reporting>  But wait, it seems impossible they
could coverge to that because the longer running one with
the old halfLife has covertged to 6.9 and -6.9 and the new
one has converted to 5.2. That's not possible with +1 and
-1 is it? Could it be I forgot how this is displayed and
it's really .69 and .52? That would make more sense.

Ok, so I need to test so I can really understand what qValues
converge to. Maybe I don't understand qValues as well as
I thought I did last night.

Also,  rewarding with avg rewards, seems like it's not the
right way to secondary reinforcement any more. Becuase
qMean, is never a real "predictor" of an instentanious reward.
Ah, but it IS, a predictor of long term averge fugure rewards
is it not? Maybe it is right. Was thnking that if one qValue
got a large value, like a massive -1000 reward was imprinted
onto the network, would the large values created every correctly
spread through the net by reinforcment since qMean would never
be "large" in that way? I think maybe it will work just fine.
Have to think deeper on all this. But first, test qValues
and understand them better.

----

Shit, nope, qValues are not predicting future rewards in the same way
ActivityTrace measures them. A string of +1 -1 rewards as is being done
in this test produces min and max Q values in the range of -.3 to +.3.
But my qValues on this run are -5.3 to +5.3 for the qValue leading
straight to the reward.

What is it showing?

Discounted, sum of all future qMean prediction errors? If qMean was a
prefect predictor of every reward, then the discounted sum of all errors
would be zero. Ah, exactly. qValues converge on the value that MAKE the
sum of all future qMean prediction values zero! (based on current policy
of course). So the qValue, pushes the qMean in some direction, but the
qValue is much larger, than how much it "pushes" the qMean. So a qValue
of 6, may on average only change the qMean by .2 or something, so .2 is
how far the qMean has to be pushed, to make the sum of all errors zero.
And .2 would be the real expected dicounted GAIN in rewards, not 6!

So what if I just make the qValues change the qMean, insead of track
it as a running average? Hum. Lets try that and see what happens.
Seems it could explode...

yes, it oscillates wildy getting larger and larger.

Ok, so now I'm seeing that using correct discounting of qMean is not
so important. However it is calcuated, the qValues converge to the
values needed ... wait no.

The error update must use the correct error gradiant to apply changes to
the qValues, and our Activity trace defines the correct error gradient,
If, the Activity trace also correct defines the future qMean values.
So what we back prop as an error form the future, must be applied  in
the correct amount, based on how much the qValue of the past, changed
the qMean of the future. And using the ActivityAvg way of calculating
qMeans, does that. The qValues matching link ActivityTrace, tells us
how much that qValue, has effected the qMean of the future.

Ok, so qValues are not reward predictions. They are numbers that converge
on the values needed, to push qMean, enough so that the sum of future
reward prediction errors, sum to zero.

Ok, so if the average qmean value when entering a node is 5, and some
qValues are 6, and others are 4, then the 4 will push qMean DOWN, where
as the 6 will push qMean UP. So they are not zero based numbers if the
system's average reward is not zero. Postive qValues don't mean "good"
and negative don't mean "bad". Postive qValues don't mean better
than average, and negative qValues don't mean "worse than average".

So just need to keep that in mind that qValues here are not 1) actual
reward value predictors (not discounted estimates of future rewards).
And 2) not zero based. But they do mean that higher is better and
lower is worse. So like other qValues, the higest path is still
the best path, assuming the qValue is actually correct.

Can I, or should I make them actual discounted reward predictors?

Also, qValues only measure the "value" of the path, under current policy.
If policy changes to use the better path more, the qValues could change,
and even go down.

In that case, shouldn't policy keep drifting twoards the higher qValues,
until the point they are no longer the higher qvalues?

So, instead of qValues setting policy, shouldn't qValues indicate how
policy needs to change? And if policy drifts to send all pulses out
the best path, and it's qValue still shows ir emains the best path,
then so be it. But if policy drifts towards the best path and there is
dimishing returns, it should keep driving until qValues equal out. if
qValues define policy, instead of define how policy needs to change,
then policy will never reach optimal location of dimihing returns.

Yes, this seems right. So change in policy, that determis how much
each path is used, needs to be driven by diference in qValues. The more
difference in the qValues, the faster behavior needs to change. The net
will equlize at the point where all qValues are equal it seems to me --
equal to the average rewards for the current policy. That shows the
net has converged on the best policy possible.

Ok, so qValues can't BE policy. So sorting with qw/a is WRONG because
that makes qValues the policy! If w is policy, then change in w needs to
be driven by q values. So having w seek a target defned by qValues is
wrong as well becuase that again, is a way of making qValues BE policy.
So w needs to be free to change as much as is needed as long as Q values
are unqual. But of course, they may always be unqual, so policy needs
to be capped in how updates happen.

And this suddenly feels like I no longer need to keep things like qValue
variance since as long as qValues are unqual, w drifts. And as q Values
chagne, w can drift up and down and average out over time automatically.
Maybe w variance can be tracked to control the speed of w learning in
this case.

So that seems straight forward. Change w updates to make them move in
the direction that the current qValues indicate.

But now, what about percpetion learning? w updates should also be
biased to try and qualize outputs? Should we sacrifice qLearning to
force balance (in the force). No. But, if qValues are roughly qualy,
then w learning should seek output balance I guess? Or seek 50/50
probablity sorting? If they seek 50/50 path sorting, then the system
has no percpetion learning in the w values. But if they seek output
balance then we get correlation based adjustments happening in w values.

How to do both? If qValues are equal, seek output balance? Ah, fuck
no, that would be wrong. Becuase w values would drift to some ratio of
dimishing qValue returns, and we want it to stay right at that ratio of
optimal qValue returns when qValues go to zero, and not start to drift
back to equal balance!

But if qValues are random noise due to the complexity of the environment,
(aka there is nothing this node can learn) would it not allow w values
to randomly drift into strange nonense configurations? If qValues are
random noise, we want to create maximum exploration. But does that
mean, maximum exploration of the w learning space, or maximism use of
each behavior? The idea of random drift in the learning space seems
not unreasuable. It could be looking for a needle in a haystack. Ah,
but what if a single setting of the w values return? Like .5 .5  2 is
the needle in a haystack, and creates tons of rewards for the 2 path,
so we sort more to 2, and lose the optimal point.

So this seems to be a point to ponder. Do the nodes need to seek optimal
configuration, or do they only need to learn "what is best" and lock in
their behavior to that path?

All the talk above about using qValues to adjust w values, I think was
assuming w values need the power to seek an optimal setting. But now I'm
thinking that's all wrong. The sort is the behavior, not the w setting.
The w setting is really part of the policy that defines how much we
want to USE that behavior vs others. Because with RL rewards incrase
the ODDS of beahviors being used.

yes, ok, so scrap all that crap above about using qvalues to define
change in w values.

So, change policy to w/a (removing q). Q values and their variance define
a target long term balance of use of behaviors for maximising rewards
but also balancing exploration. But w values converge towards that
target so w learning is just qLearning, but slower than even qLearning?
But wait why? If qValues and variance define policy, why not just use
that as policy instead of using w? We can make qLearning slower and
longer term to get the same effect. I guess for effecency, using w for
pulse rounting poicy is faster than calculating new sets of probablities
from qValues? I guess it's ok to code that way for now, but could consider
removing w values in the future (as I had it coded here in the recent path
and then threw all away because I decided perceiton learning was lost.

Ok, so fuck. Going in circles and circles. If qValues determne w,
then we have lost percpetion learning have we not? And if there is
no long term percption learning, how does the net correctly configure
itself into a cat detector? I was just hinking leaving a in the w/a of
the policy would be where the blancing was happening. But in that form,
there is no longet term memeory of what a cat is. And since policy
doesn't shape w (by volume) w is not picking it up the data.

The system must configure itself into a cat detector, even when there
are no rewards at all. Or if we just send it a stream of 0 rewards.
If we had to send it a stream of 0 rewards to force it to become a cat
detector, that would be fine. But this current idea of making w just
seek distribution as defined by qValues fails to do that.

Ok, so lets think from a different direction. What if we had yet
another variable, say b that records balance learning. It either moves
to balance the outputs, or moves to just document the long term balance.
If it documented the long term output blance, how could be used here?
So .2 .3 .5 was the b values to document the balance of output levels
(not link use).

Ok, and we have w values, that say the desired policy is something else,
like .4 .4 .2 maybe.

So, do we follow the damn policy, and ignore everything else or what?

Or can we produce a policy that somehow adjusts for imbalance but yet
still follows the policy?

I really feel the /a or something like it is very important, beuase it
allows the current beahvior of neighbors, to shift my current outputs,
which seems critical to the operation of the system. But what if I
balance HOW MUCH, the neighbors cause me to shift, so as to adjust LONG
TERM balance.

Hey, that's key. The /a is a short term control signal. But the goal
is not short term balance, but long term.

Ok, so again, in the first layers of the netowrk, we can expect for
real world exmaples, that there is little to no qLearning possible. No
correlations between raw sensory signals, and rewards. So when no
Qlearning is possible, percpetion learning should dominate.

And where qLearning becomes strong, it should dominate over percpetion
learning

Which is why, in the past, I've really liked the idea of w being adjusted
for balance, but that qLearning defined the blance point.

So equal balance for qual qValues, and unqual balance for unqual qValues.

But to get corelation learnign working, the it must be output values and
not link volume that must be balanced. And qValues only measure link USE,
not output use.

Well, what if I track output qValues? (making the firing of the node the
"beahvior")?

Each node has a qValue and variance. The output nodes define the
target balance. Can this be defined to not create conflict between
overlaping sets of nodes? That was the problem from before. Of A B C
D are overlaping outpts of X-> A B C, and Y -> B C D, then the ration
of volume of B to C, must the same for X and Y so they don't fight each
other in trying to control activity balance.

The relativfe volume number would have to be computed by looking only
at the one node. So if A was computed as 1, B as 3, C as 2, and D as 4,
the distribution would be 1 3 2 for node X, and 3 2 4 for node Y.

I think maybe that could be done, by comparing each qValue, to the average
Q value for all nodes? For the entire system? So the volume rating would
be as if the choice was being the given node, or the "average" node? If
the avage was a q of 4, and variance of 3, then we compare that to the
q and average for the output to detemine a balance ratio? Normalize so
the average is 1. So we the one in qualty is half the normal, it would
get .5  If it was twice the normal, it would be 2. So now we have these
two nodes, relative to the normal, and end up with a 4:1 balance.

So we use w learning, to balance to the target ratios defined by the
qValues. And that will allow w to learn both Q value, AND percpetion
larning together! Sweet!

And then policy is again w/a with a being a short term ability of the
signals to interact with each other, but which creates negative feedback
to keep it all stable instead of postive feedabck.

The ActivityTrace used for qLearning, might beed to be too slow and long,
for what might be important for /a policy?

Maybe even make /a policy very very short term, like single pulse level?

The short term memory that defines the a in /a, is really must be I think
the systems "state of the environment". How fast it fades controls how
fast state is forgotten when no new evidence for support is supplied.
How fast that memory of the rabit, fades, when we can no longer see
the rabit? Yeah, the /a is what makes the system change it's behavior
in response to short term memory of the rabbit, so it needs to be a slow
fade effect.

Butit's so weird that it's an inverts effect. Am I sure that's valid?
When we have a meory of "rabbit" we need to do "not rabit" more? If the
ndoes are cat and dog, as we become more sure of CAT, the system tries
to do "more dog" in response?

Wait, as cool as /a is, maybe it's not needed at all? No, the policy
needs to shift dynmaiclly on the short term with shifting state.
Excatl becuse we want to act differently for dog vs cat. Hey, shit,
no, how the system acts is controlld by what CAT is wired to, not by
how the perception system adjusts in repsonse to "thinking CAT".

Shit, what if we remove /a. How does it all work. It's just a probablity
sort by w at that point! So if i'm the ANIMAL-TAIL node, w defines a
probility sort to three higher level nodes, which could be seen as CAT
DOG and FOX. And a(CAT) tells us the systems curent belief about CAT,
vs a(DOG) vs a(FOX). So I feel beasian logic at work here. What's the
real formula to use? I feel it must be wa not w/a. And that creates
postive feedback.

Postive feedback problem. My w is 2 2 1 to balance flow. I send more
to 2, a() increases, I send more and more to a.

Ah, but what if I factor out MY activity, when using w*a? If my acitity
is all there is, the w*a results in the w balance.

wait does this then become (wOa/La)  That is w times Output a level,
divided Link A level? Ah, no, these are pulses per second numbers.
If My is 10 and output is 12, I don't want to divide, I want to subtract.
So (w(Oa-La)).

So interaction from other nodes, shifts my behavior, but not my OWN
interaction does not, so no postive feedback from my own sorting
decisions. Dman, this sounds like it could work.

So, if my distriution is 1 5 5 to A B C, and my neighbor activtes A,
I send stuff, but not much.

ah minor problem with activiety is zero. Divide by zero. I guess they
all need to start non zero to indicate even distribution of likelyhood.
Not an issue. AT never fades to zero in theory. Might have to watch
for divide by zero.

Ah, but O() values are not normalized. if w are not normalised, we can't
do w*O() without normalizing first because we would not be creating the
correct propiblity I don't think.

if we keep them sum to 1, they form a valid probablity distibution and
then we can mulitpy by adjusted O() to create a valid output stocastic
probablity distribution. If we don't, then we must normalize at least
one, before doing the product.

Ok, so time for major recode. ************************************

Keep QValue calcuation the same. Q values seek the level required to
push qMean enough so all expected future discounted TD errors sum to zero.
They aren't real reard values (they are larger then areal rewards values),
but they have the same relative meaning. Large value, better.

Policy is stotastic random sort over w * (NodeActiv5ty - LinkActivyt)
over each link option where w is a probablity distribution (sums to one)
and Activity is just the discounted pules per second value.

Q Values define a flow volume, relative to some concept of average QValues
and average variance. Need to think though how that is defined. So then
each node, has it's own calcuated "flow volume" factor.

W learning, does flow blancing, with the node flow volume values defining
the targer to seek to (instead of equal blance). So looking at node
activity levels, w is increased and decrased as needed, to move the
activity towards the flow volume targets defined by Qlearning. W must
remain a normlaized propiblity distribution and sum to one.

Thats it!


2016-02-02 12:36 AM Tuesday

Got the recode done. Mostly strating to work. But I'm having a hell
of a time figuring out how to map from qValues and variance values
to qFlowFactors. Keep trying different variations but it struggle
between keeping the flow factors too equal, so the net is not learning,
or blowing up.

I wonder, if there's an issue with a learning feedback problem where
qValues suddenly get large enough to cause enough change in flowFactors,
that then causes the policy to change faster than the qLearning can
keep up and creates some type of postive feedback effect abnd blows up
(all before I see the next display cycle?)  Who knows. Need more careful
testing and debugging and I'm getting burded out for the night.

================================================================================

2016-02-02 1:22 PM Tuesday

Here's the last run I left runnnng overnight:

------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
MyNetwork 7 x 2  time: 2290:00:08.7660
  .    1   . qValue is -0.0000 qVar is  0.00000000   .  33  33  33| 30o  84  80 qValue is  0.7975 qVar is  0.00000004   .|    292 ============
 56o 103 106 qValue is  1.0627 qVar is  0.00000016   .  52   2  46|  0o   1 -47 qValue is -0.4727 qVar is  0.00000000   .|      5 =
  .    1   . qValue is -0.0000 qVar is  0.00000000   .  33  33  33| 26o  73  74 qValue is  0.7379 qVar is  0.00000003   .|    255 ===========
  .    1   . qValue is -0.0000 qVar is  0.00000000   .  33  33  33|  .    1   . qValue is -0.0000 qVar is  0.00000000   .| 
  .    1   . qValue is -0.0000 qVar is  0.00000000   .  33  33  33|  9o  17  40 qValue is  0.3994 qVar is  0.00000000   .|     87 ====
 54o 391 150 qValue is  1.4967 qVar is  0.00000016   .  17  69  15| 36o  70  72 qValue is  0.7207 qVar is  0.00000007   .|    381 ================
  .    1   . qValue is -0.0000 qVar is  0.00000000   .  33  33  33|  8o  15  38 qValue is  0.3765 qVar is  0.00000000   .|     82 ====
+--------------------+
|:.........B.........|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 8244008

--------------
avgRewards   0.9801
qMean:       0.9827
qMeanSD:     0.0001093531
qVar*F:      0.0000  qVar:   0.1204  qVarFactor:  0.0
estimated:   0.9827
reward:     -1.0000  TD error:-1.9827   MSE: 0.0337
--------------
avgRewards   0.9802
qMean:       0.9826
qMeanSD:     0.0001093164
qVar*F:      0.0000  qVar:   0.1203  qVarFactor:  0.0
estimated:   0.9826
reward:      1.0000  TD error: 0.0174   MSE: 0.0337

------------------------------------------------------------------------------------
------------------------------------------------------------------------------------

It seems to have stabalized at about the values seen above. This is
just a single layer network, with two pulse inputs on rows 1 and 5
(0 based). It's got a fan out of 3, and if the top node sends a pulse
to the middle path, it gets a reward of -1 and if the bottom node, sends
a pulse straight it gets a reward of +1. Others paths cause no rewards.
Code left over from the hall problem is alternating the pulse rates of the
two inputs where the top is fast for a while (100 per second I think),
while the bottom is slow (10 per second I think), then it flips with
the bottom fast, the top slow. So averge pulse rate as seen above is
around 55 pulses per second. All it has to do, is for the top to learn
to send nothing out the middle, and the bottom to learn to send all out
the middle.

It learns the advantage of the correct paths very quickly, but doesn't
take advangate of it because of how qValues get mapped to the flowFactor.
The bottom node has output qValues of 40, 72, 38 (actually .40 .72 etc).
But it's flow path is 17, 70, 15, which if we normalize is 17/(17+70+15)
= 17%, 69%, 15%. And it's stabalized at this point, so it won't get
any better sortting than that with the current way the code works.
But we see the avgRewards are .9802. Which is not bad at all, becuase
the best this problem can do, is get all +1 rewoards making the average
rewards 1.00. So .98 is not bad. But cleary, it can do better because
there's no point in sending 17% and 15% down paths that give no reward,
when it could have chosen the path that gives a +1 reward. The two
no reward paths are not bad becuase they don't lead to a lower  reward
but they are bad, only becuase it delays the amount of time the sytem
will get the next reward -- which the syst4em is able to measure, as
the lower qValue. We just aren't takign full advntage of this becasue
of the sub optimal mapping of qValues and variances, to flowFactors.

The currious problem is that the bad paths, are learned to be very bad
at first. But qValues are measures of expected long term rewards, not
just "the next bad thing that happens". So as the network learns not
to do the bad things, the harm of taking the bad with just once, is not
very bad at all, because the next 100 rewards are all +1. So when the net
starts, and behavior is all random, the average reward here is around 0.
So a good path, is a +1 reward, followed by rewards that average to 0.
And the bad path is a -1 reward, followed by reards that averaget to 0.
The bad and good end up being very obvious.

But as the policy imporves to learn to avoid the bad and use the good,
then the avreage rewards creaps up to +1. Then the good path, becomes +1
followed by long term rewards of +1. The Bad path, becomes -1, followed
by long term rewards of +1. And the "do nohitng" paths, become just
the long term average of +1.

So it has learned that the -1 path is very bad, and learned not to use
it much at all. But the difference between the do nothing paths and the
+1 path, are not so great anymore, so it's failing to collect that last
little bit of improvement that is possible, by using the +1 path more.
But that's why it's got such a strong difference on that top node of the
bad path, and not as strong learning on the bottom node, so that's it's
using the do-nothing paths far too much still.

But, I've had insights this morning about how to make this all work
better.

The issue is all about these normal distribution bell curves and how
fast the qvalues are shifting around during learning. Small varriances
mean the qValues are very narrow bell curves and have very little overlap.
With very little overlap, the flowFactors should be very large and
strongly bias pulses to the larger qValue node. We can plot all the
node qValues as bell curves on the same graph, ranging from the one
with the lowest value on the left to the highest value on the right.

It makes no differnce what the absolute numbers are for the qvalues.
They can be 1 to 100, or -100 to +100 or .0001 to .0002. What determines
how much flow should be bised to the higher values is the varrances.
But becaue making percpetion learning works requires we create on
flowFactor for each node, and not make the flows relative to two given
qValues I must create an algorithm that numbers this whole forrest of
bell curves from lowest to highest. If they are narrow bell curves that
don't overlap, the distance between them must be very large, if there
is lots of overlap, then the distance must be small.

But what I realized, is that the width of the bell curves is for the most
part, determined by TD error updates, that are common to all of them.
So I can probabbly get away with assuming they all share a common shape
bell curve, and using that to determine how far to space them out in
the flowFactor numbering. And I can find the min and max qValue every
time I scan to nodes to do reward updates. Knowing min and max qValues,
and knowing the assumed variance they all share, and knowing how many
nodes there are total in the network, I can directly calculate the full
range of factors from the qValues.

So, if there are 100 nodes, and they range from 0 to 10 in qValues, I can
assume the averge spacing is 100/(10-0) or .1 apart in the qValue scale.
And if the variance indicates a spcing of .1 of these bell curves justify
a flowFactor ratio of 1:10, then I can space the flowFactors out across
the 100 nods, so that each node is 10x larger than the previous one.
And since I have 100 nodes, if the lowest starts out as a flow factor
of 1.0, then the lartest will needs to be 10^100. So then I can assign
flowFactors to each node, based stricly on it's qValue. and where it falls
in that 0 to 10 qValue range, that has to be mapped to flow factors from
1, to 10^100.

And what is obvious at this point, is that I must change flow factors to
operate on a log scale. So it becomes 0 to 100, instead of 1 to 10^100,
and the mapping is then trivalle simple linear mapping from qValue to
flow factor.

The possible error however, is that fact the qValues don't have all
the same variance. They are biased by volume. So a node with 10x the
flow volume, will have 10x the varriance, due to how TD error updates
work. Or actually, I think, 10x the standard deviation. (100x the
varriance). The the nodes with the most volume, will be the high value
qValues are they not?  (maybe not strictly true? low volume part of
the network might have a golden nuget n the middle?) Anyway, the trend
will be for the high qValue nodes to carry more volume, so in my graph
the ndoes to the right (high qValues), will have fatter bell curves.
And the fatter bell curves will mean they overlap more and shouldn't be
spaced as far apart on the flowFactor scale. So this seems to me to be
something that can be generall accounted for, in the mapping from qValue
to log(flowFactors) with a non-linearity is requred..

And in fact, that non-linearity is dermined by looking at activity levels
of high qValue nodes and creating the right mapping. So maybe this can
be measured, and factored in. But that seems obsessive at the moment.
For now, I need to try coding the simpler solution of just using cretaing
a linear mapping from qValues to log(flowFactors) and try that. This
should allow the system to far faster take advntage of what has learned.

There's a delicate balancing problem here that as the system is first
learning (far away from optimal) qVlaues are changing fast and variance
is high, making lots of overlap in the bell curves and lots of random
behaivor in the network. But as the qValues converge on a solution,
variance falls, qVaues become more stable, and paths are strenghthened.
This is the explore exploite tradeoff system. As it starts to explite
more, the policy will shift. But as the policy shifts, qValues adjust
to reflect the new policy. If we shift towards exploting too fast,
the system won't update the "bad" qValues fast enough so they won't
have accurate qValues so if it colapse onto a solution too fast, it
won't be able to undestant the full truth of the weaker paths. But I'm
now seeing this is the same issue as above. The fact that if we adjust
varriance mapping to fit the high end with fat bell curves, then the
low end nodes with narrow bell curves, will be be falsely separated --
the better bvalues on the low end, may not get used. But if we adjust
to work better at the low end, the good ones, will remain falsly random.

I guess, the best way to eerror, is on to make it work for the high end,
which is the most valuable stuff, and not worry about whether the low
end fine stuff is incorrect. But the best way, is to adjust so all ends
work well by using a slightly non-linear mapping that will require more
careful study to try and undrestand how to do.

So again, ignore that for now, just try the linear mapping, and covert
flowFactos to log(flowFactor) and see how that works! I'm psyched!
This should work. [2:36 PM]

[8:08 PM] Got code working, been debugging and playing with it.
Had a few bugs to get out and a few things to learn about the right
way to do things. Had a learning bias in the W learning that kicked
in when it hit my hard coded .01 .99 limits. Had to remove that and
switch to making it re-normalize all the w values when needed. Also,
when converting log Flow to absolute, the scale is often too large to
be possible. Like three logFlowFactors that indicte a A:B is 10^1000 and
B:C is 10^10 where A is clearly real bad, but B and C are both really
good compared to A, but B sucks compared to C. but floating numbers
can't represent values that range 10^1000 so I had to cap these when I
converted from log to real numbers. I assiend the worse a low number,
and then capped the high once. But that doesn't work, becuase it makes
B and C equaly good, and A reallyh bad. But B is bad also. Realized I
had to do from the top down, where I assign a good number to C, and
cap the smaller numbers making A and B really bad. Tried re-scaling
but that messed things up becuase it made B look "OK" relative to C,
or at least say 1/2 as good a C, which wasn't true at all. Took a few
times to figure out how to write that cap code correctly.

So, it's mostly working, realy nicely. And the code is clean and simple
this way. No need to use a gaussian() fuction at all! But there's an
issue with superstition. When two things that should be equaly good,
have a slightly better q value for one, and that casuses more flow,
which then is somehow allowing that q values to get incrfeasinly better
than the other, shifting all pulse flow to the randomly choosen "better"
path, when it should have found the two equal, and balanced the flow to
both of them.

Don't know the full answer to why the system is doing this.

Could be a simple side effect of the math I'm using that I don't
realise. Could be something really obscure, like rounding errors
accumulating. Could be just a bug in my code creating a bias that I'm
unware of.

But, what I think is happening, is that very slowly, over time the qValues
get better and better. Which means the bad ones get smaller, and the good
ones get larger. But any node with higher activity, will learn faster.
So the node with higher activity, is converging to the ultimate "good"
value faster. And in so doing, It's making the system think it needs to
use only the q value that is in the lead in the race to convergence game.
And, even with measures of variance, the one in the lead, is CLEATLY
in the lead, and as such, the system decides it must favor the one that
is clearly in the lead. But, once the system is mostly converged, this
value iwll be too high, and the other too small. So the converges should
be say .5 and .5, but the one in the lead made it to .6 and the other ot
.4, now the system is stable, but the .4 now has to slowly take from the
.6 and drive it down and build the other wone up. And with the the fact
that .4 is hardly being used anymore, the learning will be amazingly slow.

So, I'm now letting a test run, to see how the qValues converge over time.
But it might take DAYS to converge. Or weeks. But I first, need to be
sure it can converge out of this "superstitious" belive that A is better
than B when it's not in fact. But even if it can, I have to wonder
if there is a clean fix here to reduce the superstition.

I first had the mapping of q to flowfactors at 3 standard deviations
to produce a 1:10 flow rato. But I've pushed that to 100 standard devitions
to create 1:10 flow ratios to see if that would fix this superstition
and it does not. It forces the system to wait a little longer before
committing, which allows the two balanced values to get w3ell balanced,
but once the q values are large enough to trigger the system making
a decision on what to favor, it the falls off balance, and becomes
superstiouts randomly about one side. Need more testing to confirm
it is just random superstious and beahvior and if it can self correct
itself with time.

Still haven't tested the perception learning AT ALL yet of this new
appraoch. This simple test only shows in deatil how well RL is working!
And it's working very nicely! But I don't want to move on until I'm
sure there are no stupid bugs or invlaid ideas coded into the system.

So, I'm going to run a few of these in parallel, and see if they randomly
fall to different superstitions, and if they can all self correct or not.
Them time for from TV (still have 70 mythbusters to watch). [8:33 PM]

[8:37 PM]  Here's a thought. My qValues are not actual reward
estimates. They are this odd "value needed to push qMean by to make it
align with reward estimates". This means all the qValues are pushing
each other around, which is what is leading to this instablity problem.
If I could chnage how qValues are calculated and make them actual
estimated long term rewards, so these things hould be converting to the
.99 average reward number of the system, then they won't be fighting
each other so much. Yes, I was always in doubt if this Mean technique
for calcuating qValues I'm using wasn't bogus. I looked at a differen
toption for a second yesterday or recently, but saw instant gross
instablity and gave up. Late, I'll dig into that. But I think that
might the answer.

Oh, shit, another thought. I'm not doing the secondary reinforcement
updates in this test. That might be an imporant aspect of forcing the
system to be stable? Will need to test that first. This is realted
to making qValues be actualy real reward estimates, becuse though
back probigating those numbers is how qValues become real reward
estimates. Later. Got to let the current one run first. [8:43 PM]

[10:59 PM]  TV break. Then I had a breakthought! I'm not doing the
updates correctly. That's why my qValues aren't real value Estimates!
I need to subtract ot the reward value from the qMean! Fuck. That's
what's been wrong. I tried qMean = qMean-reward. That didn't work.
Then thinking a little more carefull, I ralizsed I needed to average in
the rewward. So qMean.updageAvg(-reward) was the ticket! So, if the
qValue right before the reward was averaged in as +1, then averting in
-1 right after, returns the qMean to the previous pre-prediction state!

Testing to now to see if it works.

[4:06 AM next morning] Been playing with this for hours. Explored
different code for replacing qMean use as estimation, but threw it all
out in the end.  did keep qMeand(-reward) change, but I don't think that
actually does anything important other than just the qValues.

But I'm still ver confused about what the qValues really are and how
their values end up being distributed across mlutiple qValues. Like the
input qValue is .49... vs downstream nodes on fan out are .13 .22 .13
which seems to basically sum to the .49. The input is 3x more active.
Why does that make it's value 3 times more than the others?

So I don't understand my qValues. I don't really undrestand why qmean
is "right" or how it might be somethign else, but it seems more right
than eveything else I explored tonight. And I don't understand why the
two equal output paths don't equalize, and one is always selected over
the other. There has to be something creating postive feedback between
more activity, and higher qValues. The very long term test I've had
running since last night is showing no long term trend to stabalize.
So this isn't just an issue of the system needing time to stablzie to
make them balance.

Also, Turned on sumQ2 to make it minimize qValues. That's an overfitting
fix.  if the factor is large enough, it sort of fixes the balance.
But not really -- the system is really trying hard to pull them out of
blance and send more to one side vs the other. If I set it high enough
to keep it in blance, it's also too high for the system to be able to
fully learn.

Very tired and burded out after watching numbes flash for hours on end
and trying tons of different tweeks and experiments but still can't make
qValues predict averge expected rewards (or duh, I don't want it to -- I
need it to predict discounted rewards so it can detect that closer spaced
rewards are better!)  Ok, so qValues are estimated future discounted
rewards and the values are odd becuase the future discounting is odd.

Ok, so overfitting isn't even strong enough to fix this imbalance.
I don't see any reasons, why these two qValues don't seek the same value.
Even if they are used at different levels, they lead to identical
paths and futures, and as shuch, those identical paths should back prop
identical updates except for the volume factor whic should only make it
learn slower, not learn a different number.

I've got this food switching back and forth effect causing the two nodes
to toggle frequency. Wonder if that could set up some odd probablity
about the future creating an imblance? I can't fix this if I can't
figure out what's causing the postive feedback problem.

Turned off the sumQ2 stuff so no attempt at qValue mimimizing.

Values are 1.8 for input qValues, and .94, .16 .70 for 3 outputs where
the middle is -1. The three are a perfect sum, to all decimeal places
of 1.8083 of the input node. Why the fuck is this a sum like this?
Why aren't they esimated future discounted rewards? If the three are
.15 to .95, why isn't the one before, a weighted averge and end up in
the middle of these? Is it forced to spread the qValues over each step
of the net for some reason? So a total of 1.8 in the first step and 1.8
in the second? Do I have a formula wrong somewhere?

Well, switched from using ActivityAvg to calculate qMean to using
ActivityTrace. The two good paths aren't balanced, but they haven't
locked hard onto any imblance. They are running like 40 60 at the moment.
Tryed turning on SumQ now to see if balacnes better. Turned it out
balcned worse. Maybe sign issue? Now, value of 1000 seemed to work.
Could mostly learn. Was using 20 before. Just tried 10 and it cuased
it to lock? Trying 20 now. Factor of 500 now. Seems very strong.
But it's locked into best w values possible so it looks good. Blance of
two equal paths are 58 41 on w values. Q values for the three are .0167
.0051 .0162 ish.  will let it run over night and see it if drifts to
less stable.

The other one I left running with no sumQ2Factor has driffed to 78 1 21
balance of w values. Never "fell over" but is doing what feels like a
drunk walk so nothing is motivating it to keep those two values close.

Still q values of one input, is the sum of the three fan-out qvalues. I
have no clue why it works like that. Need lots more experimentiong
to find out WTF is going on here. But at least, the network as coded
does RL learning very well. Just have this iussue that two equal paths
don't get shared and I fear this is a deeper problem. Hum, what if I
made them ever so slightly unqual?  .001 rewards or the like?

And I've not played with the secondary reinforcment stuff to see how
that changes anything.

Could try one input pulse only as a stead stream to see if anything
changes. Could try that with three laywer network.

So much to test.

And code needs a clean up.

[5:24 AM 2016-02-03 Wednesday] Time to go get some sleep before the sun
comes up. Phil expects me at BGOP forge in a few hours!

================================================================================

2016-02-03 11:37 AM Wednesday

New day. Got to run to forge. I'm late.

Both runs left from last night driffted towards full unequal balance
on the two path. The SumQ2 factor to reduce qValue size didn't
stop it! Odd.

Just turned on secondary reinforcement updates for each display to
see how the system reacts.

Thought of the mornining. The td update must use the correct partial
derivative of the error formula or else it won't converge correctly.
I changed from Avg to Trace (sum) without adjusting anything about
the update formula. Is it still correct? Was it correct? Need to
look carefully at that again to see if it's putting a bias in the
system.

I'm starting to grasp that system works by the sum of qValues predicting
discounted future rewards and if you you can raise one qValue it's
harmless if there is another qvalue that can be lowed at the same time.
If the system has multiple solutions to get the same max rewards,
of course there's an overfitting problem -- meaning there's no preference
for sending pulses out one path vs the other for truely equal paths.
So any slight small bias towards giving more active nodes higher
Q values will always call the sytem to "fall" on way or the other.

But if they are equal, that error should be offset to prevent overfitting.
And if the SumQ2 doesn't do it, wihtout messing up convergence, maybe
a little tweek in the wUpdate is a way to do it to bias it slightly
towards equal outputs? Or in the policy?

Something like, when flow factors are close, their effect has less power,
when they are far apart? A slightly non linear mapping from flow factors
to distribution? But that only seems to me like it would slow down how
fast it fall, and not actually stop it from falling. Ah, we need a tiny
weeak force to bias the probablities towards equal  a +1 sort of thing
for each w value. Lets do that.

[11:59] about to walk out. Maybe I got it? Higher qValue, gets sent
higher flow.  higher flow has larger random distribution. Small flow has
smaller distribution. This mean it's 50% random moves higher is stronger
than the weaker flows 50% random move higher. If S+sd vs W-sd  and W+sd
vs S-sd the strong wins in terms of which one on average looks "better"
to the system? So once it gets just a little ahead, it will always fall
to the first one to get a lead? Need to look more carefully at this. But
the changing standard deviation might be the key to why there's a bias.
The stronger one is in effect pushing the weaker one around since this is
a big q value sum game to corretly estimate future rewards. Q values
don't each estimate future reward, the discounted sum of q values
is estimating fugure discounted rewrads. So they are tied together.
Which is the idea has above about trying to untie them by making them
each estimate future discounted rewareds seperately. But which when I
tried, seemed imposisble or wrong. But it sure the shit doesn't sound
wrong in high level theory. Got to go to forge [10:07]

[12:34] Phil was leaving. I was too late to make to the forge so I'm
back home workig on this stuff.

Ok, so the real problem here is that qValues are NOT estimates of future
discounted rewards. The discounted sum of qValues are. And beuase
they have to "work together", they fight each other for their place
in the sum and when two are equal in terms of true effect on rewards,
there is a tendency to randomly give the win to one rather than share it.
Might be the SD issue I talked about above. Might be something a little
more subtle or complex. BUT, the problem is not that there is this
tiny bias, the problem is that they qValues never should have sumed
in the first place. They need to be indipendent estimates of future
discounted rewards. Then it makes no differen e if the volume of one
is higher than the others, they will seek the same qValue if they are
truely equal and their bell curves will stronly overlap and the system
will keep pulses flowing to both, and though it might wobble back and
forth with on winning for a bit then the other, it should stay balanced.

So, how to fix this... Fuck, just don't sum them. Apply the reward to
each qValue as if the qValue is the reward estimation! Drop the entire
qMean crap. Maybe qMean stuff is needed for secondary reinforment
or soemthing. But the basic reward update should treat EACH qValue as
an indipended predictor! Lets try coding that. [12:43 PM]

[12:55 PM]  Nailed it! So great. Sold as a rock now. The two equal
paths are showing the same qValues to 4 decimal places!

MyNetwork 7 x 2  time: 79:16:01.0560
  .                                   ... ... ...| 27o   673  99 log:    6.7 q:  0.9901|    284 ========
 54o   673  99 log:    6.7 q:  0.9900  50   1  49|  0o     .  36 log:    0.0 q:  0.3612|      6 =
  .                                   ... ... ...| 26o   673  99 log:    6.7 q:  0.9901|    262 ========
  .                                   ... ... ...|  .                                  | 
  .                                   ... ... ...|  0o   238  58 log:    2.4 q:  0.5837|      3 =
 56o   673  99 log:    6.7 q:  0.9900   1  98   1| 55o   673  99 log:    6.7 q:  0.9900|    543 ================
  .                                   ... ... ...|  1o   241  59 log:    2.4 q:  0.5867|      5 =

But those two bad paths on the bottom node are slowly headed up to be
the expected future rewarsd of the bad paths which will be very near
.9900 of the good path, with only a light drop due to time discount
delay of waiting until the next pulse. They are only "bad" in that they
delay the rewards a litte not bad beuase they create negatie rewoards.
So It will be intersting to see how the net balances once those raise up.

NOTE: the need to always explore some is crtical here. Beause due
to sudden and swift change in policy and rewards (created by starting
to do the rigth thing), the old qValues got left far behind and were
highly inaccurate. The best ones converged to the new values far faster.
If we used raw flowFactor values, we would only explore at absurdly slow
rates and take years to learn the truth about the "bad" behaviors. So the
bias to use good behaviors, also becomes a bias to stop testing the bad
behaviors and not now the truth about the behaviors you think are bad.
Suserstion? Trust what works, and don't test anything else?

It is such a great feeling to fix an issue that was bugging me not by
adding more complex kludgy code, but by REMOVEING complexity! So in
the above, the three "good" outputs have now all got the same qValues,
but the wLearning is being very slow to equalize the flows.

MyNetwork 7 x 2  time: 391:42:52.5780
  .                                   ... ... ...| 27o   379  98 log:    3.8 q:  0.9781|    271 ===============
 56o   379  98 log:    3.8 q:  0.9781  50   1  50|  0o     .  75 log:    0.0 q:  0.7453|      2 =
  .                                   ... ... ...| 28o   379  98 log:    3.8 q:  0.9782|    277 ===============
  .                                   ... ... ...|  .                                  | 
  .                                   ... ... ...| 16o   380  98 log:    3.8 q:  0.9783|    169 =========
 54o   379  98 log:    3.8 q:  0.9782  28  44  28| 23o   379  98 log:    3.8 q:  0.9782|    238 =============
  .                                   ... ... ...| 14o   380  98 log:    3.8 q:  0.9783|    145 ========


The two weaker paths will be weaker only due to the amount of extra delay
in getting the pulse like .1 second later which is such a small difference
for a 100 second half life, it's within the noise level of the system,
But, if the activity balances, then rewards will be reduced by a factor
of 3:1. That's very bad. But can this system even detect that?

Shit, qValues are still fucking not right. They are estimating
average rewards, when I need them to estimate a sum of discounted
rewards. With weak discounting, the sum of discounted future
rewareds should be like 500 or something, not .9. And the difference
between these two good values, will be noticible if the qValue
was actually the correct measure of discounted future rewards.

So, as I'm doining this at the moment, the difference is probably
in the numbers, but it's so small, that it's hidden in the noise
level of the signal and the large sdFactor is causing the system
to flood all paths as if they were "equal".

Ok, I can fix this. :)  lets think. I think I need thinking
coffee. :)  how to make it converge to the sum of future
rewards....

Ok, wait. Maybe these numbers are correct. Maybe the difference
is just down at small values and care must be taken to use
them correctly? Maybe my large 100 sdFactor I'm using is
preventing it from taking advantage of this difference.

So here is where it is now:

MyNetwork 7 x 2  time: 589:07:49.8990
  .                                   ... ... ...| 28o   314  97 log:    3.1 q:  0.9716|    271 ===============
 56o   313  97 log:    3.1 q:  0.9714  50   0  50|  0o     .  80 log:    0.0 q:  0.8016|      4 =
  .                                   ... ... ...| 28o   313  97 log:    3.1 q:  0.9715|    276 ===============
  .                                   ... ... ...|  .                                  | 
  .                                   ... ... ...| 17o   314  97 log:    3.1 q:  0.9717|    172 ==========
 53o   313  97 log:    3.1 q:  0.9715  33  35  33| 19o   314  97 log:    3.1 q:  0.9718|    200 ===========
  .                                   ... ... ...| 17o   314  97 log:    3.1 q:  0.9717|    178 ==========
sdFactor 100 (also)


Notice the difference is from .9717 to .9718, and from screen refresh to
refresh, these numbers are jumping around at the .0001 level. So though
the center path is shown as "better" in this step, in others it's less.
Just noticed the w values are 33 33 35. Not suming to zero correctly.
even with rounding that's not possible. I think the small bias I added is
messing up w Values. Need to turn that off to make sure it's really not
needed. (done) But, now, w values are currently being renormalized every
time they are updated. Must be some sort of rounding problem on output.
Would expect 33 33 34 however. Ah, no, I was wrong 32.51 32.51 and 34.8
add up to 1.  and would display as  33 35 33. Scratch that.

But, dispite the fact that maybe with these values there's a difference to
be measured. In this configureiotn, the network is getting 3 times less
rewards than the other, and qValues should be 3 times smaller becaue of
it. It shoudd be a matter of 300 vs 100 and there should be no problem
with the network selecting between 300 and 100. But, even when qValues
are correct, it's a tough problem for the net to solve. Buecause when
it's sending all pulses to the center, the expected value wil be the
300 number. And the pulses at that point, sent to the bad side, will
stll be darn close to 300 in total expected value. The only difference
will be the loss of that one reward that was biased.

Ok, so lets calcuate what the discount will be for that one missed
rewward! The +1 rewards are comming from the bottom node, which have an
average rate ofe 53 pulses per second (from above). I think the true
number might be 55 pps. So the average time to the next rwrad, when
that one is missed, will be that spacing. Or 1/55th of a second. .018867
seconds.  

Checking the code, decay is 2**(-dt/100) currently. Or. .9998692 Oh,
that's small. That's how much the sum would be reudced by! So the sum
of dicountes future rewareds, will be almost a full 1.0 less than the
alternative. So now we must ask what will the sum of discounted future
rewards be? Need to generate pulses at that .018897 rate and find out.
Test code time.

Ok, so here's the numbers when we use discounted rewards of +1 at 55
per secpond. Used ActivityTrace but had to adjust for the fact
it's adjusted to read updates per second, which came out as 54.8..
instead of 55 as it should. I think this is cumlative rounding error
problems preventing it from suming to 55 as it should in theory.

Ok, running at 55 updates per second the numbers are:
7935.3227358 54.8132780061
Now, what do we get when we skip an update
7934.3227358

So, as expected, the sum drops by one when we miss a single reward but
the number is 7935 to start with, so the difference is only a small
1/7934.3227359 or .000126 percent. (well .0126 percent)

This in fact might be no better than what the system is already working
with. Ah, yes, but, the expected future discounted rewards will drop
from 7935 down to 1/3 of that if we don't route correctly. And the
current qValues don't show that. So if a node rourtsd to output A,
and gets 1/3 of the pulses beucase of that, the darn q value should be
1/3 smaller than routing to the different path, that produces 3 times
the expected pulses! And the current system does not do that!

So, yes it might be tough to sovle this corectly, becasue that will
rquire the sytem to understand the difference between 7935 and 7934.
But when two paths don't interfer with each other, it needs
to see a 3:1 q ratio.

Tested half the pulses. And yes, it's half the size as 7934.

BIG THOUGHT!  watching the system sturggle with learning it starts off,
where average expected rewards is 0, because the +1 and -1 rewards balance
out qValues then adjust based on that world. Then it learns to not do
the -1, and quikcly, average expected rewards shifts up to like .99.
Then the q values, that are measuring average future expected rewards,
all have to shift up to .99 or to whatever is right for each. And the
"good" paths that are being used more often, learn faster. So they shift
faster, and leave the bad paths in the dust. This leaves the system
with qValues that are all messed up. And the ones that need to be fixed
the most, are the bad onew, we don't want to use, but we must do lots of
"bad" beahvior, JUST TO FIX THE qValues!

So, if we adjust rewards DYNAMICLY (as I was doing 30 years ago!) to keep
average expected rewards zero, then qValues don't have to all shift, just
because average expected rewards shift! So the sytem needs to re-map
external rewards, to internal rewards, to keep the average expected
rewards zero. Then we don't have to tons and tons of bad beahvior, and
wait years and  years, for the bad qValues, to seek out their correct
new values!

Ok, going to put the other issue asside for now, and just code this
dynamice qValue adjustment feature first. [3:10 PM]

[3:33 PM] BIG FAIL! Sounded good in theory, but what it actually did,
was create learning instablity. The net would learn to do the right
thing. Rewards would shift due to the average reward shifting, q Values,
that were set based on old rewards would be wrong and they would try to
drift to keep up with drifting reward values. But this created a mess
between the speed of the avgReward learning, and the speed of qValue
learning fighting each other and passing in the night.  avgRewaard has to
be very slow, but then it really doesn't fix anything. The qValues end
up having to "chase" the changing rewards due to the averge shfiting.
It just made the job of conveging harder, not easier! So what I was
thinking would make qLearning easier, just made it harder. I think it
wold likel still stablzie on the correct answer, but it would go though
cycles of learnng and unlearning in the process making it oscillate
towards the answer instead of just slowly drifting into it. The problem
is that qValues drift at different rates, so there is no  one "Right"
setting of the speed of drift of the rewardAvge to offset the need
of the qValues having to drift. It just creates chaos and is not the
right solution to the problem that is really an issue qValues drift at
different rates and can't keep up as a group with the enviornment shifts.
Maybe ether's a need here to fix, but this wasn't it! [3:41]

So, back to the problem of qValues not being discounted SUM of future
exected rewards!

Ok, so I don't know the future rewards. I can't calcuate it with a
running average. So I can't compare a given qValue, to what it should
have been. The qValues are the only measure of future rewards I have
to work with. Which is why we do back prop with them.

So the standard trick is to use the reward, and the disconted next state
qValue sumed together as the target. But when I get a reward in this
network, I don't kNOW what the next state is! But, I'm now thinking I
do need to wait for the next pulse to show up, and then use the qVlalue
of that first node as the next state, and then figure out the TD error,
and THEN, update the entire network! Yes, I think I have to do it like
that and I think this is the solution you have been looking for. So,
delay the use of the reward until the next pulse. Going to be be messy
to code. But lets do it. I was thinking along this line yesterday when
I was struggling with recoding how reards worked but it seemed to messy.

Thought:  make up a fake node, that all pulses logically passses through
after output, but only when rewards show up! And treat it like any other
node that gets constantly updated, and it's the referece used for the
prediction? It would become a system wide estimate of future rewards as of
when rewards show up. In effect, it's bassicaly a a type of discounted
future reward measure. That would me allow to process rewards when
they show up instead of waiting for the next pulse. Might be cool. I
think it could work. But might also not be as good as using the first
column of input nodes. That gives us more real data to work with. And
rewards can be queued. So multiple rewards that show up before the next
pulse, just get summed together. Or, a discounted sum based on timing?
Think that's needed. But discount UP or discount DOWN when quing rewards?
if there's a long gap between the next pulse, this could be important. If
it's a short gap, not important at all I think -- we just pretent the
reards were delayed .001 seconds. :)  Ok,ignore the error, and just
sum the rewards and apply them all in one pass after the first node is
identified as the target. Lets code that and see if it works! [4:32 PM]

[5:39 PM] fucking power outage. Melting snow problems or something.
UPS is keeping my computer alive, but not for long. Coded new delayed
reward update sysem. Looking cool. Realized that the qValues will
now seek a value to measure rewareds per second! How cool is that.
So in this test 55 is what the qValue will seek to if I'm right.
It's running. but will not last as long as I want due to power outage.
They say won't come back for hours (wife called). Network does
show problem of throwing all pulses to one of equally good paths
but onver time, the qValue of the bad path should eqalize the good
path and fix it. If this works as expected....  

[6:16 PM] Power back on! Computer never died! Keeping fingers crossed.

Network was not masuring q in rewrds per second. It should have stopped
at around 55 but marged right past that number! Then I realised the
discount I needed to apply was not the discont back to the last pulse, but
instead, the discount back to the last reward! Becuse that's how far back
in time the last qValue update was done! But that didn't work either.
But then I realised to make it mesure in rewards per second, I had to
use the inc factor that the ActivityTrace uses as well! It would have
just marched up th that 7935 number from above. I like rewards per
second better however, so I added in the inc factor and if I did it
correctly then it should be pps snow. Not sure if I did it correctly.
Shit, I think maybe I needed to apply inc to the reward value, not to
the discount! It's running, but the q values are still very small
(less than 1) after a bit of running and not changing much at all.
Let me look at the code and think a bit more.

[6:34 PM] shit, got debug working again, and found I had
secondaryReinforcment updates turned on all this time. That chagnes
everything. Now I have to do more testing. Applied AT.inc adjust
to rewards this time to see if they would seek the correct values.
But since I had secondary reiformrents turned on, it would have been
adding more rewards per second to the sysem and changing everything.

[8:13 PM] took dinner break. Watch mythbusters. Down to 70 on box to
watch. Test run with fast learning is still only up to q values of 10.
Wrote test code to confirm convergence and sure enough it converges on 55
for 55 rewareds per seconds. But it takes a long time and gets slower
as it gets closer. Cool thing happining on the test is that the unused
"bad" routes are catching up in Q value instead of being left in the
dust since the appraoch to valid Q values slows down as it gets closer.

Oh, wait. Q values are all being updated at the same time! So even
the slow values are updating just as often as the fast ones! The only
difference is the activity level changing the relative speed of learning.

Oh, but I get it. The updates are relative to the node they are feeding
back from which is the highest volume input nodes! And it's inching
up as fast as possible, but in effect, pulling the rest of the group
up behind fast. And the limit to how fast the rest of the Q values can
learn, is all tied to how fast those first ones can converge.

Hum, I wonder if it would actually be wiser, to use that idea from above
of feeding all pulses though a single fake node, becuse it's update volume
would be the fastest of all, and it would converge on the average reward
rate of the entire system the fastest! I'll have to experiment later.

This is what it looks like now:

MyNetwork 7 x 2  time: 662:19:48.0090

 pps  5499
|---------|   foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========
MyNetwork 7 x 2  time: 725:52:17.7600
  .                                   ... ... ...| 53o   417 533 log:    4.2 q:  5.3311|    542 ===============
 54o   416 533 log:    4.2 q:  5.3311  98   1   1|  1o     . 533 log:    0.0 q:  5.3304|      4 =
  .                                   ... ... ...|  0o     5 533 log:    0.1 q:  5.3304|      4 =
  .                                   ... ... ...|  .                                  | 
  .                                   ... ... ...|  1o    10 533 log:    0.1 q:  5.3304|      6 =
 56o   433 533 log:    4.3 q:  5.3311   1  98   1| 55o   430 533 log:    4.3 q:  5.3311|    541 ===============
  .                                   ... ... ...|  1o    11 533 log:    0.1 q:  5.3304|      5 =
+--------------------+
|..........B........:|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 2613137

--------------
sdFactor         10
avgRewards   0.9770
qMean:       5.3311
qChangeVar:       0.0000000003
qChangeVarSD:     0.0000176463
reward:      1.0000
--------------
sdFactor         10
avgRewards   0.9770
qMean:       5.3311
qChangeVar:       0.0000000003
qChangeVarSD:     0.0000177469
reward:     -1.0000

It's 500x real time, so that's over an hour of running I guess. And it's only up to 5.3
Q values. And look how close those Q values are. But the log ratios are still a healthy
distance apart. But starting to creep closer.

The big question is what does the network converge to when the values get close? 

Oh, I've got a good idea. I'll hard code the Q value of that fist node at start! That
should jump stat the whole thing to convergence! And I need to fix that avgRewards
to be avg discounted rewards per second on the debug output. Go! [8:30 PM]

Oh shit. It "fell apart"! It lost all it's learning!

MyNetwork 7 x 2  time: 965:00:30.3840
  .                                   ... ... ...| 18o  3841  90 log:   38.4 q:  0.9002|    180 ==============
 56o     .  90 log:    0.0 q:  0.8965  33  34  33| 20o  3701  90 log:   37.0 q:  0.9001|    194 ===============
  .                                   ... ... ...| 19o  3755  90 log:   37.5 q:  0.9001|    176 =============
  .                                   ... ... ...|  .                                  | 
  .                                   ... ... ...| 15o  3720  90 log:   37.2 q:  0.9001|    138 ===========
 53o    12  90 log:    0.1 q:  0.8965  29  34  37| 18o  3879  90 log:   38.8 q:  0.9002|    190 ==============
  .                                   ... ... ...| 20o  3724  90 log:   37.2 q:  0.9001|    224 =================

I didn't see it happen. But was experimenting with trying to set q values
to jump start the learning and I was seeing the same problem there.
I could set q values to force it into the right condition but then as
it tried to adjust all the q values to their true value, the high speed
adjusting causes it to lose it's learning, and reset back to equal
output state, which then dropped the average rewards per second to zero
(equal + and - rewards), which meant it had to drive all the q
values back to zero!

Ok, so this might be a problem with my sdValue and how I have attempted to 
automatically control flow balancing based on variance. Instead of
a graceful move from blanced to hard difference there seems to be
a trip wire the system falls over. When q values get too close, they quikcy
act as "the same" and produce random behavior, but just a little futher
apart, and it's Big on wins.

So what it feels like, is that as the net gets close to converging, the
q values get too close together (even bad paths still produce the same
average pps so the same q value, and then it gets too close, and
trips the "all random" mode in a flash. The long running test above
was an sdFactor of 10. I've been playing with 1 now. 10 might be
too large.

Gee, getting so excited about progress, but each time now, one thing fixed
just opens up another can of worms. Maybe a trivally simple eGreedy
would be better? Just pick the highest and don'w worry about all the
complexiy. Maybe just allowing constant higher level of exploring and
preventing the net from every converging tightly might be a fine
alternative as well. So instead of 1 98 1, 10 80 10 sort of limit.
Just throwing out ideas to think about.

A possible problem with this rps version of the qValues is that a small
change plicy, can creaet a big change in rewards (as going from 50 +1
per second to 50+1 and 50 -1 changing the target and requiring all the
q values to shift to entirelly new values. And as they all shift as an
out of control mob they loose all the delicate programming that took so
long to find.

Hard to know where to start. It takes so long to get the darn thing to
"fall apart". Slow and painful testing.

Is this net doing secondary reinforcement? Nope, Buecuse that requires
qValue upgraxdes outside the scopeof rewards. Without any regulat back
prop going on, it's not happening. That might be part of the problem.

Not only sd might need to be small. q learning speed might be a big factor
as well. Let me try this jump start trick with slow speed and small
sdFactor and see if it no lose it's mind as it seeks accurate qValues.

[10:52 PM]  Really burned out. Didn't get the sleep I needed.

First, pulse rate is NOT 55 per seconds. Frequences are 1 and 5 and the
printout is 5.5 pulses per seconds. So q values should appraoch 5 not 55!

Second, my wLearning was all wrong. Did a bad bad bad rewirte and I
think it's better now.

In fact, that broken wLearning might have been the largest part of the
problems all day! Darn

Need sleep. Need to check it all out with a fresh mind.

[11:26 PM]

New code is working much better with dirty rewrite of w learning and
jump start using 5.5 values instead of 55 is working better. Still
waiting for it to converge. Look like this now:

 pps  5496
|---------|   foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========
MyNetwork 7 x 2  time: 202:33:53.7660
  .                                      ... ... ...| 32o****** 546 log:   78610.7 q:  5.4621|    312 =========
 55o****** 546 log:   78495.4 q:  5.4619  56   2  42|  1o     . 533 log:       0.0 q:  5.3277|      6 =
  .                                      ... ... ...| 22o****** 546 log:   78614.3 q:  5.4621|    232 =======
  .                                      ... ... ...|  .                                     | 
  .                                      ... ... ...|  1o550019 534 log:    5500.2 q:  5.3371|      8 =
 54o****** 546 log:   78512.3 q:  5.4619   1  98   1| 53o****** 546 log:   78513.3 q:  5.4619|    535 ===============
  .                                      ... ... ...|  1o504472 534 log:    5044.7 q:  5.3363|      9 =
+--------------------+
|..........B........:|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 729233

--------------
sdFactor          1
avgRewards   5.2410
qMean:       5.4608
 mode        0.2198
qChangeVarSD:     0.0000017094
reward:      1.0000
--------------
sdFactor          1
avgRewards   5.4160
qMean:       5.4607
 mode        0.0447
qChangeVarSD:     0.0000016811
reward:     -1.0000

the 5.33 numbers re creepign up slowly, and the 5.4619 falling slowly. How close will they get together?
Will it trigger random beavior and make the net go all random and lose all it's programming again?

Might, but it's working much better right now.

MyNetwork 7 x 2  time: 313:33:39.4020
  .                                      ... ... ...| 24o****** 545 log:   25093.7 q:  5.4464|    242 =======
 53o****** 545 log:   24988.6 q:  5.4463  43   1  55|  0o     . 541 log:       0.0 q:  5.4052|      3 =
  .                                      ... ... ...| 29o****** 545 log:   25101.6 q:  5.4464|    305 =========
  .                                      ... ... ...|  .                                     | 
  .                                      ... ... ...|  1o228279 541 log:    2282.8 q:  5.4089|      4 =
 56o****** 545 log:   25006.3 q:  5.4463   1  97   2| 55o****** 545 log:   25007.0 q:  5.4463|    541 ================
  .                                      ... ... ...|  1o202034 541 log:    2020.3 q:  5.4085|      7 =

getting close!

[12:16 AM Thrusday]

It died! Missed it!

------------------------------------------------------------------------------------
 pps  5499
|---------|   foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========
MyNetwork 7 x 2  time: 610:03:38.0430
  .                                      ... ... ...| 20o961004 475 log:    9610.0 q:  4.7546|    209 ==========
 54o     . 473 log:       0.0 q:  4.7297  38  43  19| 23o802735 475 log:    8027.3 q:  4.7505|    238 ===========
  .                                      ... ... ...| 11o623426 475 log:    6234.3 q:  4.7459|    104 =====
  .                                      ... ... ...|  .                                     | 
  .                                      ... ... ...|  4o895893 475 log:    8958.9 q:  4.7529|     43 ==
 56o   655 473 log:       6.5 q:  4.7297   7  34  59| 19o832899 475 log:    8329.0 q:  4.7513|    187 =========
  .                                      ... ... ...| 33o642706 475 log:    6427.1 q:  4.7464|    322 ===============
+--------------------+
|..........B........:|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 2196218

--------------
sdFactor          1
avgRewards  -0.4137
qMean:       4.7400
 mode        5.1537
qChangeVarSD:     0.0000025868
reward:      1.0000
--------------
sdFactor          1
avgRewards  -0.3825
qMean:       4.7400
 mode        5.1225
qChangeVarSD:     0.0000025813
reward:     -1.0000
------------------------------------------------------------------------------------

I don't understand how the time is 610 hours when the last time was
only  313. I went and took a shower but it doesn't feel like i've been
gone that long! I've got three running. Did I copy the wrong one?

Or is there a bug here that makes it jump forward in time, and blow
everything up?

Way too tired to figure this out tonight. Got to go to bed. Will leave
all three running tonight just to see what happens.

It's running 500x real time. It was 45 minutes. That's 375 hours.
So 313 + 373 is 688. Well, that is about right I guess.

Ok, I have to find a faster way to duplicate this tomorrow so I can see
what happens.

Idea I had in the shower.. The real behavior learning and policy is a
encoded in the w values. So as long as we don't mess up the w values,
it's ok for qValues to go a bit crazy at times. So, maybe for one
I need to make w learning super slow. That would allow for some shifting
of the qValues at times as long as it doesn't stay shifted too long.
And I know the current wlearning code is a hack wirtten while I was
asleep -- so that needs serious attention. And of course, with super
slow w learning, testing becomes a long slow drawn out process.

But also, with slow w learning, we should also just abot max our the
resolution of the w real floating point numbers and not go around
truncating them and caping them I'm doing now. So many be a different
format, lke the 0 to infinity version I was using. Have to see
how to fit it in to the design. All for later.

So far, great f*n progress. Q values make sense now. They are highly
stable as long as policy is stable. Need to identify this colapse
problem and find a solution. Probably sould set up a set of very
simple test objects to test and verify the basics of the network. Even
this two node test is advanced. One node, constant stream of pulses
sort of stuff is needed. Different reward tests.

Once basic rewards are working, on to testing percetpion learning and
see if t works, and if it's compatable with this version of rewared
learning, etc. Back and forth, back and forth. I have high hopes
of this design idea for percpetion learning (output balancing) but
it's never been well tested. Night [12:33 AM]

================================================================================

2016-02-04 11:03 AM Thursday

New day. Didn't sleep well becuse I'm so excited about the progress
on this. Maybe I need to cut back on caffine if I'm going to get so
hyped about this that my heat races. I was burnt out last night
and just needed sleep, but couldn't sleep! I was laying in bed
for hours thinking about stuff. Got enough overall.

This morning, I find all 4 of the tests I left running in a valid
learning mode. All however have locked into one side of the two
equal sides. Looks like they have been cycling by converging
too closely on the final soltion, then falling apart, and starting
over.

------------------------------------------------------------------------------------
pps  5499
|---------|   foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========
No jump reall slow w learning l.w -= 0.0000001 7 x 2  time: 5181:04:31.9441
  .                                      ... ... ...| 55o300753 529 log:    3007.5 q:  5.2901|    540 ===============
 56o300633 529 log:    3006.3 q:  5.2901  98   1   1|  0o  1173 528 log:      11.7 q:  5.2849|      4 =
  .                                      ... ... ...|  1o     . 528 log:       0.0 q:  5.2848|      7 =
  .                                      ... ... ...|  .                                     | 
  .                                      ... ... ...|  1o  1375 528 log:      13.8 q:  5.2849|      7 =
 54o302236 529 log:    3022.4 q:  5.2901   1  98   1| 52o302035 529 log:    3020.4 q:  5.2901|    535 ===============
  .                                      ... ... ...|  1o   860 528 log:       8.6 q:  5.2849|     10 =
+--------------------+
|..........B........:|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 18651871

--------------
sdFactor          1
avgRewards   5.2001
qMean:       5.2901
 mode        0.0900
qChangeVarSD:     0.0000017483
reward:      1.0000
--------------
sdFactor          1
avgRewards   5.4206
qMean:       5.2901
 mode       -0.1305
qChangeVarSD:     0.0000017078
reward:     -1.0000
------------------------------------------------------------------------------------

There's the one with slow w learning. All the q numbers are very solid and
I dodn't see any jumping around. But checking moments later, I see the
mnumbers are slowly changing.

For this to lock on to the right answer and not fall apart, the speed of q learning
has to be slow enough, so that the difference in these two qValues is below
the noise created by the learning so that qChangeVarSD drops below the level
that it can not consider the two answers "the same" and not lose it's
mind.

The diference between the two q values (right and wrong) when this converges
is the numbers I calculated yesterda, which is only the decay caused by the
space between the two pulses. Something like 1:7000 difference. I think
the qLearning is too noisy here and it will keep losing it's mind becuase of
it.

I'm going to run to forge to for a little bit. I'll leave this one running.

Need to set up a test just to test for this problem (one node, two outputs
should do it).

other thoughts about this..

This might just be a special class of problem that is very hard for this
network to solve. But it might also be a very special and unsual class
of problem I can create in the lab, but has little to do with the real
world? I'll have to give some thought on whether this class of problem
is unsual.

With more noise and a harder learning problem this sort of converge
on colalpse problem might not apply. It may never come close to converge
on colapse becuase both paths are producig rewards, and the better path
will be obviously better by a far greater margin than .000001.

Adding random rewards to the same test to see what that does might
be interesting type of nosis to test to see what it does.

Hoever, the code needs lots of clean up from the mad work yesterday.

But off to forge now [11:19 AM]

[1:38 PM]  Back from forge visit. Time to work again. To get a better
understanding of this colapse on converge problem, I have to set up better
testign that doens't take hours to reach the problem. So before I do
that it's time for major clean up work. The new RL qlearning is so much
better I need to wipe out all the old code. I need to add features
to help me do long term testing, like allowing a description to be
added to the screen each time I start it. Date and time of when it
started and how long it's been running. Need to turn off real time
running to maximize CPU and speed, but need a display update system
that does something adjust it's display rate to real time so I don't
have to re-turn display refresh. Using CRT controls to clear and draw
more stuff could be good as well. My tests will likely start to
get longer and more complex as stuff works better so I'll need better
tools for doing these longer term tests. My box here has 4 CPUs so
I can run 4 processes at once with full CPU without slowing the others
down. So I'll need to take advantage of that! Time to clean up
[1:43 PM]

[4:19 PM]  Lots of file cleanup work. Delted lots of old comments from
xnet.py and have been cleaning up code and removing old stuff I don't
care about at this point. All saved in RCS for history if needed.
Remanemd this file from "dnet.notes" to "notes"  Getting very tired of
typing "vi dnet.notes".

Quick important thought -- Secondary Reinforcment

Secondary reinforcement happens by back propigation. But we aren't back
proping zero (non) rewards. So we aren't doing any secondary reinforcment
currently. Have thought I neede to reward(0) at times to force it.
Have thought I needed to do reward(avgRewards) to do it correctly. But
now, without qMean in use, I don't have any running measure of what the
net currently "thinks" the current estimated "reward" is. The activity
traces would tell me the answer if I scanned the net and created a
weighted average of all qValues by activity trace. And maybe, there is
a reason to use a different exponential decay to define different "views"
of current estimated rewards? But, here is what I think should be done.
As pulses pass though the net, moving from q=5 to q=4, the net has just
predicted a -1 drop in rewards. This is -1 TD error that should be back
propigated (to all nodes using eligbility trace). Can't do only one
node backup, becuase this represents a delayed reward that could need
to back prop to 10,000 steps back to be correctly learned. Must only
use eligbility traces to back prob. But that's compuationaly expensive.
SOOOO, I should accumulate each of these one-step TD errors as the pulse
passes through the network, and at the time of the NEXT reward update,
I should fold these into the reward value and back prop it all! As the
value passes from one pulse to the next one, rewards have to be correct
discounted (or infalated). Got to figure out what is correct.

So the, we are doing the correct secondary reinforment learning at the
same time, we are doing reward processing!

And, if the system hasn't had rewards lately, the correct way that
that point to do reinforment leanring, is to send in a reward of 0!
Which then gets added to the accumulated TD error, and back propigated!
Know that I have a good undersanding of what qValues are (now that they
have a "correct" value), this I'm sure is the correct thing to do.

It might also have the side efrf3ect of helping the system converge
quicker to accurate qValues? But, I've got more clean up to do befor
I'm reading to code that, and get into testing that.

2016-02-04 4:33 AM (still) Thursday

I'm going to cut all the header comments out of the xnet.py code file
and put them here for history. Just keep some very short chagne history
notes in the code from now on.

================================================================================
+ 2016-04-04 4:36 Begin massive cut and best from xnet.py
================================================================================

    xnet.py - new ideas -- AGI ideas

    2015-05-20 Started this file. Took it from dnet.py to do a rewite for my
	       new ideas.  x was just beuase I couldn't think of a good name.
	       x for like xbin, or experimental arcraft.

	       [delete major comments in history]

	       Old ideas that started on 5-20 was the idea that perception
	       learning should be temporal correlation based ONLY. No need to
	       worry about predicting inputs!  clustering should be by temporal]
	       corrleation, so that action choice, is cluseted by things that
	       tend to happen close together in time. If we learn to do behavior
	       X in response to stinums S1, and S2 is strongly temporlly collreted
	       to S1, then X is a good guess as the behavior for S2 as well! That
	       idea may be all that's needed to understand the goal of perpceiton
	       learning -- sort pulses together, that tend to happen close together
	       in time.

	       [2016-2-4 still thinking the above is correct. In fact, I've
	       decided the same thing again recently, as if it were a new thought.]

    2015-05-29 Heading in new direction now. Was using weights and perception
	       learning in links, and activity tracking and qValues in
	       nodes.  qValues in nodes, would bias the ratio of pulses,
	       by complex weight learning. But now I'm trying something
	       much simpler. Going to try throwing out whe weights from
	       the links, and moving Activity tracking and qValues to
	       links (aka S->A->S Q values) instead of State qValues.
	       Made simple pulse route using qvalues of links alone to
	       create probablity. Worked great for simplistic learning to
	       connect input x to output y, but now need to figure out how
	       to make it do good percpetion learning without weights!
	       AKA Tryign to merge percepitonlearning and RL learning
	       into one optimization target! I've thought in the past
	       this might be possible, but now I'm beginning to grasp
	       how to do it due to the w**2 optimiation term idea from
	       machine learning to keep weights small.

	       [20116-2-4 deleteing old comments. Never made the above
	       work.  percepetion learning has to be stored somewhere,
	       and the qValues need to store RL reward predictions,
	       not percpetion learning. If that could work, percpetion
	       learning would have to created be an internal rewards.
	       That might be possible. But for now, that path is on
	       the back burner; Current research direction is qValues
	       in nodes, link weights that do perception learning by
	       adjusting so as to balance flow and using the qvalues
	       to define flow balance targets.	Weights then end up
	       storing both perception learning and behavior learning.
	       Could in theory turn off qLearning and throw all that out,
	       leaving the system with no long term memory to lock in it's
	       "programing".]


	       [save old comment :]
	       Got two pulses sortting to two diferent ports! That's the first time I've
	       done that!

    2016-01-18 6:01 pm

    		Looking at code for first time in 6 months. Had to study for a while to remember
		what I was doing. Summary. Network uses qValues in links, to estimate
		rewards. Keeps a running average of every qValue pulses pass through to
		as the core foundation (pluse adjustments for minimizing sum q^2 to keep qvalues
		small).

		[2016-02-04 delete lots of good history comments here -- all saved in RCS file.]

		[2016-02-04 Summary. Wrote Simulation class to make testing easy.
		Thought up using w/a for policy sort. The /a is novel. But gave up later.
		Later, tried qw/a to move q learning to policy! 

    2016-01-30 2:34 AM Saturday

    	Write HallTest! Very cool.   [2016-03-04 using all the new run time stuff]

    2016-01-31 5:49 PM Sunday

    	Fixed ActivityTrace to auto set for exact inc to measure in pulses
	per second no matter what halfLife is set to.

	Created ActivityAvg to track qMean and rewardMean for doing correct
	secondary reinforment updates. Added secondaryReinforment method
	that needs to be called periodicly (which rewards the net with the
	average reward value).

	Deleted skew and kurt stats. Just an experiment. No value known.

	Converted qMean, qVar, and avgRewards to use new and correct ActivityAvg

	Changed ActivityAvg from isa to havea to prevent accidential use
	of getV instead of getAvg etc.

	Turned off qVar use. It's wrong, not needed make simple one layer
	deep version of hall (that doens't work) for studying Q learning.
	qLearning works. Qvalues represent forward looking discounted
	estimted future rewards!  qMean, measures discounted past rewards,
	but will never actually be a good "predictor" of individual rewards,
	because it's goal is to predict discounted sums, not individual
	rewards! Dman, I wasn't getting that. I was worred it was doing
	so "bad" ad predicing rewards, but never relaized it's NOT SUPPOSED
	TO! with qVar off, Qvalues are working nicely for very simple tests
	now! But as qValues stablized to true values, w learning turns off
	the effect, so the qLearning doesn't do shit! ha ha. This can't
	work. But I need to think through what's right.

    2016-02-01 5:49 PM Monday

	After deep diving into understanding qValues, I've decided it's
	time for a major rewite of how it all works.  w learning does
	output activity balancing, which is cool due to it's function
	in theory as perception learning. But totally prevent the net
	from actually using, qLearning. Not cool.  qw/a was cool, with
	the /a seeming to do percpetion behavior. But I'm giving up on
	that as well.

	Q learning needs to adjust flows, but currently, qLearning
	is working at the link level, and there has been this conflict
	beween node blance for percpetion larning, and flow balancing for
	link level qLeanring. They all need to work toegher, instead
	of making the system fight itself with different targets. And
	I have an idea that might work to solve that.

	Have to switch back to qLearning working at the node level,
	instead of the Link level. So it learns the "value" of nodes
	"firing" instead of pulses travling over links. (hum, wonder
	if we can still do link level qLearning, but combine inputs to
	define node? where there be in value to that? High resolution
	for qLearning.	But would it just produce the exact same answer
	anyway? Later. For now, just implment node qLearning. Then,
	here's the trick. I have to define flow targets for each node,
	that don't cause conflict based on where the pulse is coming
	from. If A and B are common nodes of a X and Y, then the volume
	ratio that X tries to send to A and B must be the SAME volume
	ratio that Y is trying to create in A and B. They must have
	the same optimizaiton target. And to do this, the idea I have
	is to track varance and mean of every node qValue, and use
	a comparson of variance and mean of any one node, relative to
	some average for the network (not sure how to calcuate that yet).
	(maybe qMean itself?).	Then compare those two to define a relatve
	sort based on bell curve overlap, normalize the average flow to
	1, and assign a number to the other. Then all the indivdiual
	flow targerts are relatge to the same 1 value of the average.
	Those become the relative flow targets for all w learning!

	That makes the system combine percpetion learning and qLearning
	into the w values. Seems cool and correct.

	Policy changes from qw/a to a type of simple bayesian net.
	Or w*(Ao-Al) which is w times the output level, with our link's
	contribution removed so as to remove all self feedback, postive or
	negative!
	
	I think that's it. So lets re-code (6:19pm)

	[2016-02-04 It's only been 4 days, and that seems like a month
	ago! Making such great progress on this new approach. qLearning
	in Nodes, Qvalues define logFlowFactors to define flow balance.
	w values are very slow adjust to seek flow blance defined by
	logFlowFactors. Policy is stostic sort of a*w with self link
	volume removed from q.	Haven't gotten to even test the percpeiton
	larning yet, but the qLearning has been greatly evolved so that
	it's now using the CORRECT bellman equaltions (for first time) and
	seeks values of rewards per second and using ActivityTrace as the
	correct solution to back prop TD errors over the enbtire network.

    2016-02-04 1:46 PM Thursday

    	New day. Current reserach issue is learning colapse
	and reset bug. But need to do major clode clean up
	and delete all the old stuff I no longer am using.
	This file is 2308 lines long. Will delete lots of
	stuff including old comments. Down to 1829 after comment
	clean up.

    END COMMENTS -- search token

================================================================================
+ 2016-04-04 4:36 End massive cut and best from xnet.py
================================================================================

[5:10 PM] Still cleaning up xnet.py  But thouhts on Q learning.

I always get confused on what it is. But SARSA learning does single stop
back prop of the action chosen. Q learning modifies that to do back
prop of the best action, regardless of what action was selected so that
with Q learning, qValues converge faster to the true qValues while policy
continues to explore to make sure the qValues are correct. Exploration
doesn't delay the back prop of greedy policy values. I can't do that here
because I don't explore JUST for checking "bad" qValues. Current policy
I'm experimenting with explores due to activity of neighbors to implement
perception learning. But gee, maybe that's not true. If the qvalues
are not close enough, the only explore I do is forced q value caps that
force 1% exploration. Perception behavior is only happening when qvalues
overlap enough to allow it. So, as coded right now qValues win. Well
gee, then maybe I need to only back prop max Q values when I add
the secondary reinforment learning? talked about above (accumulating
and apply on next reward() call? Something to keep in mind.

[7:09 PM] Still lots of cleanup work.  xnet.py down to 1376 lines was
2308 before cleanup

Added simulation display header showing start and run times and command
line comments to keep these straight.

Moved pps into net show() -- shows updates between shows, not long term.
also shows learning factors now.

Hunted down and removed lots of currently unused vars. More to be
removed.

Time to get back to learning reserach. Cleanup is putting me to sleep.

Will set up ColapseTest now.

[7:47 PM] testing Colapse Test. Watched it colapse!  bad qValues crept
up too close to leading good one, w learning causes the system to start
doing bad things. Average reward fell from 8, to 7, and the good q value,
the one being used the most, started to drop faster than the two bad
q values -- and in no time, that made it look to the system as if it
were the worse option by far, causing the system to shift to using the
bad the most, making it all that more worse, creating a swift and
sudden race to the bottom.

first on the list, is to try and set it up for failure, but set the
qLearning so low, that it doesn't get "too close" so I can find out if
it can in fact accurately calculate the bad and good qValues.

[8:57 PM] went out for burger and gas and intress. Back.

Left really slow qLearning colapse running and it looks like it's
converged onto answer with colapsing becuse q learning as small
enough. For most testing, the top path, whichis the -1 reward
was ondly showing a little better than the middle path. But now
that it's converged, the top is the worse, with the other two
showing better. Which is what I expected.  will let it run to see
how numbers might shift.

------------------------------------------------------------------------------------
XNET Thu Feb  4 19:58:33 2016  RunTime: 0:59:45  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 1833:10:00.0000  PPS  18839
wLearningSpeed: 0.00000100  qLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o log:       1.0 q:  9.4000093|     50 =
 97o log:    6417.5 q:  9.4043051   1   1  98|  .o log:     462.7 q:  9.4003183|     70 =
  .                               ... ... ...| 96o log:    6436.8 q:  9.4043180|   5880 ===============
--------------
avgRewards      9.3558
avgAvgRewards   9.4743
qMean:          9.4043
 mode           0.0485 Happy!
qChangeVarSD:     0.0000006649
reward:         1.0000
--------------
avgRewards      9.2463
avgAvgRewards   9.4699
qMean:          9.4043
 mode           0.1580 Happy!
qChangeVarSD:     0.0000006361
reward:        -1.0000
------------------------------------------------------------------------------------

Key idea. This colapse wouldn't be a problem if I was using e-Greedy path
selection! It only colapses becuse I try to throw it back into percpetion
learning mode when when the q values get close enough together.

My need to do percpetion learning is what is causing the colapse I think!
I will have to do testing with fast qLearnkng, but an eGreedy plolicy hacked
into the system, and see if the qValues jumping around can keep the wLearning
correctly shiftted. Ah, meant, eGreedy w learning, not eGreedy policy!

Ah, just a few minutes later, I see the numbers are still shifting, but
the results are staying the same...
------------------------------------------------------------------------------------
XNET Thu Feb  4 19:58:33 2016  RunTime: 1:05:16  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 2001:50:00.0000  PPS  18919
wLearningSpeed: 0.00000100  qLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  0o log:       1.0 q:  9.4015003|     57 =
 97o log:    6145.0 q:  9.4045539   1   1  98|  1o log:     734.9 q:  9.4018650|     51 =
  .                               ... ... ...| 95o log:    6170.6 q:  9.4045666|   5892 ================
------------------------------------------------------------------------------------
XNET Thu Feb  4 19:58:33 2016  RunTime: 2:10:04  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 4031:00:00.0000  PPS  19111
wLearningSpeed: 0.00000100  qLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o log:       1.0 q:  9.4038616|     53 =
 97o log:    1327.9 q:  9.4046948   1   1  98|  1o log:     752.5 q:  9.4043335|     51 =
  .                               ... ... ...| 95o log:    1349.5 q:  9.4047083|   5896 ================
--------------
avgRewards      9.3591
avgAvgRewards   9.4987
qMean:          9.4047
 mode           0.0456 Happy!
qChangeVarSD:     0.0000006236
reward:         1.0000
--------------
avgRewards      9.2118
avgAvgRewards   9.5139
qMean:          9.4047
 mode           0.1929 Happy!
qChangeVarSD:     0.0000004612
reward:        -1.0000
------------------------------------------------------------------------------------

Need to let this run and see if it's still converging or just drifting around.
Even though this seems to have converged on an asnwer, I can't trust it won't
get a little too much noise, and crash.

Would like to have a better (more stable) avgRewards. to see if it matches the
converged answer.

OH, and BANG! It crashes!
------------------------------------------------------------------------------------
NET Thu Feb  4 19:58:33 2016  RunTime: 2:17:24  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 4249:00:00.0000  PPS  21546
wLearningSpeed: 0.00000100  qLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 26o log:   37994.0 q:  8.4228517|   1460 ========
 97o log:       1.0 q:  8.2502934  23  34  43| 32o log:   38639.7 q:  8.4257844|   1993 ==========
  .                               ... ... ...| 39o log:   36747.4 q:  8.4171898|   2547 =============
------------------------------------------------------------------------------------

Just coded experimental greedy wLearning. Whichever qValue is the largest, has
the w raised, the rest are lowered. No output balancing or perception learning
at work when it's coded like this.

[11:49 PM 2016-4-4]  It seems to have converged, and my gut is telling me that
at least for these slow learning speeds, this is going to be stable with the
Greedy w learning appraoch. Doesn't answer the question about how to do percpetion
learning. So, maybe this type of problem is just a very hard speical case
that only really shows up for a very stable perfect system where the rewards
happen 100% of the time.

So, with my fancy logFlowFactor stuff, and w learning attempting to balance
flows at given ratios, it's hard to not trip over these problems. So options
include droing ideas of "balancing" flows, and turn it into a hard switch
between perception learning and RL. Sounds questionable. Seems like for a lot
lf real world learning it's going to be a problem of balance? I didn't grasp
there could be a need to sepreate to q Values so close together becuase I
didn't grasp the problem that happens at convergence.

Other option, random rewards to add RL noise into system to prevent it from
actualy converging into a close match? or somet other similar trich to prevent
these cases form converging so closely?

OK, need to add secondary reinforcement. That might chagne the behavior. Might
have an important effect on how the large "good" qValue gets to move so much
faster to a new lower convergence point and leave the low volume "Bad" ones behind
totally confusing the system into thinking the bad ones are good! 

Ok, lets first, leave this, and try to code secondary reinforcment and see what
that does!

------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
XNET Thu Feb  4 22:38:53 2016  RunTime: 1:09:36  Real-Time Simulation speed=2000X  [ w greedy experiment ]

Colapse Test 3 x 2  time: 2073:00:00.0000  PPS  17795
wLearningSpeed: 0.00000100  qLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o log:       1.0 q:  9.4019733|     66 =
 97o log:    4374.7 q:  9.4042172   1   1  98|  0o log:     825.9 q:  9.4023966|     58 =
  .                               ... ... ...| 96o log:    4403.9 q:  9.4042322|   5876 ===============
--------------
avgRewards      9.4446   9.4012   9.4052   9.4065
qMean:          9.4042
 mode          -0.0404 Sad!
qChangeVarSD:     0.0000005095
reward:         1.0000
--------------
avgRewards      9.3166   9.4012   9.4052   9.4065
qMean:          9.4042
 mode           0.0876 Happy!
qChangeVarSD:     0.0000005027
reward:        -1.0000

------------------------------------------------------------------------------------
------------------------------------------------------------------------------------

Oh, wow. The Greedy experiment collasped and can't get restarted! Oh,
now, talked too soon, it's going again! And was with extra slow learning
for q and w!

Coded secondary reinforcement with accumulation of TD errors, and its
always zero! But that's because of this net only has one input node so
all loops lead back to where it started and sum to zero! Will not do
any good unless the network has multiple input nodes to act as targets!
And then I think the sum will always be just the difference between
the input nodes the current run starts and stops at! And that sounds
oddly questionable in value. And I still can't grasp how discounting
of these accumulated rewards should work. The eligbilit traces do the
discounting for us if we apply when the reward shows up. If we save
it for the future before we apply it, then we have this odd problem
of applying rewards into the ufture, insteads of only into the past!
And the past nodes will not get as much applyed as it should (so we
could inflate the reward so he correct amount gets applied to the past),
but then the inflated rewards are getting applied to future nodes,
in an oddly inflated form where it shouldn't be applied at all anyway!

I need to think more carefuly about the math at work here but I'm
sensing this is all wrong. I'm sensing that the TD errors have
alrady beem correctl applied and that somehow, this net was already
doing secondary reinforment correctl on it's own! When we update the
input nodes, as we apply a reward into the past, the input nodes change.
And then on the next pass, the value of the intput node is applied
into the past as well -- back proping it. And when input node A is
the target, all the other input nodes are using it as a targer. And
when B is a target A is adjusting towards B. So they are cretaonly
all cross filtering information.  

I just need to think more about a secondary reinforcement exmaple,
and prove to myself that it's actually having an effect, and if not
think though how it SHOULD have an effect on the network..

So, think CAT node in the middle of the network. I get negative rewards
every time I see cats so that qValue goes down. When I don't see
cats, the rest of the net is getting postive rewards. So the rest of
the net has higher values. So then I see a cat, how does the negative
Avalue of teh cat change the rest of the net?

Darn it doesn't, becuase nothing is using that value in back proping.

Ah, so here's an idea. The input nodes, effect evetything now, when
rewards are back probigated. So maybe what is needed, is to back
prop the cat node, back to the first nodes lowwering all of them?

Ah, but no, if new beavior A leads to CAT, new behavior A needs to
be punished even when there are no new real rewards. CAT has to
have it's value sent back in time using eligbility traces. Beavhor
A that needs to be pumished, might be a node to the side of CAT with
not direct path forward or backwards. But A might be very active, right
before CAT showed up, so using eligbilit traces, the CAT bad will
be passed to A. So, with  lots of nodes, and lots of changing activity
levels, one big part of the net might be punished, but not the rest.
Another bit part in th emiddle rewarded. The iput and output nodes,
would have qVlaues near the average. Every time the bad part of the net
is activted, the "bad" should spread to whatever else is acdtivted.
If only more bad is activated, the bad stays in the bad part
of the net. But if part of the good, is activated and part of the
bad, then the two mix, and become more average. The bad lowers
some of the good, but the good raises some of the bad.

So logically, we want every TD single step error, to be back propigated
to the entire eligiblity trace. But not forward! Just backwards!

Tomorow, I have to write out the math and see what happens. Got to stop
now, or else I'll stay up for hours more...

Loving the new cleaned up code from todays work however! [2016-02-05 1:56 AM]

[2:28 AM] Back after shower. Had thoughts in the shower I had to
record! The current setting of all the w values,  defines the policy.
When a reward is received, it should "bend" the policy. But the policy
should be 100% stable after that. When the two paths Qvalues become
equal, that's not a sign to totally change the polcy to a 50:50, flow.
It should be a sign that the current plicy is perfect, and needs
no more changing! The difference in Qvalues should define how the
policy _changes_, it should not be, a defintion of what the policy IS.

So as long a NODE A has a higher qValue than node B, then polic should
shift towards using more A. If Q starts to drop, policy should not
go back to what it was, it should just STOP CHANGING and Freeze. So
in this case that is creating problems, when the two paths euqalize
the policy statys the same as 99:1. If the policy drifts a little
to 98:1, Then the Q value of the bad path should rise, and push
it back down. So, what needs to be tested is not ONE pulse being
sent down a path, but a small policy CHANGE needs to be tested. and
if makes the balance of Qvalues worse, it should push the policy
back to what it was.

Qvalues can not define sorting ratios as I'm doiong now! That's worng!

Q values should define delta changes to policy sorting ratios. I
don't know how to fit this in with percpetion learning, but what
I need to do, is figure out how to RL learning in this type of
network, and get that working correctly. THen, look again at the
percpetion problem and see how to add it in.

Not sure how to implment what I'm saying here. But in effect
real rewards, should propigate to node QValues. Then node Qvalues
should "propigate" into policy w values and "suck" the q value
difference out of the node Qvalues. But how? Here's an idea,
For every change in ratio of sorting flow, take Q value out of
the high node, and put it into the low node(s)? So if the high
node keeps getting more rewards, it will keep forcing the ratio
to shift higher and higher. If it reaches a point where the
policy effect has eeualized the two qValues, then there is
no more shifting needed, and w values sould stay where they are.
Noise makes it go up and down, but if any shift away from that
ratio makes one side hire than the other, it will force a shift
back.

So w values swill seek to equlIe all the network q values. If
it can't equlise it, then it just gets that w value nailed
hard to the one direction. How much it shifts from one to
the other might not be important. That amount won't determine
the converg3ence piont, it will only determine how FAST it moves.

So ok, difference in qValues, cuases the target RATIONS to shift.
target ratios stop shifting whe qValues equlIZe. Then we can
use w values to target that ratio, making them implement both
the percpetoin flow blancing and the qLearning target ratios.

So qVALues measure node values as they do now. But a new
link level values will define the target ratio. Damn, more
complex. Which of the three fan-in links gets the most effect?
Oh, so it must be controlled by the amount of difference!
Yeah, something like seems right. But I'll have to look far
more carefull at this. Try to get an implmention and think
about raminfiation.

But, it's clear that this is the fundamental problem with
the current design that is causing this colapse effect. Just
not sure how to implment it yet! Cool. [2:49 AM]

================================================================================

2016-02-05 9:19 AM Friday

New morning. Excited about diging in to this new insight.

Yeah, it's clear my basic concept of how to structure this has been
lacking.  q values don't show the value of behavior due to the fact the
value changes as we learn not to do bad stuff. So the accumlated learned
information isn't stored in the qValue. It's stored in the system's
current policy -- the w values. The w values combined with the new
updated q values, is what tells us how good or bad a behavior really is.
In this colapse problem, as the q values get close to each other,
the system forgets all it's hard won historic information accumulated
about how bad the -1 behavior has been, and the instant the good q value
drops below the bad, my current system says "Oh, I was wrong all along"
The q values shouldn't change it's mind back in the other direction,
until the new (incorrect q vales) stay that way as long as they had
stayed the other way. The system must accumuate as much evidence for
switching back, as it has accumulated for the good direction.

Which means we have to be more subtle with w values. They can't just peg
at 98 1 and fail to accumulate evidence showing the hours of time spent
with the qood q value being larger than the bad. So something like the
0 to infinity version we were using before for flow balancing where the
product of w values was kept at 1 sounds far better. The problem with
this colapse is not that the real q values have swtiched, but that due to
noise in the environment or noise in th elearning sytem will alow the two
to swtich becuase their real values have become so close together. But
statistically, the good value will be higher than the bad more often
then bad will be higher than the good, so it will keep pushing w in the
right direction forever. Andif things do switch, and the bad beocmes
a little better, the system should require the enviorment to "prove"
the bad is now better, by providing an equaly amount of total reward
seconds of evidence in the other direction.

Ah, yes, we can make w values the log of accumulated evidence! Beautiful.

That's it! That's the right theory! So there will be a learning rate
factor that basically defines the units of the log accumulated evidence
which is like the halfLife and inc variables for the ActivityTrace.
They aren't even all that critcal, but they will match the resolution
and range of the w value precision to the nature of the problem.

But, if we have a fan out of 2, then w are talking about the balance between
the two. So I think that's 1 w value, to accumulate the evidence about
the _difference_ between the two outputs over time. And if the w values
product is one, then that's fitting. One is the inverse of the other
so it's only one varible of accumulated evidence. And if we have a
fan out of 3, we have two degrees of freedom. Sounds right. But how
exactly do we calculate with three? Take median of three q values,
and look at how much each is above or below the median, and that amount
is what is accumulted in the log w value! Buecause what we need to
accumulate is difference not absolutes. That seems right. And the damn
funny thing here, is that is what I was doing before with the output
balancing. And I tried to carry it into the RL code, but didn't get
it right! Switching to 0-1 range q values prevented them from long term
accumulation of information, and the way I applied logFlowFactor
to the biased output levels I think was wrong.

So, lets ignore percpetion learning and just focuse on this RL side
of the problem. It feels like we can make them fit together.

Ah, I'm sensing a rise in the force. :)  An idea comming forth. We
use w values (or some new name) to accumulate Q data blance evidence.
And THAT sets the desired output flow balance target! Then we use
another w value to do flow blancing to THAT target. So, Q values
measure CURRENT evidence. Long term Q value balance is accumulated in
b value (balance -- just made that up), and the w values then do long
term flow blancing towards the b target! Policy is driven by current w
value! Fuck yeah, Seems solid. 

But all this cool logFlowfactor system will be thrown out the window
now. Sad. Cool invention, just wasn't what was needed.

Got to see if it works. But first, morning coffee, and ingress run.
[9:49 AM]

[10:44 AM] Back from startbucks and Manhatten Bagle and ingress. Time to
try coding this idea and see how it works.

[1:22 PM] coded these ideas. Made b values 0 to 100 with a sigmoid like
(though not actually). Testing with 3 output ColapseTest with -1 0 and
+1 rewards. The system learned that +1 was best and 0 second best, and
-1 really bad. But then as average rewards crept up, and qValues tried
to keep, b values saw both 0 and +1 as above the mean, and as such,
decided to use both of those. So output is 1 49 50 in w values or 1
50 49 bouncing around. The issue is that q values is 3.8 but average
rewards are 4.7 ish (half of what they could be if it would stop using
the center path). But at the moment, the q values are running up a
snails pace to 4.7. The fact that the +1 path is twice as good as the
middle is hidden in the noise. But so far, the best path contiues to
have a slightly better Q value. 2.2 vs 3.957 vs 3.958.

I'm adjusting b values based on differences from qMean. So if the true
q values are 1, 4.9 and 5.0, the 4.9 and 5.0 are going to always look
"good" and the 1 "bad". Which means the system is going to seek to
0 100 100 output. That's not what is needed here is it? I needs to
push towards the 5.0 until it's not better. If it stays better, it
should dominate. Now I'm thinking that we need a greedy approach here.
But no, that could lead to colapse again. If average rewards shifts down,
the best one, the one being used the most, will dive downward the fastest.
And keep diving down becuase of the slow response of the policy to change.
But that means the second worse one, will take the lead, and plicy will
keep shifting toward the second worse.

Ok, so the issue is, qLearning just needs to adjust must faster than
policy. So when policy shifts, and q values get out of wack as it adjusts
to the new correct qvalues, they will fix themselves long before policy
has been perminately damaged.  b learning for policy, has to move much
slower than policy shifts.

So to greedy or not greedy? That is the question. If I switch to
greedy b learning, I'm thinking speed of learning should be relative
to the second best qValue. All the others that are lower, should
be ignored, because we really don't give a shit how low they are if
they are never used becuase they are so bad.

Yeah, and I'm waiting for this first test to finish.  41 minutes running
so far. Still using the second two outputs full.

------------------------------------------------------------------------------------
XNET Fri Feb  5 13:01:33 2016  RunTime: 0:41:59  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 1256:10:00.0000  PPS  17917
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o q:  3.4493744|     59 =
 96o q:  4.4973658    . 1000 1000   1  50  49| 48o q:  4.4934771|   2940 ===============
  .                               ... ... ...| 48o q:  4.4944280|   3001 ================

avgRewards      4.4992   4.7295   4.7342   4.7339
------------------------------------------------------------------------------------

So the two are nearing the averge of 4.7  And the worse is creeping up in value.
if the bad one gets high enoughs so that the mean will be above the value of the
second best on, then that will make the system stop using the second best, and push
the averge rewards up to 8.4. But if the system stablizes with the mean below
the value of the second best, it will never switch outof this cofiguration and keep
using both outputs. Yes, it needs to be more greedy. It needs to accumulte the
fact that the thrid output has been consisteny higher than the second for an hour
now.

Make me think greedly learning with produce of b being 1.0 might work here. Just
find the highest q value, push that b up, the other two down. (b *= 1 + speed) vs
(b /= 1 + speed) and see who wins the battle.

Ok, need to leave other one running to see how and if it stablizes. But want
to code this greed b learning and test that now.

[2:28 PM] got greedy b learning coded and running. B values now do 0 to
infinity using b /= 1+speed updates to go down. Last one is calculated
as product of others. Speed is pure greedy. No measure of how greedy
it is relative to second best or waht not. It seems to me, that if this
can work, the distance sholdn't matter. Two q values close together will
trade places randomly and if one is statstically better than the other,
it will be in the lead more often.

Progress of mean b learning from above:

------------------------------------------------------------------------------------
XNET Fri Feb  5 13:01:33 2016  RunTime: 1:35:36  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 2854:30:00.0000  PPS  18200
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  0o q:  4.6312295|     54 =
 97o q:  4.7285383    . 1000 1000   1  49  50| 53o q:  4.7280397|   3036 ================
  .                               ... ... ...| 44o q:  4.7289401|   2910 ===============
--------------
avgRewards      4.3656   4.7322   4.7325   4.7325
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
XNET Fri Feb  5 13:01:33 2016  RunTime: 2:19:00  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 4149:40:00.0000  PPS  17841
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  0o q:  4.7210797|     53 =
 97o q:  4.7338667    . 1000 1000   1  49  50| 49o q:  4.7334067|   3026 ================
  .                               ... ... ...| 47o q:  4.7343079|   2921 ===============
--------------
avgRewards      4.7250   4.7358   4.7340   4.7367
[manually calcuated mean of three q options is 4.72959 -- way below the 4.7334]
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
XNET Fri Feb  5 13:01:33 2016  RunTime: 2:36:34  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 4667:20:00.0000  PPS  17812
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  0o q:  4.7279567|     50 =
 97o q:  4.7340669    . 1000 1000   1  50  49| 46o q:  4.7336060|   2980 ===============
  .                               ... ... ...| 51o q:  4.7344993|   2970 ===============
--------------
avgRewards      5.0476   4.7356   4.7346   4.7354
------------------------------------------------------------------------------------
Wow, the below is getting real close with the bad one. How close will it get?
------------------------------------------------------------------------------------
XNET Fri Feb  5 13:01:33 2016  RunTime: 3:39:11  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 6458:10:00.0000  PPS  17488
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o q:  4.7321849|     62 =
 97o q:  4.7336138    . 1000 1000   1  50  49| 49o q:  4.7332353|   2992 ===============
  .                               ... ... ...| 47o q:  4.7341207|   2946 ===============
--------------
avgRewards      4.5924   4.7186   4.7337   4.7369

------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
[as of 8:43 PM]
XNET Fri Feb  5 13:01:33 2016  RunTime: 7:40:32  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 13137:30:00.0000  PPS  16586
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  0o q:  4.7327356|     63 =
 96o q:  4.7340784    . 1000 1000   1  49  50| 44o q:  4.7336369|   2952 ===============
  .                               ... ... ...| 52o q:  4.7345397|   2985 ===============
--------------
avgRewards      5.2281   4.7346   4.7356   4.7344
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
[as of 8:56 AM Sat morning]
XNET Fri Feb  5 13:01:33 2016  RunTime: 19:54:37  Real-Time Simulation speed=2000X

Colapse Test 3 x 2  time: 34858:00:00.0000  PPS  17822
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o q:  4.7329256|     50 =
 96o q:  4.7345901    . 1000 1000   1  49  50| 46o q:  4.7341567|   2955 ===============
  .                               ... ... ...| 49o q:  4.7350563|   2995 ===============
--------------
avgRewards      4.6989   4.7302   4.7367   4.7362
qMean:          4.7346
 mode           0.0357 Happy!
qChangeVarSD:     0.0000013735
reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                4.7041206120
--------------
avgRewards      4.7616   4.7302   4.7367   4.7362
qMean:          4.7346
 mode          -0.0270 Sad!
qChangeVarSD:     0.0000013214
reward:               -1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                4.6024423281
------------------------------------------------------------------------------------

Idea, I need to put a process ID to make it clear what is what. Could use unix
or just a random number! Oh, no supid. I've already got the start time down to the
second. That's the way to id it.

Looks like the median b learning is stuck. Don't think the bad value
sill creep up another to free it. But I'll keep watching.

Another idea. Random slow rewarding with average rewards might help the system
stay in sync better? Might help the "bad" nodes seek a value near the average
faster? But, for a more complex environment, that's what's going to happen
anyway. This test is a worse case where there is clear and constant
correlation between a node and rewards. Woldn't hurt to test that to see
how it changes behavior. Even if that's not used in the netowrk, it might
be a useful idea to understand when desining reward functions for applications?

The problem with the delays is that I'm using a long half life eligblity
trace system to pick up highly delayed reward correlations, but then
testing the system with rewards that have no delay at all. A worse
fit test. God for finding bugs and imporving the system, but not normal
world use.

Need to check for underflow on b updates. I think they will drop to
zero as it's codded with enough long term updates (which is normal for
a long living process)  Got to keep them from hitting zero and blowing up.

Also, they represent accumulated evidence of which q is best. A q could
be best for 10 years, and then the environemt changes and is no longer the
best. Should I really force the system to experence 10 years of unlarning
to erase all that evidence?  capping how low or high b values can riase
to determins how much evidence they are willing to accumulate and how
long it will take to erase that evidence if things shift. Yet another
learning paramater in the damn system. How many will we have before I'm
done I wonder? Could make this time based -- so the amount of evidence
to accumuate is really time based not pulse based as it is now. Then I
really could cap it a "years" or "hours" or any number I wanted. Aka,
a year of evidence one way would in fat take a full year of evidence in
another way to unlearn. Just thinking.

[3:24PM]  So the mean beased b learning is still running and being nice
and stable, but showing no signs that the bad option will rise up high
enough to make the second best below the mean and allow the system to
learn not to use the second best. That approach is dead. But it seems
stable at least so far.

The greedy learning has latched onto good option and is growing fine.
Can't see why this won't be stable. But it will be good to see how it
tops out.

Started a second greedy learning with rewards chagned to be +1, -1, +1 to
make too equally good options to pick from. It's randoly latched onto
one of the two good, and q values are now growing to the max. Once it
gets up there, the two good values should stat fighting each other,
and that will be intersting to see what results. Neither is any better
than the other, so any policy that mixes those two are equally good.
But if the q values are accurate, the noise should make them randomly jump
ahead at about 50/50 I would think and that should cause the b learning
to shift to 50/50 I think. If there is no volume bias prevening it.
Will be intersting to see what happens. Looking real good so far.

[3:39 PM] Getting tired. Code seems to be working. Just watching these
three very boring processes run. Strees of all the excitement this week
with this code is adding up.

But, thinking ahead to the next step. Percpetion learning.

I've got w values flow blancing nodes now. But, this creates the probem
I was trying to fix with this entir eappoch of doing qValues in the
nodes. That's the problem of neighbors fighting each other trying to
set different flow volumes.

So, is this a problem here or not? I'm not sure. So if X feeds A B C and
Y feeds B C D, and X picks B as the dominate them Y can't pick C becuse
whatever the B C q values are doing both X and Y are loking at the same
values and making the same choices realtive to B and C. Either one of
the two dominates, or they are tied. But Y could pick D as a dominate
output, and A could X could pick B. X will try to maximize flow to B.
Y will maxize to D. All tha twill do, is cuase X to try and reduce A, by
not sending ANYTHING to it, and end up sending all it can to D. Nothing
wrong wtih that! If X sends all to A, again, nothing bad as far as
I can tell. B and C just get nothing. I'm not seeing a problem here.
Because flows are caused by the node qValues, I think the links won't
end up fighting each other. Even when operating in a state of sending a
balance of nodes to B and like 2:1, that's becuase when A is small, B:C
bave a blanced q values when the ratio is 2:1. Sort of complex actually.

But it looks like there might not be a problem. Which cold mean this
is going work without any speical complex stuff to make percpeiton
learning work.

Basically, when there's ANY bias on q values, the accumulated b values
will show it, and shift output to the strong one. Where there is no Q
value bias, the output will stay balanced and percpetion effects will
kick in.

It's going to be complex as hell to test all THAT interaction between
percpetion and qLearning at the same time. But, I guess, we turn off
bLearning at any time, and then watch what w learning does for perception
learning.

Cool. In theory. But if it's close, but not right, it's going to be
slow and painful to test. Outch. Well, not going to mess wtih that
until the b learning and RL is tested in more detail first.

[4:38 PM] this is the jump started center test bad test.
Starting to get close to convergence.
------------------------------------------------------------------------------------
XNET Fri Feb  5 16:11:02 2016  RunTime: 0:27:02  Real-Time Simulation speed=2000X  [ jump start with b and q values center bad test ]

Colapse Test 3 x 2  time: 615:40:00.0000  PPS  13561
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 95o q:  9.4648691|   5888 ================
 96o q:  9.4648656 1000    .    .  98   1   1|  1o q:  9.3216108|     61 =
  .                               ... ... ...|  0o q:  9.4278690|     51 =
--------------
avgRewards      9.4497   9.4623   9.4621   9.4619
qMean:          9.4641   9.4156   Happy!
qChangeVarSD:   0.0000005417
------------------------------------------------------------------------------------
[8:46 PM]
------------------------------------------------------------------------------------
XNET Fri Feb  5 16:11:02 2016  RunTime: 4:33:19  Real-Time Simulation speed=2000X  [ jump start with b and q values center bad test ]

Colapse Test 3 x 2  time: 6113:40:00.0000  PPS  13806
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 96o q:  9.5009163|   5884 ================
 97o q:  9.5009062 1000    .    .  98   1   1|  0o q:  9.5002528|     62 =
  .                               ... ... ...|  0o q:  9.5011748|     54 =
--------------
avgRewards      9.5393   9.4971   9.4971   9.4971
qMean:          9.5009   9.4899   Happy!
qChangeVarSD:   0.0000003814
------------------------------------------------------------------------------------
Very cool. The bottom q value is the largest which means the b learning
is happening to bring it into play. But how long might that take? Will it
swap back and forth and never get to the point of sharing the output? If there
was more randomness in the rewards, I suspect the sharing would be more quick
to happen. Oh, so much to test and experiment with to understand what this
algoirthm can do.

[2016-02-05 10:10 PM]

So the first greedy version of b learning crashed after 7 hours with a divide by zero error.
The b value was listed as nan so the others were probably zero. I capped b values on later versions
to prevent just that problem.

The jump start above is still running and at the momemnt the non-selected q is larger so
it should be reduceing the b values. But it's now showing yet. The value is still listed
as 10000. And I can't see any more detail of the real number so I don't know if it's making
progress towards equalizing and using both outputs or not. I'll let it run over night
and see what's changed in the morning. The output is highly stable with only a few of the last
digits changing. I can sense testing the code is going to get harder when it takes 20 hours
of running to test just something as simple as this! Good new however is that all these
are doing what they should in general be doing. Will have to create some more complex
tests and see how the code responses. Retest the things I did before, etc.

-------------------------------------------------------------------------------------------------
XNET Fri Feb  5 16:11:02 2016  RunTime: 6:03:04  Real-Time Simulation speed=2000X  [ jump start with b and q values center bad test ]

Colapse Test 3 x 2  time: 8156:20:00.0000  PPS  14124
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 94o q:  9.5007767|   5883 ================
 97o q:  9.5007662 1000    .    .  98   1   1|  1o q:  9.5001329|     59 =
  .                               ... ... ...|  2o q:  9.5010610|     58 =
--------------
avgRewards      9.5159   9.4981   9.4981   9.4981
qMean:          9.5008   9.4927   Happy!
qChangeVarSD:   0.0000004286
reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                9.5021066584
-------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------
XNET Fri Feb  5 16:11:02 2016  RunTime: 6:10:35  Real-Time Simulation speed=2000X  [ jump start with b and q values center bad test ]

Colapse Test 3 x 2  time: 8327:00:00.0000  PPS  14039
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 48o q:  9.5004253|   2865 ===========
 97o q:  9.5004078  509    .  491  49   1  50|  1o q:  9.4999556|     60 =
  .                               ... ... ...| 47o q:  9.5004140|   3075 ===========
--------------
avgRewards      9.3469   9.4981   9.4981   9.4981
qMean:          9.5004   9.4929   Happy!
qChangeVarSD:   0.0000005895
reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                9.5017507500
-------------------------------------------------------------------------------------------------

WTF! 5 minutes later and look at the second screen? It's gone from 1000 to 509/491! And output
is balanced! Maybe it's been osscilating back and forth?

-------------------------------------------------------------------------------------------------
XNET Fri Feb  5 16:11:02 2016  RunTime: 6:13:05  Real-Time Simulation speed=2000X  [ jump start with b and q values center bad test ]

Colapse Test 3 x 2  time: 8380:40:00.0000  PPS  14056
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 84o q:  9.5004826|   5339 =================
 97o q:  9.5004716  886    .  114  88   1  11|  1o q:  9.4998891|     60 =
  .                               ... ... ...| 11o q:  9.5004996|    601 ==
--------------
avgRewards      9.3741   9.4982   9.4982   9.4982
qMean:          9.5005   9.4929   Happy!
qChangeVarSD:   0.0000006140
reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                9.5018140337
-------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------
[8:51 AM Sat]
XNET Fri Feb  5 16:11:02 2016  RunTime: 16:39:32  Real-Time Simulation speed=2000X  [ jump start with b and q values center bad test ]

Colapse Test 3 x 2  time: 23087:50:00.0000  PPS  14046
qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...|  1o q:  9.5010436|     58 =
 97o q:  9.5011319    .    . 1000   1   1  98|  1o q:  9.5001169|     65 =
  .                               ... ... ...| 94o q:  9.5011413|   5877 ===============
--------------
avgRewards      9.4303   9.4999   9.4999   9.4999
qMean:          9.5011   9.4980   Happy!
qChangeVarSD:   0.0000005653
reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                9.5024697795

-------------------------------------------------------------------------------------------------

Yes, and it's flipping back to the top now. That's sort of cool.
The two paths are in theory identical in q value so it's not wrong that
it's drifing. But to do perception learning the system must remain more
stable. It seems? But it didn't flip all the way, w ratio is 65 35 now.
Now back to 58 43. So it seems to be stayin garound the blance point,
and not locking hard to one side or the other. It might be OK that a
system with such constant and artifical reards would do this. Real life
will make the network far more complex and cahotic.

Sooooo much needs to be played with this new b learning based RL algoritm
to see how it responds in different situations. And tha whole accumulated
TD code is still stuck in there not doing aything in this single node
test but will have effects with more. I should turn that off for now.

Time for shower and bed. [10:34 PM]

================================================================================

2016-02-06 8:51 AM Satruday

Checking the processes running last night. They went back to being all
1 sided on the test with 2 equal outputs. Clearly this algorithm is
finding no reason to balance the outputs. It's not stuck on one side
either however. It just seems to randomly and slowly drift about.

I need a way to collect long term data streams and graph large data
sets so I can see what these long term proceses are doing. I'll have
to track down a solution to that! Writting to a file will be easy but
I have to find a tool for turning that into graphs.

Also updated the screen shot of the run with old b learning code
using the median to determine which to increase and decrease. It
was the -1 0 +1 outputs stuck sending to 0 and +1 so it never got
past 5 rps when it should have be doing around 10. That's history
now. Time to delete that code.

First, for some starbucks, ingress and food. Then I need to do some
code clean up and start exporing how this new code using the b
variables work on different things. I need to get to know this new
beast.

[9:47 AM] Back from starbucks and ingress.

Started a -1 . -1 test. Forgot that would give it only one reward vale and
nothing to differentiate on. But the -1 was to force bad to have to
sink down from the 0 starting q values. It's keeping somewhat balanced
in this test as all the q values sink to -10 rps. But sense the
middle is not creating a different reward it's using it as much as the
outer too. Currently down to -2.5 and balance is 41 27 31. And now
35 38 29. So it's bouncing around alot.

I need to figure out how to speed up this testing. Change half life
to shorter? Space pulses much further apart (less pulses) to have
the same effect as a shorter half life?

On this net:

qLearningSpeed: 0.00000100 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100  sdFactor: 1.0
  .                               ... ... ...| 27o q: -2.6087452|   1765 ============
 96o q: -2.6159103  303  355  342  29  36  35| 36o q: -2.6093801|   2106 ==============
  .                               ... ... ...| 33o q: -2.6097592|   2129 ==============

with all three outputs being used, at 10 pps and only two giving rewards it will
seek to -6.6 q value range (less in practic) before it can learn not to use the
middle one. Let me try to adjust numbers to make it learn that faster.

[11:03 AM] added real time trigger frequency which adjusts simulation
time so as to make the trigger happen at a real time frequencey for
displaing the screen updates. This way, I can change the simlation and
not have to hand-tune the display update frequency.

[11:48 AM] Removed logFlowFactor code. Sad I thought that was so clever.

cleaning up code from crazy rush yesterday. Realized that all the
fancy way I calculate b values is silly. It's 0 to infinity values
now, where the product is 1.0. But all I'm doing is incrementing the
link with the highest q value (greedy b) and decrementing the others.
So all i'm doing, is COUNTING the number of times one link is the
winner vs the others, and I cap when this exponential growth cap reches
1000 or something (complex to calucate how many steps that would be).
Why make it exponeital? Why not just start at zero and sum 1 and minus 1!
The expoential techniqu then maps directly to flow percentage balance.
So if I changed to counting the mapping to flow blance wold be different.
But this really is just a count where one goes up 1 step, and the other
two go down 1/2 step (with a fan out of 3).  could change to make the
winner go up (fanout-1) 2, and the losers go down one. Keep it an int
and never have to worry about rounding errors accumulating. Could cap
the winner, and not change the stats for any of them when the winner
is capped. Maybe. This is all new teritory becuase until yesterday
I never thought about the need for this sort of accumlating evidence
seperate from the q Values.

Well, mostly kinda didn't. Had thought about the issue of relative
rewards vs absolute and realized that the sorting nodes would have to
accumlate some measure of relative difference between it's two binary
sorting options and that these relative difference would not be the same
thing as global qValus that could be back propigated.

But now I'm wondering if there might be a way to drop the calucation of
the actual real global q values and just keep track of relative rewards
for each decision process. Like the sorting node would keep these b
sorting values an and the ratios and not need anything else? Of course,
that's what we are mostly doing now anyway, one q value per node. But when
making a sorting deciions, I'm looking at the three q values I sort TO,
not the one I'm sorting FROM. Hum, not sure. Just thinking...

Also, this issue of the net having to re-adjust all qValues from a
current average of something like 0.0 when it starts, to say 10, which
is the current reward rate for the current policy -- maybe the thing I
tried before might work better now that I have b values -- which is to
adjust all incoming rewards by the rewards long term average so as to
create an internal long term network average of zero.

That might grealy speed up the network re-seeking to new qValues becuase
all it has to do, is figure out the new average around zero. But it
keeps all q values relative to zero in the long run.

So much to think though now that this whole new idea of b values
and accumulated evidence is in play. But for now, certainly keep
the calucation of real qValues, and lets think more about how to best
accumulate evience with the b, and how to map that evidence to a volume
target.

[3:22 PM] Been cleaning up and playing.  b vlues are 0 10 bMax now and
increment by +2 -1.

Add avgReward offseting input reward values. Idea is to try and prevent
all the qValues in the net from having to drift up and down with policy
chagnes etc. This keeps the averge q value near zero so that howeer much
avgRewards drifts, the q values don't all have to drift. Seems like it
might reall speed up convergence.

But, adds yet another moving target into the system to cause feedback
loops and instibliyt.

For exmaple one part of the net.enviroment trains with a an avg reward
of 10. But in another part of the evnionment the reward is 100. If we
chagne the average to 100 then we get back to the 10 part of the world,
the q values are all wrong. They should be like -90 but they might
be near zero. The drifting avgRewards in effect I think may risk not
applying equally to all parts of the net, so the net gets painted with
different avg rewards adjusting the real rewards. Sounds like trouble.
Might help speed up testing where the q values alll start out "wrong"
and take forever to drift to real values? But in real life, I think it
spells risk by hiding the "Truth" of the world from the network?

So, when outputs are equal, this greedy b code doesn't balance. That's
the bottom line. 

I wonder if a larger net might tend to have more stbale q values on
the input side and less stable on the output? Or if the input side
in general might tend to have q values that help distribute activity
and create the needed percpetion learning? And this lack of balance,
is just a small net problem?

So, I came up with this whole b value to accumlate q value evidence
exactly becuse the system was going "random" to easily, now I'm facing
the problem I can't MAKE the system go random when I WANT it to go
random to do percepetion! Tough. Big unanswered questions remains --
when SHOULD it go random and how do I tell?

Maybe in a networok, the nodes could be bised so the input nodes on the
left go random more easilly with it getting less easy to be random as
it goes across?

At the moment, as I watch it running, a +1 0 +1 net had learned to
balance on the two +1 ouptuts. Onl took 10 mnutes of run time to get
the q values adjusted. It's at 49/50 now. I have some random added to
reward values (+- .01 I think)

Shit, maybe that sovled it? Add the random to rewards, and it forces
q values to bounce up and down enough to get the b values to stabalize.

I also set very fast q learning (.0001) was 100 times slower.
That allowed the system to converge on real Q values faster I think.
Thinking that with the use of these b values, faster and more chaotic
qLearning might be ok?

Looks like it's staying pretty well balanced!

-------------------------------------------------------------------------------------------------
XNET Sat Feb  6 15:51:21 2016  RunTime: 0:14:46 [+1 -1 +1 rewards]
Colapse Test 3 x 2  time: 4016:58:34.0955  PPS  16423
qLearningSpeed: 0.00010000 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100
  .                            ... ... ...|  5o q:  0.9810239|   4060 ===============
 10o q:  0.9810344  52   .  48  50   1  49|  .o q:  0.9715331|     72 =
  .                            ... ... ...|  5o q:  0.9810476|   4013 ===============

         993862               0          915802
--------------
avgRewards      1.0005
avgRPS          0.9335   0.9649   0.9649   0.9648
qMean:          0.9810   0.9540   Happy!
qChangeVarSD:   0.0000001680
reward:                1.0002292560
rewardAccumulatedTD:   0.0000000000
target:                0.9823199701
-------------------------------------------------------------------------------------------------

Switched to Hall problem to take a look. It's got two nodes, where
one has the None, -1, None output, and the other has None, +1, None.
With fast qLearning, it converged quickly, but again didn't balance
between the two None options, but as all the qValues convered down to
near idenical numbers, the colapsed again! Buecuse of he high qLearning
speed allowed the different options to all fall within the noise of
the learning.

So, tried something new -- qLearning speed seeks 1/1000 of difference
ebtween high and low qValues! So as the net converges, qLearning gets
slower and slower. And on a net with big differnces, qLearning is
automaticaly much faster. In theory. It's working so far. It hasn't
collaped as before. And learning is slow enough to make the bad middle
option look worse than the ok bottom option which is not being used.
But the twoo OK options that could in theory be shared, is not getting
close to share them.

So I fix the net falling apart due to falling into chaos, but I can't
get it fall into chaos on purpose to force percpetion sharing of
data to both paths.

I like the idea of auto adjust on learning speed, but I'm not so
sure this algorithm will extend well to a much larger net that might
have a larger range of values active noramlly. But I'll keep
this feature for now.

Going to add some random into the rewards for the Hall problem
and see how that changes things.

Ah, look, the two values got close enough and it's starting to share without
yet clolapsing. Lets see what happens...

-------------------------------------------------------------------------------------------------
XNET Sat Feb  6 16:36:41 2016  RunTime: 0:24:44

  foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========

Hall Test 7 x 2  time: 658:17:16.1705  PPS  17761
qLearningSpeed: 0.00000644 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100
  .                            ... ... ...| 36o q:  5.3760431|   1664 ============
 48o q:  5.3757663  61   .  39  74   1  25|  .o q:  5.3750715|     28 =
  .                            ... ... ...| 12o q:  5.3760279|    509 ====
  .                            ... ... ...|  .               | 
  .                            ... ... ...|  0o q:  5.3805165|     30 =
 59o q:  5.3815126   . 100   .   1  98   1| 58o q:  5.3812913|   2152 ===============
  .                            ... ... ...|  1o q:  5.3805752|     23 =

+--------------------+
|..........B........:|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 2369836


--------------
avgRewards      0.9970
avgRPS          5.7591   5.2254   5.2248   5.2242

qMean:          5.3789   5.2087   Happy!
qDiff:          0.0064409374   5.3750715110 to  5.3815124484
qChangeVarSD:   0.0000135986
qLearningSpeed  0.0000064374
reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:                5.4113066351
--------------
avgRewards      0.9677
avgRPS          4.0705   5.2254   5.2248   5.2242

qMean:          5.3781   5.2087   Happy!
qDiff:          0.0064145946   5.3750715188 to  5.3814861134
qChangeVarSD:   0.0000188409
qLearningSpeed  0.0000064375
reward:               -1.0000000000
rewardAccumulatedTD:   0.0057546909
target:                4.9877784116
-------------------------------------------------------------------------------------------------

[6:23] back from noodels & Co for dinner.

Found both versions (one without random, the other with 1% random rewards) had managed
to start balancing and not colapse.

For this net setting qLearning to 1/1000 of the range of qValues in the net has given
it the power to converge close enough to correctly distinguish a 1 discounted
pulse difference between bad and good and not start using the bad one.


I don't really expect them to stay well balanced but if they drift on and off
and just don'g 1) colapse and forger everthhing, or 2) don't lock to only one
of the two option, that's probably good enough. Will have to just test other things
and see what I find.

-------------------------------------------------------------------------------------------------
XNET Sat Feb  6 17:03:06 2016  RunTime: 1:19:00  [ added random to rewards ]

  foodLeft freq: 10.0 ==========  foodRight freq:  1.0 =

Hall Test 7 x 2  time: 2124:42:00.6233  PPS  17548
qLearningSpeed: 0.00000626 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100
  .                            ... ... ...| 18o q:  5.4022645|   1044 ========
 41o q:  5.4019882  46   .  54  48   1  51|  0o q:  5.4014726|     14 =
  .                            ... ... ...| 23o q:  5.4023737|   1153 ========
  .                            ... ... ...|  .               | 
  .                            ... ... ...|  1o q:  5.4067931|     19 =
 65o q:  5.4077785   . 100   .   1  98   1| 63o q:  5.4075391|   2201 ================
  .                            ... ... ...|  1o q:  5.4068666|     25 =

+--------------------+
|:.........B.........|
+--------------------+
Beasty location: 10   Food Count: 0   Seconds: 7648920


--------------
avgRewards      0.9996
avgRPS          6.1820   5.1649   5.1645   5.1642

qMean:          5.4054   5.1660   Happy!
qDiff:          0.0063058659   5.4014726477 to  5.4077785136
qChangeVarSD:   0.0000114090
qLearningSpeed  0.0000062575
reward:                1.0072788209
rewardAccumulatedTD:   0.0000000000
target:                5.1427453010
--------------
avgRewards      0.9793
avgRPS          5.3107   5.1649   5.1645   5.1642

qMean:          5.4050   5.1660   Happy!
qDiff:          0.0062762651   5.4014725339 to  5.4077487990
qChangeVarSD:   0.0000129598
qLearningSpeed  0.0000062576
reward:               -0.9922934079
rewardAccumulatedTD:   0.0000000000
target:                5.1154615276
-------------------------------------------------------------------------------------------------

[8:55 PM] came back after a while an it had solved this simple version of the HallTest.
It wired one node to the one output and the other to the other! Difference in Q values
were very small. Want to make display fancier by having it update the Beasty Hall
diagram eery time the Beasty moves!

-------------------------------------------------------------------------------------------------
at Feb  6 19:01:27 2016  RunTime: 1:55:06  Real-Time Simulation speed=100X

  foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========

Hall Test 7 x 3  time: 191:51:03.9758  PPS  1100
qLearningSpeed: 0.00005506 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100
  .                            ... ... ...|  1o q: 11.9933208  21  50  29  51  15  34| 55o q: 12.0059470|    139 ================
 56o q: 12.0059337   . 100   .   1  98   1| 55o q: 12.0059351 100   .   .  98   1   1|  0o q: 11.9896265|      2 =
  .                            ... ... ...|  1o q: 11.9918080  24  25  51  36  46  18|  0o q: 11.9892652|      2 =
  .                            ... ... ...|  .                            ... ... ...| 49o q: 12.0059286|    127 ==============
  .                            ... ... ...| 48o q: 12.0059212 100   .   .  98   1   1|  2o q: 11.9972380|      3 =
 50o q: 12.0059129 100   .   .  98   1   1|  1o q: 11.9924018  28  35  36  30  24  45|  .  q: 11.9966614| 
  .                            ... ... ...|  0o q: 11.9920501  31  29  39  32  52  17|  0o q: 11.9510907|      2 =

+--------------------+
|B..................:|
+--------------------+
Beasty location:  0   Food Count: 239713   Avg SPF: 2.0   Last SPF: 2.0   Seconds: 0


--------------
avgRewards      1.0789
avgRPS         11.5878  10.0604  10.0058   9.9492
qMean:         12.0057   8.7523   Happy!

qDiff:          0.0548563443  11.9510906571 to 12.0059470015
qChangeVarSD:   0.0001780452
qLearningSpeed  0.0000550619

reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:               11.9075909935
--------------
avgRewards      1.0748
avgRPS         11.5364  10.0604  10.0058   9.9492
qMean:         12.0057   8.7523   Happy!

qDiff:          0.0546893662  11.9510923394 to 12.0057817056
qChangeVarSD:   0.0001672356
qLearningSpeed  0.0000550548

reward:                6.0000000000
rewardAccumulatedTD:   0.0000259960
target:               12.3579060093
-------------------------------------------------------------------------------------------------

[10:09 PM]  Wow. Got the display working better for Hall so I can
see what's happening. Beasty is updated in real time so I alway see
him move. I honestly don't have a clue, how this design can work!
When you track qValues in nodes, and one path to that node is needed
to solve theproblem, that node qValue will go up. But as it goes up, it
tries to SUCK data from the other neighbors beuse making this node fire
is "good". But when the other path makes it fire, it could well be bad.
And that drives the q value down.

It looks like a total cluster fuck at work here. We need to reward LINKS
not nodes. It's the use of links that we need to know whether it's good or
bad, not nodes. If there's a subtle and complex way to use nodes, and not
suck data that shouldn't be sucked, I wish I understood how that works.

Ok, so, to make nodes work, the network has to be large enough, to
allow paths to connect node to node, and keep the other data out of
reach of the nodes they shouldn't be sending data to. My small current
HallTest network is not that way. And I just tried even and odd outputs
to control left and right. And boy, that has no hope of working since
it bring left and right control data right next to the same nodes.
There's no way they could keep apart. The network would need to learn to
send left data to one part, and in that part of the output net, give all
the right outputs bad ratings. And then in another part of the net, send
the right data, and do the reverse. I could see how it could learn that.
But it seems you need a much larger network to solve the same probnlem,
than if you just tracked link Qvalues instead.

Is there a reason to track both I wonder?

and of course, the reason I switched away from tracking link qValues,
was the problem of implementing percpetion and of getting signals to
balance for percpetion learning.

Boy this is just a constant battle. I get one thing fixed and working
well, then realize I've just borken the other end of the same problem.
Add b values to make sure the network stays locked down after learning
and doesn't turn chaotic, and then the network suffers from not being
chaotic enough BEFORE learning.

There's an answer here. I just have to keep running in circles untill I
find it!

The calcuation of qVlues is far better today than it was before.
So I could use this improved Calcuation of Q values with links without
any problem. LInks could work the same they do now, but just use
the link Q instead of the destination node Q. I thnk that would be
trival to change and would work just fine.

But there there is no correlation balanceing at work to create perception
learning.

Well, even with the struggles the network seems to be sort of finding
solutions to the puzzle, just not good/prefect solutions, just random
crap that works a bit better and is not stable and is not converging.

So, I need to explore larger nets, to give the node based net room
to learn. And gota fix this even odd ouput that is far far hardr than
it need to be for the poor thing.

[11:39 PM]  Ok, so below is the hall problem all very nicley focused in
on a solid solution. Three levels deep network. So this was confgiured
thae the solution was onl a staight though wiring, but it was working
well the instant it was turned on, making the learning trivally easy. But
it's nice to see th esystem converging on a stable and good solution
none the less! And not colapsing.

The B values I wonder if they aren't causing other problems (like causing
osscilations), but they sure are key in allowing the sytem to lock on
a solution and keep locked on it.

-------------------------------------------------------------------------------------------------
XNET Sat Feb  6 22:40:42 2016  RunTime: 0:58:38

  foodLeft freq: 10.0 ==========  foodRight freq:  1.0 =

Hall Test 7 x 4  time: 521:56:43.4105  PPS  5903
qLearningSpeed: 0.00002196 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100
  .                      ... ... ...|  1o 12.6415  19  20  60| 51  42   7|  1o 12.6416   4  10  86| 33  40  28|  .o 12.6415||     10 =
 52o 12.6441   . 100   .|  1  98   1| 50o 12.6441   . 100   .|  1  98   1| 49o 12.6441   .  94   6|  1  92   7| 48o 12.6441||   1343 ===============
  .                      ... ... ...|  1o 12.6414  44  48   8|  1  64  35|  1o 12.6421  53  46   1|  1  74  25|  4o 12.6437||    126 ==
  .                      ... ... ...|  .                      ... ... ...|  0o 12.6221  21  37  42| 16  60  24|  0o 12.6315||      6 =
  .                      ... ... ...|  1o 12.6416   5  34  61| 24  62  15|  1o 12.6414   .  40  60|  1  98   1|  1o 12.6423||     26 =
 55o 12.6441   . 100   .|  1  98   1| 53o 12.6441   . 100   .|  1  98   1| 54o 12.6441   . 100   .|  1  98   1| 54o 12.6441||   1440 ================
  .                      ... ... ...|  1o 12.6415  81  13   6| 19  51  29|  1o 12.6416  70  30   .| 46  53   1|  0o 12.6415||     12 =

+--------------------+
:....B..............:|
+--------------------+
Beasty location: 16   Food Count: 868906   Avg SPF: 1.9   Last SPF: 1.9   Seconds: 0

--------------
avgRewards      1.1379
avgRPS         12.6517  11.8182  11.8118  11.8053
qMean:         12.6440  11.8222   Happy!

qDiff:          0.0219682072  12.6221083950 to 12.6440766022
qChangeVarSD:   0.0000932246
qLearningSpeed  0.0000219568

reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:               12.6236878758
-------------------------------------------------------------------------------------------------

So, for this node based network to find answers I think I just have to
understand needs a lot of net to work with so nodes can can be issolated
from each other. BUt that'sgoing to be such a waste.

I'm going to have to switch back to link based qValues and figure out
how to make perception work.

Here's a thought. If perception learning has to happen at the node
level, and q learning works best at the link level, then make the node
level percpetion system train the links with rewards and punishments.
I think I wrote this very idea a week or two ago whenthinking about why
I had to swtich the node level RL learning and was thnking "the only
way link levle could work ..."

But maybe that's it. I'vehad it backwareds all the tme. Beucase pecoetin
happens "first" in the input side, and RL happens "last" on the output
side, this has biased my t hinking that percdoetin happens at the
lower level, and RL then is a higher level effect that "tells" the
lower levelpercoitn system what to do! Maybe that's biased my thinking
the wrong way for a decade now? Maybe Rl is the low level system, and
perception is the high level system that trains the low level Rl system.
Cool thought. But does it have any meret?

Ah, and again, I was thinking that if RL was "equal q values" and had
no strong opinion, then it would "allow" the percpeiton behavior to
take over. But maybe that's all wrong as well. Maybe percpetio nis king,
and it "decides" when to dominate, and when to all "RL" to do it's thing?

Like I was thinking early today, maybe the first columns of the net
are just wired to be mostly perception links, and the later ones are
"allowed" to be mostly RL?

O K. I'm just going to have to damn rewrite this again, and move fucking
qLearning back to the dman links, and then, make RL work again from the
links. THEN, add perception after the fact. I bet it will be easy to
move. And I've got the qLearning working very nicely compared to what
I had before.

This is something else I missed for the past 10 years. I was so focused
on percpetion, I didn't bother to try and make RL work and work out the
bugs with that as I have been doing these past weeks, and even some last
year(s?) when I got the first bad version of RL working. I think a cool
thing to do here is to see just how far RL can go lke this, without doing
anything really at all speical to make percpetion work! Even without
percpeotin balancing, if RL just scatters pulses all though the net
when qValues aren't stable, it can find lots of solutions that way.
Certainly any solution that are just straight wiring problems can be
solved like that.

So, again, short term plan: focus on RL and do as MUCH as I can with RL
alone. And just ignore the percpetion problem completely. Like adding
these B nodes to help it stay convergd, and the adjustable qLearning rate
to help it run fast when "seeking" and slow when converting. Then test,
with simple tests, to find out what RL alone can do, and what it can't do!
And then stat adding the percpetion like features needed to make it sove
what it can't do with RL alone?

Maybe that will make it clear and obvious what aspect RL along can't solve
and why and what form of perception is really needed? Sounds like a plan.
For tomorrow. Super bowl day. That I won't watch.

[12:01 AM Sunday]  Time for bed.

================================================================================

2016-02-07 12:13 PM Sunday

Ok took sleeping medication last night and got a lot of rest, but I'm
in a daze this morning. Have been so excited about proress on this
stuff that I can't keep my mind from buzzing even when I'm dead
tired. To tired to work, too awake to sleep. The drugs helped
me sleep, but now I'm slow to get going. Just got back from starbucks.

The Hall program I left running over night seems good. It's disovered
good paths and has lots of paths at 100%. Realized, that even
though I have a forced 1% min explore on nodes, the net will still
try to work around that and giv3en enough room in the net, it
will reduce the actual output bad moves to far less than 1% because
it will require those bad 1% moves to go though mutliple nodes
that have all been marked as "bad" reducing the actual bad outputs
to far less. Pint being, given enough network space to work with
a 1% forced explore per node does not need to translate to a 1% forced
bad moves on the final output!

Here's a cut and past of the current condition of the overnight
Hall run.

-------------------------------------------------------------------------------------------------
XNET Sat Feb  6 22:40:42 2016  RunTime: 13:30:55

  foodLeft freq:  1.0 =  foodRight freq: 10.0 ==========

Hall Test 7 x 4  time: 7263:49:53.5950  PPS  5985
qLearningSpeed: 0.00009890 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100
  .                      ... ... ...| 48o 11.7904  12  16  72|  2   2  96|  9o 11.8066  48   7  45| 11   1  88| 11o 11.8045||    290 ====
 63o 11.7887  70  12  19| 75  12  13|  7o 11.8111  47  33  20| 88   1  11| 42o 11.7910  37  18  45| 51   2  47|  9o 11.8051||    253 ====
  .                      ... ... ...|  8o 11.8102  49  35  16|  1  82  17|  4o 11.8242   5  67  27|  1  11  88| 32o 11.7923||    942 ============
  .                      ... ... ...|  .                      ... ... ...|  7o 11.8159  52  22  27|  1  32  67|  2o 11.8600||     71 =
  .                      ... ... ...|  7o 11.8149  80   8  12| 60  39   1|  3o 11.8902  51  28  21| 61  19  20|  6o 11.8078||    171 ===
 44o 11.7905  11  17  72| 12  11  77|  8o 11.8207  17  43  40| 22   5  73| 24o 11.7927   9  72  19| 10  72  18| 35o 11.7923||    992 ============
  .                      ... ... ...| 28o 11.7922  73  12  15| 96   3   1| 18o 11.8094  66  34   .| 49  50   1| 12o 11.8215||    270 ====

+--------------------+
:..........B........:|
+--------------------+
Beasty location: 13   Food Count: 13155837   Avg SPF: 2.3   Last SPF: 3.7   Seconds: 1

--------------
avgRewards      0.9356
avgRPS         10.2048  12.3948  12.3953  12.3956
qMean:         11.7977  12.4168   Sad!

qDiff:          0.1015015789  11.7887226794 to 11.8902242583
qChangeVarSD:   0.0002990217
qLearningSpeed  0.0000989039

reward:                1.0000000000
rewardAccumulatedTD:   0.0000000000
target:               11.7760508790

-------------------------------------------------------------------------------------------------

Interesting in this cut and past, the net has turned more chaotic than
it was ealier this morning. The RPS is dropping, and it's down to 6
something now from a high of 12. So clearly this net is not doing a
good job of converging on even a simple solution as this one is.

The above is a dead simple solution because the two inputs tell the net
where the food is and the solution is to just map the left intputs to the
outputs, where any of the first 3 are can be used for the first input
and any of the last 2 for the second (don't know about the middle --
depends on rounding).

Ok, so I think this is just more evidence that the qLearning really
has to take place in the links and not the nodes. If it happens in the
nodes,  you need LOTS of extra space so that entire mountain ranges of
good nodes can form to lead pulses from inputs to outputs and entire
vallleys of doom can form to issolate the data running on one peek from
data on other peeks. And then you can't ever make the damn data paths
cross without lots of care with the toplogy.

Yeah, qLearning has to happen in the nodes, and I'll jut have to figure
out how to do percpetion learning from there. So job one this morning
is recode for qLearning in the links.

[2:36 PM]

Coded q learning in links. Created a isLinkQValues and isNodeQValues
to turn each on and off easilly. Testing link learning.

The first qValue reached on a new sort is a link now, so the lead-in
links, form the foundation of what qValues get back proped to the last
pulse. So there are 3x more "first" q Values in the net than before.
Intersting. Seems ok.

On simple -1 None +1 output test, the q values seem to be converging
on numbers with larger sepearation than it was doing with the node
learning. I think the fact that they don't have to tie back to a single
input node is important here. But that gain I assume would be lost in
a larger net due to there being very little correlation or differences
at work in the first input lines? But then again, if they are pain and
plesure signals, hey, sure, there will be a + or - q correlation that
back props to all the other qValues instantly. That's cool becuase it's
instantly back proping a prediction of pain or pleasure without the need
for the real rewards to show up yet.

Test idea -- reward a network baesd on if a pulse passes though some
random node in the net and see how it learns the correlation and how it
learns to use, or avoid, that node! Or, reward the use of a link and
see the same! And if don't do qValues on nodes, but do reward it baesd
on the node, do the links learn what then need to correctly to make the
system use or avoid that node?

I'm wondering if there might be a reason to use both link and node q
values. I'm thinking that if we use links, the links leading into a node
(our out of it?) fully predict the node value so there's no need for the
node? Actually, it would be the nodes leading OUT of the node times it's
probablity that should fully predict that the node's q value would be.
Well, take that all back, with eligblity traces back proping rewards
from the far future, that's not true. Oh, but hey, it's mostly true.
It The value of a node I think has to be a probablity distribution over
the value of the links -- but that distribution does not have to match
the actual use of links because that could be biased by the correatlions
between rewards and links. If there is no such correlation, then the
use of the links will correctly defeine the q of the node I think, but
if the use of links don't correlate with the rewards, it wouldn't be.
But hey, how could the use of links NOT correlate with rewards since
their q values are DEFINED by the correlation of the links with rewards?
Yeah, I'm thinking the q of the node HAS to be equal to the q of the
links averaged by the probablity of their use. Making tracking the node
q values pointless. Tracking link q values should always give higher
resolution informaiton about rewards -- and give it to us just where we
need to know it, to make wise links pulse sorting decisions.

Q LEARNING SPEED

Ok, but watching this simple net train, I'm noticing something really
intersting about the learning speed statistics:

qDiff:          0.0098143091   0.9611920334 to  0.9710063426
qChangeVarSD:   0.0000000175
qLearningSpeed  0.0000098156

The change SD (standard deviation) is far far smaller than the learning
speed! If q values are changing that slowly, it seems to me we have
the learning speed set to high? Or more important I think I'm not
auto-controlling the learning speed the best way posisble. Learning speed
is part of what controls the noise in the learning, but other factors
are at work as well. And what we don't want, is for the learning speed
to be the dominate noise in the process. We want to measure the noise
level of of all the other noise sources in the learning process, and set
the learning speed to be below that so that learning noise is never the
dominate force in the system, AND as the other noises souces slow down
due to convergence, elarning noise drops as well. I'm measuring those
"other learning noises" currently by looking at the diff between the max
q value and the min q value. Which I thnk was a good gues, but not as
good as it should be. I need to track the actual other noise effects
in teh q update formula other than learning speed, and track that!
The learning update is:

    discount = at.getDecay(clock - self.lastRewardUpdateTime)

    target = (self.rewardValue + self.rewardAccumulatedTD) * at.inc +
	discount * targetNodeOrLink.qValue 

    dq = (target - nodeOrLink.qValue) * nodeOrLink.getV(clock) *
	self.qLearningSpeed
	
    nodeOrLink.qValue += dq

    self.qChangeVar.updateV(clock, factor=dq*dq)

So the other factors are controlled by the differences in targerts
which is directl realted to the differences between the input nodes
of the net, and all the rest of it, and the pulse flow VOLUME levels.
And this test I'm doing now, has 1 pulse per second vs the ones I've
used in the past that were around 10 a second so the SD number is 1/10
of what was in the past due to volume being 1/10. So my current rule of
measuring update noise levels is capturing the noise in the first term,
but not adjusting for the noise in the second term!

So the noise comes from:

  1) time variation of rewards (discount term).
  2) reward values (such as the .1% random() I added to this test)
  3) If rewards happen (the environment)
  4) accumulatedTD (don't know if this is done right, but it's an
     attempt at secondary reinforment. So we could call this internal
     secondary reinforcement reward noise.
  5) Difference in qValues from target to source nodes.
     target is always the q values from the first input columm and source
     is the node being trained. This is what is closely tied to the
     idea of qMax vs qMin for the entire network. But it's related to
     both how well the net is trained yet, AND the total varation that
     remains after the network converges. If the converged network has
     a difference of 10 the learning update should not be limited by
     that. learning speed should convege to zero as the net stabalizes
     no matter what the q values are once it's stable. So measuring the
     differnce in the net is not the correct way to set learning speed!
  6) Varation in pulse volumes. Contolled by both the
     ActiveTrace halfLife, as well as the environment. The half
     life doesn't need to converge, so the halflife noise might be the
     limiting factor on convergence when we do a lab test that has no
     fake enviornmental noise. Oh, and since we are a stocastic network,
     our own stcastic sorting noise services in this pulse volume nose
     to a level defined by the half life.

 So the level of all this noise, gets amplifed, or muted, by the learning
 rate. So the learning rate isn't really the noise source, it's just
 the volume control. That's a better way to think about it.

 So I need to set the learning rate, as a fucntion of how stable the
 network is I guess. How the hell do I measure that? And I guess
 I need to know what the noise level is, which I'm amplifying or
 muting with the learning rate? So the correct learning rate is
 something like a ratio between the noise level the learning rate
 is cotnrolling and the network q value dift rate? The drift
 rate being a sum of the changes over time. S0 +1 +1 +1 is a much
 higher drift rate than +1 -1 +1 -1 (which shows a stable
 network).

 So, I can use the ActivityTrace to measure drift per second, and
 the ActivityAvg to measure learning noise (what I have already with SD)
 And maybe those two will point me towards the correct learning rate.
 But my current SD is measuring noise levels after adjusting by
 the learning rate. I need to measure the before noise. So,
 time for more statistics, Yeah!

 [9:17 PM]

 So, I coded lots of new statistics, tryed to calculate the learning
 rate, and I'm still sortof lost.

 So, to record some numbers.  a 7 hour run with the a learning
 rate defined as 1/1000 of the qDiff seems to have stabalizesd
 with q values around:

   .9622__ .96703___ .9712___  .000009 is the approximate rate

   0.96224___      0.96704__       0.97128834  learning rate 0.0000025145
   0.96224441      0.96704820      0.97132909


And learning rates run down to 1e-11: (two runs)

   0.96224120      0.96702872      0.97136904
   0.96218023      0.96696505      0.97116248

And learning rated run down to 1e-10 (two runs)

   0.96220077      0.96700409      0.97123052
   0.96224686      0.96705069      0.97123540

And learning rated run down to 1e-10 (two runs) random reward turned off

   0.96207218      0.96688281      0.97076478
   0.96215677      0.96696398      0.97106984

And learning rated run down to 1e-9 (two runs) random reward turned off

   0.96202632      0.96682898      0.97073141
   0.96207557      0.96689278      0.97076818

And learning rated run down to 1e-8 (two runs) random reward turned off

   0.96210550      0.96692004      0.97095428
   0.96215866      0.96695682      0.97096207

Looks like most of this is different at about the .00001 level.
Now doing a 1e-5 learning rate and will let it run to see
if the two seem to converge on the same answers to the level
of the changing digits. This is about 1/100 the SD value.

Even with learning rate run down, all the other stats like drift and SD
hang in the range of +- .0001 sort of change levels.

But this test has random noise on the reward size. So let me turn that
off and see what happens. Answer, the drift numbers look superfically
about the same. Making me think this is the result of the random stocatic
nature of the net combined with the dynamic character of the net with it's
effective internal feedback between qValues trying to seek each other.

Still kida lost as to what the correct "best" way to auto set the learning
rate might be. But a fraction of the learning variance might be the
answer. At least in this network that seems to be the limit of what's
possible. I was feeling that the learning various would be related to
the learning rate and as the learning rate drops so would the variance
causing both to noise drive. But it seems the other noise in the system
limits how low the variance can actually drop so keeping learning rate
a fraction of that might be the answer as how to keep learning rate
set correct -- at a fraction of the inherent noise from the learning
signal. like 1/100 or 1/1000 of the SD of the learning target signal.

But, more tests still running...

Ah, tried 1000 pulses per second test. Blew up. Requires very
small qLearning rate (like e-5 minimum).

First sd/1000 can't work becuase sd is a function of things like reward
value. Double the reward value, and sd doubles even though nothing else
has changed relative to how fast rewards are changing.

It's not easy to normalie SD either becuase rewards can be postive
and negative. Maybe at best you would use the q value range as a way
to capture it but it all gets very messy.

And, this whole idea of trying to speed up learning to help the network
move faster when it's far away from convergence and slow it down when
near convergence is full of conceptual errors.

Learning automatically works like that even you hold the learning rate
constant due to the difference between reward values and q values.
So there's not a lot to be gained.

More impotant, the bigger issue is how close together q values can
become where where we need to know which is larger to keep the network
from colapsing. Learning has to be slow enough, that the relative
difference of qValues can be detected down to the resolution needed to
tell good from bad.

And THAT has a lot to do with halfLife discounts. It's all about the
average reward discount -- which is halfLife and average reward spacing
that detemines how close different rewards can become in percent terms
(how many digits of significace is needed to tell which is better).

So when I speed the pulse rate up to 1000 (from 1), it had the same
effect (I think) as if I had increased the halfLife by 1000. It made the
discount between reward very small, which means the difference in qValued
causes by losing only one pulse out of 1000 (the difference between the
good and bad paths on this test), was very small. The learning rate
had to keep the learning noise BELOW that very small update amount
to keep the net from colapsing as it tried to converge.

So, it looks like dynamcially changing the learning rate to help it learn
faster doesn't make a lot of sense at all. But what could be needed,
is adjusting the learning rate to the amount of the average learning
discount adjusted by the aveage noise.

So if reward discount is in the .0001 range learning needs to be maybe
1/100 below that. But if SD0 noise is in the 100 range, we have to
lower learning another factor of 100, to keep the learning noise below
the discount. So correct learning rate might need to be something like
average reward discount / SD0? And that's not to speed up learning,
just to set learning at a numbner that fits the problem -- and which
isn't far slower than it need be for the problem. But more important,
not too large, which prevents the network from converging and stabalizing.
So let me look at those numbers and see what it says!

[1:44 AM Mon]

Boy, I've been looking at this qLearningRate issue for hours and hours and
I'm more confused now than when I started. I'm just giving up at this
point. I can't for the life of me come up with a good understanding of
how to know what is "right" for the learning rate yet alone automaticaly
compute it! It's something realted to the SD0 number compared to how
close the qValues need to converge but I don't see any real way to know
what the qValues will need to converge to Becuase that's all tied up in
the compliexties of the learned policy vs the environment.

I guess it just has to be picked by complex trial and error testing.
Too small and the net takes forever to learn, too large, and the net
will have problems converging or staying converged. It's all tied up
in the needs of the applciaiton vs the charicteristics of the problem.

Well, more testing with the simple Convergence test, made me realize one
reason it's hard, is becuase the halfLife is long, but the problem only
has emdiate rewards. So the halfLife is not a good fit for the problem
it's trying to learn. So I reduced the halfLife from 10 seconds to 1
second to see what that does. And the answer I think is it spread the
Q values much further out making it easier for the network to understand
good from bad. But I'm still playing. And I need to go to bed.

================================================================================

2016-02-08 9:25 AM Monday

Yesterday

1) moved qLearning from nodes to links. Added isNodeQLearning and
isLinkQLearning to swtich back and forth. Learned how to take advantage
of Phython lack of types to tread both Nodes and Links as "q objects"
without having to formally define such.

2) Experimented with automatic setting of qLearning speed and realized
it's impossible. It's controlled by the enviornment and the goals of the
designer in terms of desired learning speed vs what needs to be learned.
The qLearning speed controls how much of the SD0 noise is amplifed in
the Q values and if the noise is too high it can't learn small high
resolution differences in qValues. Also learned that these networks
don't seem able to converge down to one fixed answer. Reducing learning
rate to zero slowly forces it to converge on one answer, but evey time
you do it, it stops at a slightly different answer. Maybe that's just a
lack of data -- so that for ever extra digit of convergence you want you
have to run the system for 10 times as long? Maybe it works that way.
The stocastic noise of path selection adds noise that is no doubt forcing
this need for longer runs and limiting resolution/speed of convergence.

3) halfLife is critical! The ColapseTest was failing not as much
because learning was too fast, but becasue the test was for instant
reward feedback, and I was using a 10 second halfLife so that meant the
Differences in qValues were tiny (3rd or 4th decimal place). Setting
half life to 1.0 made the difference .5 vs .8 vs .9 -- in the first
decimal place.  halfLife must be matched to the general needs of the
problem. When matched, it's a one digit learning problem, not a 4 digit
learning problem. With halfLife at .1, q onvergers to -.16, .00, .98

4) Learning even more that there's a lot to the RL side of this problem
I didn't undrestand as well as I need to. I need to keep pushing forward
on RL experimentation until I feel I have done all I can do, without
making percpetion better.

Today

Think about halfLife. Test the link verison of qLearning some more.
Explore whatever shows up.

Could halfLife be set(adjusted) dynamically at different values for
each node?

If pulse activity of two nodes overlaps a lot, then if their halfLife
overlaps in that same time frame, it will be very hard for the system
to extract signal -- back to that 4 digits of percpetion problem. But
halflife really is a design parmater based on goals of the designer
so it's probably pointless again to think about aut-setting halfLife.

But maybe, what I like about this thinking, is that when halfLife
is too large, and node acivity overlaps too much, qValues become
random -- FORCING balanced flow and PERCPETION! So that might
be how percpetion vs QLearning is balanced -- by the design needs
of the problem. The settings of qLearningSpeed and halfLife along
with the dymanics of the eviornmemnt, determine where in the network
percpetion stops and RL starts. If the network can't FIND any
correlations, in the reward data, the entire network is acting like
a percpeiton network. And if the fully decoded percetoin of the sensory
data still revels no correlations, then the network is unable to learn.

So the goal of percpetion, could be to turn overlapping data in time,
to longer and longer temporal strings so as to give the system more low
frequency signals to correlate to rewards.

So it sarts with data like this

A AA  A  A  A AA  A AA   AA
 B   B B BBB  B BB  BB  B

And ends up with high level signals like this after perception decoding.

XX X XXX  XXX XX    X      X  X
   Y    Y Y        Y YYYY YYY YY   Y

So the percpetion is what happens when RL can't find correlations
due to halfLIfe and qLarningSpeed hidding the correlations below
the noise levels of learning and the goal of percpetion learning
is to spread out the time frame so it can learn.

The entire reason I added the B vars and thought they were so imporant
was really to just help extract more signal from noisy qVaribles --
but maybe that's all wrong. If the signal is below the noise, that
is how percpetion kicks in, so maybe I can just throw out the B stuff
all together and do gredy RL learning directl to the w variables?
I'll leave the B in for now, because it might be needed to seperate B and
W for percpetion learning and it seems to work ok for now. But it will be
great if I can later remove it and make the net that much simpler.

So, time for starbucks run.

[12:27 PM]

Went for coffee. Got sidetracked writtig a long letter to Michael about
why I'm not going to send him a resume becuase of how close I am to
Ai and how close the world is to solving AI. He said the two bets were
for $10 each. I now I intended the last one to be for more but I don't
remmber how much more, and it seems he never agreed to it being more in
his mind, so $10 it is. I'm $20 down so far.

So, time to play more with the curent link level code and learn what it
can do.

[1:55 PM]

Writing showNode() in Network to make it easier to dump node details
for testing. But by adjusting wLearning and halfLife correctlly on
the TwoNode test, the dual outputs of None None on th etop node
were withing the noise level of qLearning, and they are balancing
beauthfully so it's balancing a tie for good paths without any
problem! Even better than balnicing all three when all are unknown
beuse now it can turn off known bad paths, but still leave the rest
working as perception learning paths!

-------------------------------------------------------------------------------------------------
XNET Mon Feb  8 13:48:42 2016  RunTime: 0:06:27

Two Node Test row1: -1, row5: +1 7 x 2  time: 813:38:38.9140  PPS  14812
qLearningSpeed: 0.00100000 bLearningSpeed: 0.00000100 wLearningSpeed: 0.00000100 halfLife: 1.0
  .                   ... ... ... ... ... ...|  1o||   1839 ========
  5o 49   .  51| 49   1  50  0.98  0.78  0.98|  .o||     32 =
  .                   ... ... ... ... ... ...|  5o||   1808 ========
  .                   ... ... ... ... ... ...|  .      | 
  .                   ... ... ... ... ... ...|  .o||     36 =
  5o  . 100   .|  1  98   1  0.85  0.98  0.85|  5o||   3610 ================
  .                   ... ... ... ... ... ...|  .o||     33 =
Node [1][0] 0.53070
  q:   0.98236191    0.77862445    0.98256209 
  b:   973966  49         0   0    999891  50 
  w:   0.49332349    0.01000000    0.49667651 
Node [5][0] 0.53070
  q:   0.85400657    0.98394437    0.85494094 
  b:        0   0   1000000 100         0   0 
  w:   0.01000000    0.98000000    0.01000000 
--------------
avgRewards      0.9530
avgRPS          0.8751   0.8422   0.5861   0.3318
qMean:          0.9780   0.7924   Happy!
reward:                1.0017602724
rewardAccumulatedTD:   0.0000000000
target:                0.9920614384
--------------
avgRewards      0.8455
avgRPS          0.8393   0.8422   0.5861   0.3318
qMean:          0.9736   0.7924   Happy!
reward:               -0.9999535835
rewardAccumulatedTD:   0.2052758106
target:                0.5865613856
-------------------------------------------------------------------------------------------------

Also noted that when halfLife was .1 the activity  value was so short
lived that the w learning could not use it correctl to do output
balancing. Well it could, but the b vlaues had to go to near 0 0 100
before the w could start to adjust. The w couldn't balance for 20 20 60
for exmpale. That might be an issue about how I do w learning. A greedy
learning system might allow it to work even with a very short half life.

Maybe activity tracking for nodes, used for w learning and perception
learning might be set different than the activity tracking for links which
are used for qLearning? And maybe the activity tracking for nodes for
percpetion, could need to be dynamic based on frequency levels of data
flows, where as Link activity tracking is constant for the entire net,
since they all have to cross train each other's q values? Just wondering.

Back to output clean up work.. Just wanted to save that screenshot above.

[11:12 PM Monday]

Played with SimpleTest which is a two pulse routing test for a long time.
Had to speed up wLearning relative to qLearning to make it work better.
With the help of some modificatons on rewards to make it route the two
pulses to different places it's able to learn that. But it's amazingly
slow.

The faster wLearning made it do more activity spreading which helped it
to better converge on good answers. With out that it tended to quickly
through all pulses into the same path, and since that was such a high
volume, it's q value rose up the fastest making it look like the best
path then the other paths were such low volume, that the q values would
have taken a very very very long time to catch up to high volume path.
So the network failed to converge on a decent answer (though it likely
would have given it a week or so to train). That really might be more of
a startup problem than a long term problem because it happanes because
the qvalues are all zero at start, and must seek some other value based
on polocy.

Saving the network state to a file and being able to restart from that
state seems like it might be a good idea for testing. It could help deal
with that startup issues by getting a network stable and then testing
from that stable point instead of restaing from scratch all the time.
A real internface with a file save and load would be highly useful.


looked into graphics options and found the graphics.py package for
simple graphis and the Tkinter package that I see is already build into
this version of Python I'm using. I need to learn to use them so I can
make the display more advanced. It's getting harder to work with larger
networks due to a lack of interaction ability to examine and understand
what's happening.

I downloaded a packge of the DeepMind atari game stuff long time back.
Never looked at it. I should find it and to a quick examination of it to
see what it and what language it's written in etc. Before I do much GUI
development I might consider whatever it is DeepMind is working with. It
would be good to learn in case I end up working with them at some point.

The SimpleTest is not in fact so simple. I should back off and do some
more testing with simpler ideas and smaller networks first to see if
there's more to learn about the powers and weaknesses of the RL code.

I turned off the TD accumulation stuff. No clue if it's good or bad
and just wanted to take it out of the picgture when I was playing with
SimpleTest. I just got distrag3d on SimpleTest because I was testing
each text object to see if they all worked and did the mods needed to
make SimpleTest work again.

Should continue tomorrow with the next object I think there is one I
haven't tested yet with the new changes to display etc.

And you know, using the graphics package to create a window to display
the network in should be easy. I should look at that to give more room
for displaing the net on one big window leaving more room for quick and
dirty debug code in the tty window.

BUG: The display auto time adjust didn't work at all on the SimpleTest
becuase SimpleTest kept changing the frequency of pulses which changed
how many simulation cycles it took for the next Display by a big factor.
That needs to be done differently. Maybe a real time queue as well as
a simulation time queue or something?

So much to do! I'm getting burned out just thinking about it and
worrying I've not done more.

[12:58 AM]  Watching HallTest train.

The w learning trying to balance outputs to match B values is really
messing up when the outputs are highly unbalanced due to qLearning.

That is, one output says B: 6 2 2, but the downstream ndoes are like 100
1 1 becaue 1 and 1 suck, , so the w traning becomes 0 5 5 and sends all
pulses to the bad 1 and 1 outputs because their volume is too low! That
is messed up! The balancing is needed, but not when it's fighting what
the B values are trying to get the sorting to do that much. Feels like
there's a soluton here waiting to be created.  how to balance outputs when
b are near 3 3 3 but not worry about balance them when b start to shift.

================================================================================

2016-02-09 10:05 AM Tuesday

Yesterday

1) Cleaned up output of show() to make it more compact to display larger
networks.

2) wrote net.showNode() to dump data of links to zoom in on one link
value and watch it change.

3) Wasted time writting long letters to Tague about AI.

4) Took bLearning out of dispaly because this version if not using it.
added in bMax. Put more params in Network() init so each test can set
it's on network paramaters to match what best fits the test.

5) Spent hours staring at SimpleTest run which is two pulses trained
to sort to two places. So slow for such a simple tasks. System was
mostly learning however when the paramaters were adjusted correctly.
Was not learning when it was just +1 to reward P1 for sorting to a given
otput, and 0 reward for p2 sorting there, and none everywhere else.
P1 would sort to the right place, but it would drag P2 along with it
getting lots of 0 rewards. Didn't seem able to avoid it. I wasn't
punishing P1 becuase 0 vs None was all it could get, and it was not
doing a good job of learning to use None. But by traing P1 to sort to
4 and punishing it for sorting to 7, and P2 +1 for 7 and 0 sorting to
P1's output, it was learning mostly ok.

6) Noticed how the current interaction between B values and w values where
w was trying to make the downstream node activity match the sorting ratio
defined by B was doing some really messed up stuff when the down stream
node activity was already strongly biased to what the B was trying to do.
So downstream activity was like 100 0 0, and b was trying to sort all
to the 100, but got to the point of 6 2 2, but that made w become 0 5
5, trying to force the downstream activity to become 6 2 2 by sorting
everything bottom two. But the volume of the node was like 1 vs 100 so
it had no where near the power to have any effect. But the odd thing
was that 6 2 2 was not growing larger becuase the 2 and 2 outputs had
higher qValues. Probably due to the balance effect causing most of the
little activity there was, to go to the wrong placce and allow those
two qValues to seek up with improved polocy but learning the one o0utput
that needed the most exploration, nearly untouched. Over long long
periods maybe it owuld learn? It should have gone to X.. and then
forced output volume up instead of down. Looking at the runs I left
running, I think they made it out of it in the long term.  

7) Opened up the DeepMind package for their Atri game I downloaded
long ago. It should run on Linux but needs a bunch of packages. Didn't
try to install, but I should some point. The code that came with it
was all written in lua? From Brazel? Wasn't a lot of code, so
I don't know if that was all of it, but I think it likely was.

8) Had neat ideas while trying to fall asleep about b values and
w values and using the same logic as the pulse net, but making
it a number processing net, by treating numbers as averaged counts
of pulses over a time period. So instead of sending individual pulses
into net, send in an array of numbers that represent how many pulses
went in each input over a time period. Was thinking it would be
useful to think of the net this way, even if I didn't swtch paths
to a number calculating net to give different insight into what
my algorithms are doing. And cool thing, maybe we can do both!
So send numbers in asyncronously! So video inputs could be
feed in as numbers at one rate in one part of the network, while
other pulses or numbers were feed in at different rates in other
parts of the network! It would be highly cool if we could take
pixel values and feed them in dirrectl without complex vonversion
to pulse trains! So ending a 6 into the net, it like pretending
we are sending 6 pulses, at that one point in time into the network!
But then the value we send in, would have to spread out in multiple
paths. Hum, or maybe not?  couldn't get to sleep and ended up
taking sleeping pill by like 5AM

Today

Thought: min exploration.  qValues can drift due to improved policy
so even the bad qvalues need to be used a bit to force their qvalues
to be recalculated and keep up to date with the rest. But maybe
a way to do this, is in the application of rewards, by forcing
a mn activity level. So act as if the q was used once in a blue moon
but don't actually force the node to be used. But then I guess, the
problem is that the qValue will drift in time to the average qvalue
giving it a false value, and potentially causing it to be used again
so it's real q value can be determined. But if it's real converging
qValue is higher than it is, that would be good to force it into
activity. But if the real value is lower, that will force it into
activity only so it could drive itself back down to extinction again.
So we end up with the node activty any. I guess it's better to
force the node into activity to start with. Maybe the best way to
think about this, is that all nodes should stay active in the long
term, to keep all qValues accurate, but that the behavior the netowrk
produces is not cotnrolled by absolute pulse outputs, but by relative
pulse outputs, so that constant low level activity can never actually
hurt the real behavior the system is seeking to produce!


Neat new B var idea from bed last night!

So, q values for the three paths trade off being which one is the
max. So we should track that, and set the B values to represent
the RATIOS of how often each path is THE MAX! Of if path one
is the max 5% of the time, B should be 5%. And if path 2 is the
max 80% of the time, it should be 80%. Making the last b value
15%! So as it works now, if path 2 is in the lead more than
the others, the B values accumulate that fact, and turn into
a .X. where all pulses are sent to the 2nd path. But if B is
only in the lead 51% of the time, then we should be sending
51% of the pulses down that path! Not 100% as long as the other
two qValues keep getting into the lead.

So this is cool, because it's what I was trying to do with thinking
of the qValues as overlapping bell curves, beuase when they do
overlap, one will be higher than the others at different ratios.
But I had no idea how to calculate that overlap ratio from things
like means and variance values. But this is a simple impercial
solution to calculating the ratios! And it totally solves the
problem of how to map closeness of q values to flow ratios. And
calculatign this is easy. We just move each B value a fixed
speed towards 1 or 0, to make it converge on the ratios. So
the max qValue b value we move towards 1.0:

  b += (1.0 - b) * speed

And the others we move towards 0!

  b += (0.0 - b) * speed

 The speed ratio determines how long of a time period we want
 to average the Max Race over.

 This should be much better than what the net is doing now,
 because now, the net "flips" B values form one path to another
 shortly after on path takes the lead in the race. But wi this
 the flow will more slowy shift from one state to the next,
 and even more imporant, this should solve the percpetion
 problem of how to make the net "explore" when q values are
 too close -- it's the answer!

 speed thought:  Low volume paths clould learn faster. We
 can't fix this for qLearing becuse it must follow the actual
 discount created by the activity trace, but for this speed
 learning, we might track how long it's been since the last
 update, and adjust so as to act like N updates at once becuaes
 it's been N periods of time since the last update? This would
 make b learning speed a function of time, but not a function
 of pulse volume! Might be good. but for now I won't try it.
 Thought for later to remember.

Percpetion Learning!

So the other thought from bed last night. Thinking of the algorithm
like values instead of pulses, made me think the input values should
be a probablity distribution. So the "numbers" sent into the network,
should be scalled to turn them into a probablity distribution acorss
the inputs. Hey, then we could think of a single pulse like sending the
probablity distribtion of 00001000 into the net! Cool thought.

But the idea is to send this probablity distrbution into the net.
And to have each node, also to be a probablity distbrution in terms
of their memory as each column represnets some state of the world
in it's probablity distribution. And the income PD is then applied
to the update, throught weight values, to produe the next probablity
distrbution, which is then repeated to the end.

The B values then bias this to make the information follow the path
of highest reward.

But the Percpeiton learning would be a second set of numbers, lets
redefine our w values to be the perception learning alone (not mixed
with q Values). The thought is without q learning, the w values
would be seeking the sort of weights that happen in the RBM idea of
trying to make the hidden layer the best possible predictor of the
input layer. But beuase this is tempoal, it's more complex. In
RBM, if I remember correctly, we start with a zero value net, apply
the new input, and make stocastic determinations of the next layer
from the inputs. The reverse that, and make stocastic determations
of the intputs, from the hideen layer, then cicle again, back
to the hidden layer, and in all that, do training based on everything
that chagnes (failed to predict I guess).

What we need, is the same sort of thing, but the goal, that I've been
thinking of as flow balancing, Might be better understood as information
maximizing and the way to do that, is to make each new input, CHANGE
the state of the system, as much as possible. So, beuase my netowrk
has short term memory at every node, and that memory can be thought
of as the eligbilty trace, but also, as the system memory of the recent
past hisotry, the goal of the system, is to fold in the new data,
while MAXIMISING the amount of informationj stored in the short
term meomory! And that's the same ballbark sort of goal that RBMs
are doing whe they maximize the information storesin the net, from 
just the LAST input. Same idea, but we just have more data to work with!

How to correctly maximize information, I'm not 100$ sure. And how
to do the learning update, to make that goal happen, I'm not 100%
sure. But a big idea it seems to me, is that we can think of this
as a problem of taking the input value, and spreading it over the
hidden states, as a probablity distribution update. So input is
1 pulse, we put .2 here, .3 there, and .5 the other place.

This idea came to light thinking about what my restruction of routing
pulses down one path does. If we think of the iput as a set of
numners that count pulses, then the sum of the numbers across
the system must remain constant so it acts like a probablity'
distribution. And 1 pulse number comming in, must be spread ina
 flow blancing act to the rest. So percpetion mapping, with the
 goal of information mazizing should work that same way.

I think, the goal of information maximizing, might be the same as a
goal of variance maximising in the node values, across space and time --
spatial and temporal variance mazimising. If we took a big table of the
q values at one layer of the network over time, the goal would be to
maximize the total variance of the data in that table.

So, how do we maximie that variance? By making each data update from a
pulse, or form an input vector, CHANGE the values as much as possible!
So we seek the w values, that define flow balance (Bayiasn PD updates),
that maximize long term vairance!  of the internal state values.
And I think we do that, by trying to send more data, to the node with
the smallest value! That is Given the ActivityTrace devices, and I
can send a total value of 1.0 split up across multiple ActivityTraces,
but they values must sum to one, how to make the set of ActivityTrac3es
chagne the most? So the varance between the start value, and the finsih
value will be maximized? And I thin the answe ris end all the data to
the AT with the lowest value. Beuase each activity trace will be updated
by self.inc (pretend it's 1.0), and precent wise, this makes the smallest
one change the most precent wise.

I need to test and think if that truely does mazimize variance.

But the cool thing is that sending it to the lowest active node is the
very flow balnainc idea I've bee looking at.

So, do a small learning update, to adjust the current w values to make
it send more to the lowest node. Ignore the B values, and just make
the w values track what we need to do for percpetion alone. If we just
do small updates to make it maximize change and maximize variance,
this should make the w values seek the value, which is like the RMB
information maximising ratio.

So, the exact update, needs to be reserached, but I think when done
correctly, it's bassical a flow balancing update, and an infomration
maximiing update, which is theritlcall very cool. Just need to nail
down the correct update.

AND THEN, we have two seperate flow ratios -- the b rations, which define
RL learning reward maximisng, and the w ratios, that define percpetion
learning infomraiton maximiinsg!

So the question becoes, how to combine them to create policy! First
maybe the answer is a simple PD multiply. That seems highly likely.

We always want to reward maximise, beause that's the goal here, but to
the extendt that there is no q value biase, we want the flow volume
to informaiton maximize!a

So, we throw out all ideas of defining flow balance targets, and the
conflicts that creates in the network with different nodes trying to seek
different targerts, and just think of it as a layer to layer probablity
transofrm, to both rewared maxzimize and information maximize!

Maybe, we need to give on more stright over the other, such as in the
first iput layers, the infomration maximizing is more in portantand
and in later the reward is more important?

But in truth, I think withot the biaes this approach might make
the network innately perfect -- doing the best is can possibly do
by the simple PD combination of both updates in the multiple of
the two factors.

So if B values are 1 0 0, it foces all data to the 1, no matter what
the w vlaues hapen to be (can't let any value actually get to zero
due to divide by zero on normalizing). But if w values needed 0 0 1
to information maximize (like that third output was dead and had
no data sent to it), we would end up with divide by zero isues. So
I guess the trick there, other than a forced min on b and w values
would be to inject data into the net so that no node was every allowed
to go inactive or be unused. Or, incrase the fan out as needed
to make all nodes active in each layer and gurantee that all nodes
have more than one input. Extra random data is not a bad thing all
the time.

Ok this all sounds very fucking cool. Could this be the final answer?

These new ideas always seem so cool when I first thing them up. But themn
I bujild them, find out what they don't do correctly, and then have to
start again! But this progress this week has been amazing. Playing with
the nets is getting all these issues resovled. Getting very close here!

But before coding these new ideas, for b and w and policy, first I need
my Starbucks, and Ignress run. And oh, hey, it's snowing today (mixed
rain and snow with 1 to 3 inches accumlation predicted). And Julie is
driving home for a visit!

Ingress Sojourner badge at day 340 out of 360 for Onyx! Just 20 more
days of hacking to be done with this MUST_HACK_EVERY_DAY_FOR_A_YEAR crap!
Love the game, but just want to bitch at the same time.

Can I solve AI before Sojourner is done! That would be cool!

[11:40 AM]

[12:09 AM]  Back from Starbucks and Ingress.

Had a thought to record. Maybe, there's a way to not use the B values
at all when routing a pulse, and just bias on which max Q value biaesd by
information maximizig? Or somehow, not do the a values either, and just
sort by which downstream node activity is the lowest?  at the same time?
The diea I'm going for, is to throw out the tracking of probablities,
in B and W, and throw out the stocastic random number pick a route based
on calculated flow volumnes, and instead just sort the pulse based on
curent q values and output at values in a way to create the same stocastic
effect, using only the randomoness in the network q and at values as the
tools for scatting over time? Thinking there might be a path here to much
great effecency and far less memory? But to be honest, I don't see it.
All we know is which path is maxQ and which is min activity. How do we
pick when there's nothing to bias us? But maybe, those two together
can be traced in one set of w values by modifying in one direction for
qMax and another for Amin? Maybe. But lets do two variables for now so
we can see what they do better.

Could today, Feb 9th, 2016, be Singularity day? Cool to think if might
be and I'm the only one in the world to know it. Am I a dreamer or
what?

[2:10 PM]

Got the new b and w learning coded and working. Leting a few tests
run to see how they converge.

Codded w learning as l =+ ... instead of l += and it took me far too
long to figure out why it wasn't working! That's a drawback of Python
not giving any sort of warning about that.

This new approach is looking awsome. So smooth in converging. So
simple in concept. Ever test I'm running seems to basically be
working.

But there are questions to be resolved.

For one, to keep it from martching to 0 and 1 and ending up with divide
bt zero problems I capped the b learning values to .001 and .999.
The w learning is not capped, but becaues of the caps on b leanring, w
learning ends up being capped as well as a side effect. At first the old
code was capping w learning since that was the single policy control, but
that didn't work correctly and created very odd side effects. Capping b
values doesn't seem to have that. But it yeilds interting results beucase
flow seems to converge on to 86 6 6 sort of rations with that .001 cap.

Maybe I shouldn't cap b either, and instead, cap the policy selection
of paths so that the least uses paths still get 1% flow. I bet that's
better. I'll have to try that.

This new techniqu keeps equal good, and equal bad paths fairly
balanced. But balancing between equal good paths is sort of sloppy and
lopsidded and shifts slowly from tilting one way to another. I wonder
if my b capping might be causing that now that I think about it?

Odd feedback issues with volume flows jumping up and down. The caping
volume at 1% minimum might stabalize that better?

Then there's the whole question of learning speeds. I have no idea
what's right. The system however seems to work, at all learning speeds
I've tested. I'll have to do more careflue testing to udnerstand the
blance and issues between all the paramaters, from halfFlie, to q, w,
and w learning speed.

Have to wonder if the discount time window of the learning speeds should
in fact match the half life of the ActivityTraces? Maybe I'll try using
AvtivityAvg to implment b and w values so they become discounted on the
same time based curve as activity levels and see what that does.

Maybe it's important that the last ones like w learning be fater than the
ones that in effect come before it to prevent osscilations in the system?
These are questions I'll just have to research to find answers to.

But so far, it's looking really good. And conceptually it's so much
simpler than what I've been struglgling with in the past evne though
it's conceptually the same ideas in many ways. The simplicty of it has
me just so excited.

[2:51 PM]

Created a 1% policy sorting cap. Turned b value caps off. Now I've
got 5 different tests running on my 4 processor machine using 100% cpu
just wainting to see how they converge and if there's any signs of the
colapse like problems. But so far, it's just cool  But I need more CPUs!

Maybe it's time to go to pizza? Haven't been there in a while
due to my insane level of work on this stuff.

[7:30 PM]

Watching net run. Play with how to prevent b*w suming to zero, and
how to do exporation. Set w to seek down to 0.00001 instead of zero so
it never goes to zero. That works well. It gives the RL learning the
priority when actitiy become heavilly biased in one direction.

Added forceExploration of .01 by default but it's a prameter that can
be tuned. It works fine at zero, except the net quickly gives up all use
of a bad link, and beuaes it's not being used, the qValues aren't being
updated. So it will never learn if things change. SO it's best to keep
some level of exploratrion. Maybe evne a very high level could be useful.

This alogirhtm works well over a wide range of different parameters.
including very small values of learning spseds like .01 range. But it
locks onto false ideas very quickly at those rates which is where the
forceExploration becomes impolrtant.

And of course, fast learning means fast forgetting. So a system that
needs to learn a wide varity of things and not forget them will have to
have slow learning.

Darn, it feels like it's all done. But of course it's not.

I don't know how well this percpetion system actually works. At all.

So now I have to create some more complex problems for the network to
try and solve. Darn, it's baout time to build in a system with feedback
to allow it to learn complex sequences!

But first, I'll need something more subtle, like learning to keep outputs
at a certain ratio maybe?

Should try the hall problem, but make it harder. Take away smell. Take
away hint moves? Use each but not the other?

Try different outputs to control the Beasty. If only one output is used,
will it learn to route all the correct pulses to the right place, etc.

This is working really well, and making all my old tests look like childs
play so I have to step up to harder tests now! But I'm bured out a bit
today with all this excitemetn of making this work so well!

Oh,and setting halfLife back to 10 seconds on the tests that were really
instant reward, wasn't a problem for thise beast. It could sovle it with
a 10 second half life as easy as with a 1 or .1 half life. But I need
to do mor etesting with that as well and see what happens.

The only think tricky so far, is getting it to balance shared outputs.
It does balance them, or more often, jump from one to the other over time,
but how fast it jumps and switches is all about the learning speeds and
how the diferent leanring speeds seem to want to interact.

Lots more testing needed.

Oh, I need to know if Python gives some sort of underly error when you
do x * .0001 until it unjderflows or whether it just turns to zero,
or something else? I'll test that now. The w weights I've no proteced
from that, but the b weights will march to zero. Don't want it to blow
up after running for a week or something!

Nope, no error. Just turned to 0.0 after a time. So b will go
all the way to zero in time.

================================================================================

2016-02-10 10:15 AM Wednesday

Yesterday

1) Solved AGI! That's what it felt like becuaes the network took such
a giantic leap forwrad in performance on the learning tests and such a
large leap downward in complexity of features. Switched from using b
to accumulate a win for q values, to measuring the odds of the path's
Q being in first place. Changed w, to not follow the target defined by
q values, but to work like b but for percpetion learning by estimating
the odds of a node being the lowest level ouptut. Made policy b*w
(no longer using a) and that made the network a killer at solving the
connect input X to output A type of problems. Which is all the problems
I had really created tests for so far becuase previous, the network was
struggling to solve them.

2) Played around with the end cases and ended up with using a small
non-zero target for the A learning to pevent it from every going to
zero. This gurantees the RL learning of b can overpower the percpetion
learning of w and force all flow to one path.

3) Added forceExploration by biasing at the policy level (just add a small
number to each factor before doing the probablistic picking of a route.

4) Started Experimenting with differnt learning rates and found this
network was extreemly flexible with learning rates. It would work well
with a large range of weights. But much more careful study still needed.

5) Cleaned up the code and deleted most the other stuff becuase this is
working so well I can't see any point in turning back.

6) Took time off to recover from hetic work. Pizza, move. and a goods
night sleep with the help of sleep drugs.

Today

In the shower this morning, I was able to do some thinking about what
I did yesterday and what this all means.

For one, I chagned policy from w*a to b*w and took out a. This means
the net has no ablity to do amplificaiton or gating. One signal has
no short term ablity to control another. All the network can do is
chopping up flows into smaller signals, and then sum signals together.
That really limits what can be "wired" with this network. Aka, what it
can learn. That will have to be expanded on.

I could do testing with this limited network, and see how well it
can perform at more complex divide and sum problems. Can it learn
to take 1/2 of signal and route it one output, and 1/2 to the other?
Can it learn to blance flow at a given split level? Like one input,
with the learning goal to divide 20% to A and 80% to B? If I change it
to multiplication network instead of sumation network, will it still be
able to learn that sort of fixed ratio balance? Important questions
to dig into and good idea for further testing.

Another network design thought. All outputs should probably be used.
If the network only needs two outputs, the network should probably
wire down to only two output nodes giving only two output q values for
the whole network. Learning to use "do nothing" outputs is probably
pointless. But that means that output controll also really needs to be
balance based. That is, the net is forced by design to send all pulses to
outputs, so the sum of the input pulses will always equal the sum of the
output pulses. So it's not possible for it to learn to lower all outputs
at the same time or riase the level of all outputs at the same time if
we have no "unused" outputs. So all control, needs to be relative. So
we use two outputs to control the evnironment, and the difference in
level between those, is ultimately the outputs control of that signal.

So this means what the net needs the power to learn, is relative flow.
This was really set as a requirement once I decided to force pulses
to flow all the way though signal and to not split. But it's just
starting to sink in a little deeper that this is what this style
network will always do. And that is exactly what I'm doing with
the new b and w values which represent relative flow rates.

b is long term behavior learning. So in a human like configuration
it would need to be very slow learning.

w is percpeiton learning, where the network learns from the data
to wire itself into a cat detectore. I could not turn off q
learning (ie just never send the network a reward), and test this
style networks pecpetion learning ability. That's another
set of tests to write and run.

But w, beuase it's inverse flow learning (large numbers for small
flows) it is logically something more like 1/f where f is long
term flow averages. (well 1-f really).  so bw is like b-fb. 
Hum. Intersting. 

So if we add amplification back into the system and make the policy
b*w*a, will that work? Will the w term correctly blance both the b
and a terms? So it becomeas ba-fba? I think it might. But I was
wondering if what I really needed was b*w and a*w making it a*b*w^2.
Nah, because if q learning is off, it just becomes 1 1 1, so it has no
effect on the balance.  then perception only learning would be b*w^2
which makes no sense. So just a*b*w I thikn would be correct if that's
the correct way to add multiplication into the system.

b is long term q learning. So it re-balacnes the net slowly over the long
term due to q leanring. A is long term flow balancing, so it tries to
wire the net for even long term balance (same amount of information
represented in every node (make all the mind pixels the same size
by default). And *a is short term, amplification and gating control
behavior so it becomes a multiplying network instead of a sumation
network.

Ah yes, I see it. If two input nodes X and Y, feed to a common output
node A, then the pulse volume from X to A is the number that node X
is sending to A. And when we multiply that by the output volume of A,
we get self feedback! Shit. But if the output of A is only the level
contributed by YA, then we have multipled X*Y to create the output value
of of A.

And to prevent feedback we must not count our own flow volume in the *a
bias, so as i was doing before, I eithr need to use the output node level,
and subtract my own flow rate, or, sum the flow rates of the others.

I think I need to just stop using the node activity levels since it's
not used for RL learning, and sum the activity of the other inputs to the
output to get the a term in b*w*a. I didn't like coding it as subtracting
out m own flow beuase it was senitive to bugs related to when I updated
the different activity traces.

So I need ot add the *a back in, and see how it effect learning of these
simple pulse routing prblems that don't need the *a to work!

Sounds like a plan. Lots of idea to test. Time for Starbucks morning
run however.

[11:18 AM]

Back. Time to dig into code. Want to add *a back first.

[11:40 AM]  Wow. A big problem. When the flow from other nodes along a
path is zero, that forces us to never send any data to that path. So in
an empty networkm, where on pulse sends to 3 outputs, all outputs have
zero flow from other paths, (so b*w should dominate) but it's all zero,
and it all translates to a 0 0 0. I can fix with a small non-zero bias
on all outputs. But that seems to be a sign that something is seriously
wrong with this approach. But what is wrong? Should the volume ratios
just bias, but not dominate the flow? What???? Why???

So, go back to basics. What the network needs, is the power to use one
signal to gate enough. Flow from X to A should gate and regulate Flow
from Y to A. And visa versa to make it symetical.

So when XA increases YA needs to increse. But if that YA increase triggers
more XA flow, we have a feedback effect that if not done correctly,
explodes and sends all data from X and Y to A.

So the feedback must be less than 1. Or, we go back to the old idea
from days ago, of using negative amplification! No, don't think we should
need that since we have the w learning doing that.

So if we had a two fan out net, with X feeding A and B, and Y feeding B
and C, and no RL learning, and assume the long term volume is balanced,
so w is 1 1 for both nodes. That leaves us with amounts to 1 1 bias
from the b*w terms. So if only X fows, it should equal blance to A
and B. And same for Y to B and C.  but of Y is flowing, then X starts
to increase, it should pull flow Y creating an amplication effect in B,
and a negative signal of X in C! That sounds great. So not only does X
effect B, it creates the same negative effect in C! And the long term w
values will bias all this to create long term equal actiivty in all nodes.
That's all great. Perfect.

So how to create that amplifiation without explosive feedback and without
turning the other flows to zero?

Here's a new thought. Instead of using the flow volume in B to figure
this out. X can use the flow volume of Y. Before it is divided between
B and C. That solves the feedback problem. And prevens me from having
to substract out my own flow. So X shares an ouptut with Y, so X uses
the volume of Y, to bias how much is sent to the shared node!

But if X and Y both feed to A and B in an X cofiguration net, then they
can't bias each other. Ah, but maybe, that's just what is right? An X
cofiguration net can't do anything? Well, excepet RL learning. The flow
blance will be 50/50 and the amplificaiton will be none! That sounds
valid. But maybe, the flow balance is really information maximizing,
and maybe, it would end up forcing all the flow x to a and y to b, uness
they were correlated and then the mix would be something different as
need to informaiton maximize! That shoulds possibly correct. And it
makes me wonder if my net is already doing that! Need to create na
X net test to find out what it does! But first, push forward on this
amplification and gating question.

Here's another thoughts. Y needs to bias it's flow based on the volume
of it's shared neighbors X and Z. But this bias should be based on the
neigbors MEAN flow. So when X and Z are are at their mean flow levels,
there's no bias. When X is above it mean flow, it should create postive
bias, and below it's mean flow, a negative bias. So the bias factor
is not just folow volume v, but volume minus mean, or a-m. So when
node X it at it's mean flow volume, it's not biasing it's neighbors.
It's like it's not there. But when volume goes up, it sucks data
from it's neighbors, and when volume goes down, it pushs pulses away.
Creating amplification. And gating. So we need to either calucate
the mean, or create an on line update that works as if it were working
on the mean. But since X is controlled by Y-Ym and Y is controlled by
X-Xm, then maybe this is just a big relative thing like the b values are
relative to which q value is the max at any instant, and the w values
are relative to which a values are the max at any instant.

But how do we regulate HOW MUCH our flow is biased by a change in the
neighbors volume level? Or do we not need to care, because the HOW MUCH
question is answered by the work of the w factor?

Fuck. Another question. Maybe the w learning really does not to
be biased by q learning so that the value of the b variable should
define the flow target, that the a variable is seeking? So maybe we
need to adjust the flow rates, by b, before we check to see which one
is currently low? Ah, but then we get back to the problem of nodes
fighting each other's targets. So lets not do that yet. But when we
figure out this amplification effect, maybe these three effects of
reward maximising, information maximising, and amplificaiton need to
be combined in a different way other than multiplication. The reason
it sounded posisble, is because n and w end up fighting each other now
and I had to add that small offest to w to make sure b "won" the fight.
If w was optimizing for the target set by b, there would no fighting
in this one node, but there would be fighting between nodes. Which is
worse. How it is now, all w learning is working together across the
entir enetwor, to try and create equal long term average flow volumes
everywhere so they don't fight, they all work together.

There might be interesting posiblites to adjust how b learning, w learning
and a learning all work together. Yeah, lets call this new amplification
effect a learning. But it might not be valid to call it "learning"
if it's only implemented in the policy. But if something needs to be
learned in relation to it, we could call it a learning. I think when all
this constant evolving design stablzies on the "perfect" solution, a big
remaning of concepts and terms and variable names might well be in order.

So if flow is amplifed by the activity level of neighnors, should we do
something like track more varibles like a new a variable for each link,
that works like b and w, to measure the probablity that a neighbor is
highest? Now, that might make no ense if a given neightnor is always
higher (like x is always higher than a dead neigbor). But we could
track relative levels, to create the mean and bias by current-mean.
ActivityAvg(ActivityTrace()) would track the mean.

But we need short term ActivtyTrace for short distance q learning and the
mean would always be a long term -- effectivly infinit horizon (which
is same for w learning). Ah, but wait. Driting q creates drifting b,
which creates drifting flow volumes as the net seeks to converge on
optimal beavior. And as behavior changes towards an optimal, we need
to w to forget to old information from the past about flow volumes were
like under the old policy. So the discount time horizong in fact maybe
should match the discount horizen we use for q learning?

Maybe w learning avergining needs to be done with an ActivityAvg so
it's perfectly in sync with the RL learning disocunt window? Would be
cool if that's the correct path because it elimites the need for the
wLearningSpeed variable making it dependent on halfLife. That should
very possible. Maybe all the long term learning needs to be done with
the same halfLife discount window. But, we can figure that out later.
Still need to answer how to do amplificaiton.

Side note, if feels as if, once we add in amplificaiton, the learning
code could be just about done!

DMAN SHIFT LOCK. I NEED TO FIGURE OUT HOW TO DISIBLE IT ON LINUX!

Ok, just realizing I had sort of forgotten something. B does not set
a balance between links. It sets a factor for ONE link. The balance
between links, is the relative B values for each.

So we could add in *(a-m) to each, and it would set a balance relative to
each output level. Ah, but (a-m) would need to be a factor of 1 when a-m
was zero. Ah, so the real answer might be *(1+a-m). So the flow volume
pulse and minus form the mean, creates a number that changes from above
and below 1! NO, WAIT, that math is wrong. I need it to vary from 0
to 1.0, like b and w, if the idea is to multiply these factors together.
With an average of .5. Or .33 for three nodes?

Ah, wait. I can't use activty of neighbnor.  q learning must have the
power to regulate the amplificaiton effect! How much of my data is sent
to output B, controlls how much I gate it! And how much my neighnor
sends to me, controls how much he gates me! So when B becomes a factor
of X and Y, the RL controls the realtive strength of the two signals
before they are combined. But what does that really mean?

A = X*f1 * Y*f2 where f1 is the b learning effect for XA and f2 is the
b learning effect for YA? I guess so.

I'm chasing these ideas because I'm trying to figure out what the balance
should be when my neighnor to the left is fighting with the neighnor
to the right for control of my flow. How should the relative strength
of each neighbor be balanced? I was thinking I might need to normize
a signal like with (x-m)/sd but i would not want to do that with the
actual flow volume volume becuase I don't want to noramzie OUT the RL
balance power. That would take away the power/work of RL to control
the relative stength of how XA controls YA flow. So, if we normalize,
that just means we normalize on the Y volume before it's adjusted by RL?
So that would say we DO want to look at the neighbor's current flow volume
instead of the link volume which is what we get after it's adjusted by
RL learning. But should we use before or after for w learning?

So, we need X-Xm as a starting point That's a -n to +n number for each
neighbor to show much much each is pulling or pushing pulses their way.
And a value of 0, means no bias towards or away from that path. Making
a dead path (0 volume), totally fine, just one that contibutes no
amplifiation bias to the flow. So nothing spcial should be needed to
account for that. So if my neighnor to the right, has 100 ties the
average vooume of my neighbor to the left, should it have 100 times the
bias power over my flow? Or should that power be mitigated by dividing
the difference by SD?

Ok, but before answering that question, lets assume we do mitigate it by
SD, that would make the formula (X-Xm)/SD as the bias factor. So how
do we use those three bias factors in our routing policy? We have to
map those three numbers, into a flow bias.

Ok, so we have this -inf 0 +inf numbers, that mostly hang out in the -1
0 +1 range (68% of the time), and we need to map them into a 0.0 0.5
1.0 range if they are to parallel the ba and w factors so we can just
multiple them all together. Aka, sigmod to the rescure. (I don't like
the cpu load of having to use sigmod, but lets just ignore that for now,
see if the design is good, and see if we can deal with the compuational
issue later if with simpler lower cost on-line update formula).

Ok, just had to verify I was right about sigmoid() it does map to the
0 to 1 range (not -1 to +1).

Ok, so this sounds correct.

b*w*sigmoid(X-Xm) -- don't normalize

or

b*w*sigmoid((X-Xm)/Xsd) -- normalize by standard deviation

or

b*w*sigmoid((X-Xm)^2/Xvar) -- normalize by variance and use activity
                              square as the amplificaiton factor?

So, to normalize by standard deviation or not? Or by variance?

In a untrained (by RL) network, all nodes will have approximatly
equal average activty levels. All sensory inputs should probably be
configured to be equal at the very start as well, and all outputs, will
be assumed equal as well. The median and SD of all nodes, in general,
should be the same. I think. At least for median. Not sure about SD.
But it seems to me, the network should work in a way that it would create
to equal SD by default. The RL learning, is all about how signals end
up being routed and combined, to form the right "algorithm" for the
beahviors needed. If flow is not the same, it's becuase the RL system
has decided the connection is a "bad" connection and should not be used
(cut from the wiring list) or added to the list as the "right" connection.

So when I ask if we should normalize by SD, I'm asking if the weak bad
link to the left, should have as much effect, on my flow, as the strong
GOOD link the right?

But the good link is good beucase RL thinks that the correct way to
"wire" the net. And the bad link is bad, beucaes that not the right way
to wire the net. So, by normalizing, we would be saying "I don't think RL
nows what it's talking about and we should factor out what RL was doing.

Ah, but again, We are not talking about link activity. We are talking
about node activity of a neighnor. And to the extent that ndoes are
unequal, is because the w learning didn't have the power to offset
RL learning. No, fuck, wait. We decided we did need to use node volume
didn't we? Because we want the nodes RL learning to control how important
that nodes is to our flow balance!

Is it odd I write "we" when talking to my past self as if I was talking
to a co-worker debating what "we" are doing? Makes me chuckle thinking
about that. As if I had created clones of myself and we are all woking
together on the same project. Well, if that's how my brain works,
I'll just have to run with it...

And again, if we normalize with SD we lose whatever learning the RL is
trying to do by cutting down volume to me shared node!

So, I think this all says we should NOT normalize the numbers by SD or
Variance. I guess we could use the sqare difference, and normalize by sd.
But I guess that's stupid. If we use the square difference shouldn't we
just normlize by sqrt()  Or is that stupid becuase it's the same as not
using the squre. Is (X-Xm)**2/SD something else that might be useful?
Probably not. Lets just go with sigmoid(X-Xm) for the LINK volume.

Oh, shit, it's not that easy!

Wait, I have to go fix this shift lock key..

The answer was: "setxkbmap -option ctrl:nocaps"

I'm free from that stupid shift lock now! So lets get back to AI work
[2:33 PM]

So I was saying, oh shit it's not that easy becuase for each output Node,
I have MULTIPLE links feeding it. So how the fuck should I track average
volume to get X-Xm. Could track in each link, then do X-Xm for each link.
Yeah, that's the ticket.

Otherwise, I would tack in my link, the averge of all the links feeding
to mmy shared node, and I could do that and it would result in the same
number of variabls. So either in my link, i track my own mean volume
and the other links use my tracking number, or I track the sum of the
other active in the other links, and I use that. Same result either way,
but tracking my own makes it conceputlaly far simpler.

So, every link tracks it's own mean. Guess I'll use an ActivityAvg on
my ActivityTrace. So I don't have to add yet another lerarning speed
variable.

And I just sum the activity-mean for all links feeding the shared node.
Then sigmoid() that. Ok, we have a plan. Lets code.

[4:48 PM] Coded the idea. Fixed bugs. Then started to study more.
Created TwoNodeOverlapTest with two nodes with fanout of 3sharing
one output. Turned RL off.

Without any balancing this would produce twice the volume in the common
node. But the w learning reduced the imbalance to maybe 5% ish range.

Which at first, I though was something wrong with the new code. The long
term A value was not .5, but rather, .49 ish. Which since this test had
dead constant and equal pulse inputs made no sense that would not be .5.
Whey is there a negative bias when the mean is subtracted from the volume
for  constant pulse?

Well, it's not constant flow in the link, which is what I was looking
at, becuse of the stocastic nature of route selection. But with a nice
even random flow, why the negative bias? The distribution has a skew.
Where is that comming from?

Ah, I tihnk maybe I get it. The atm value is set right after "at" is
updated. But then it gets sampled at a random time later when at-atm
is calculated for a:  At decays over time.  atm does not.  so atm will
be higher than it's correct average, when sampled at times other than
when it was updated! I could fix that by adding a dedaying getAvg()
to the ActivityAvg object and using that.

Let me do that, just to see if that edxplains the offset! Good learning
exercise.

[5:30 PM]  Ok, coded a getDecayedAvg() method and when I use that for
calucating the a values, the long term average is better. But instead
of the mean being a little low, it's now a little high. So without the
decay, the average of the at-atm was .like -.29 for a short 1 halfLife,
but with the use of the decayed avg that cnaghed to .01 high. I'll stuck
with the decayed avarege beuse it's close  But in truth, it shouldn't
make any difference. I don't think.

It would mean that the amplificaiton bias is always a little lower
than it should be. So if one neighbor was stronger, this effect would
bias the flow AWAY from the stronger side and towards the weaker side.
But if the bias is positive, it would bias the flow to the strong side
in average over time.

And sure enough, this test is showing that. The center node, which is
not fully balanced by the work of the w learning is noticbly stronger
with the use of better a values -- the a value doesn't help flaten it
more it in fact pushes it up a little.

And so, when I started looking a this, I thought it wasn't flat becuase
of a problem with the new a code. But it's just the old issue that I was
wondreing about yesterday, in that how I'm implemented the w clearning,
it will NEVER flaten out the flow, all it does is add a bias towards
flater. And this is probably good, becuaes otherwise if it made keep
adjusting w until it was flat, it would constantly fight b learning
attempt to move the flow in a different direction. So how it is no,
w learning allows it to flow, just not as much as b learning is trying
to adjust it. This might create problems if b learning is trying to
set flow volumes to a fixed ratio -- like if it needs to set a 2:1 flow
volume to maximize rewards.

But, hum, maybe the issueis that if the network needs to create 2:1 flow
volume, it should do it using multiple layers, where each layer biases
the flow a little close? That sounds pretty good for stablity. If
every layer had the power to hit 2:1, how would the network distbute
that responsiblity over 100 layers deep? Something like whichever layer
got it done first, would be the one that did the work  It seems to be,
that it would be more stable if each layer just added a little bit more
to the offset and that the q values would distrbute in size from one
end to the next.

Ok, so, in a single node, RL can't push the flow all the way to max
unless the q values are a "clear win". If it's a partial win, then one
node only gets the job partially done. And same for flow balancing.
Nodes don't try to totally balance, they only try to push the system to
greater blance one step at a time!

And this new amplification efrfect, probably doesn't matter if it's
biased one way or another, beuse the rest of the net will just use this
as it's base "behavior" that it's traning.

Ok, lets then move forward assuming this is all "good" and not a problem.

[6:47 PM]  Been playing and studing. With two overlapped nodes, mixing
a strong constant signal with a weak one, causes the strong to totally
push the weak one out of the shared link! If the two can't balance the
strong gets nearly 100% of the shared link! So the system does it's
best to blance which is to give the weak single two nodes, and the
strong signal 3 nodes! So the singals don't mix at all. Though that
looked broken at first, I think it's actully good, becuase it will then
get the nodes closer balanced, and in the next level it will allow all
the signal to mix!

one node has a frequency of 1 and the other 1.8 and that made the w
balance in the first 48 48 2, and in the second 32, 33, 33.

That would be a .56 output over all 5 outputs if it were balanced.
But it couldn't so 1 divided by 2 made a .5 output for the first 2 And
1.8/3 was a .6 output for the last 3 nodes.

But if the two can balance, then the shared node will be whatever mix
of the two signal is needed to make that balance work. I think. Let me
test that.

So I set frequency to 1 and 1.25. That's 2.25 total for 5 outputs
means it will balance at .45 hz each.  .45*2 is .90 so that will mean
.10 from the first port to the center, and .35 from the second one to
the center and .45 to each would be balanced. But w balancing can't
perfectly blance it, so it will be less balanced than that and the
center flow will be something between the two others on the output.
So lets wait for it to converge and see where it lands..

Ah, no, the middle shared node won't be in the middle of the two.
It will always be higher than the other two!  w balancing doesn't keep
adjusting. I have to get used to that. It reports on how often the middle
is higher than the others. So the high side is converging on what seems
to be about 25 37 37, which means the middle is the low output only 25%
of the time and the other two are the low output 37% of the time.

So w balancing can never fully balance the output -- it only gets it
closer than what it would have been without that output balancing.

And the numbers look to be 42 42 15 from the first, and 25 37 37 from
the second.

By I think, that the more noise there is in the system, it will be less
balanced maybe? Maybe not? Still works out to be more balanced than it
could be without the w blancing and it seems to be a very stable approach.

Yeah, so extending to a 3 layer network where data mixes fully, and we
see lots of good balancing, with less adjustemnts needed in the last
layer than the one before.

So, this net is meant to be full of well balanced data unless RL tells
it to stronly unbalance. And weak unblanced RL should just effect the
"color" of the mix. But I need to test that more.  using colors to show
the mixing would be a neat visual indication of how it works! One day.

Ok, now that I've explored the steady state mixing. I need to pulse the in
input up and down and frequence and see how the net responds. I was doing
that, but then backed off to understand the steady state mixing first.

So, looking at the one input being held constant, and the other
pulsing up and down I see what seems to be an issue. I've thought of
the activity level as a short term intentanious measure of the nodes
current pulse rate. So in thinking about how the interaction will work
to reate this "gating" effect, I was thinking of a very fast acting
effect. But I'm using the activity trace to define activity level but
also as a long term memory horizen for reward learning. It can't really
be both. If the activity trace halfLife is long, then the activity level
is very slow moving. The side that is sending pulses, is fast moving by
definitoin (it's a pulse) but the activity trace of the other side, is
very slow moving. I need to let this run and see where the paramaters
equlize at and how it's acting when it does that. But the problem is
that the product effect relies on the activity trace, and that is very
slow moving. I think maybe I need to either use a very fast moving
activity trace to implment the product, or not use an activity trace
at all, and use something really fast, like the time for the last pulse
sent from the other side?

[7:32 PM]  Yeah. When P2 sends a flood of new pulses to the center
link, the other side should respond instantly with a flood of pluses to
match. But the response from the other side is controlled by the speed
of the halfLife -- so it's a very slow shift of pulses in response. Not
at all what I was thinking was needed here! I need something else!
Slowing down the response to the speed of the short term memory is of
no use at all!

[10:13 PM]  Ok, I've fucked around with how to code the a stuff for
hours now. Still strguggling to understand what my test should actully be
doing. The switch to lastGap as a super short term measure of activity,
doesn't seem to have helped. I think I have a fundamental issue with
how this is working by mapping activity diferences into 0 to 1 values.
The problem is if the other two paths have a value of .5, and one path
is swinning from 0 to 1 (max range) all it can do, is either turn off
the feed to the common node, or increase it 50%. (for a 3 node fan out).
Because 1 .50 .50 is really just .50 .25 .25 distribution. It should
have the power to sweep from 0 50 50, to 1 0 0 but it doesn't. So how
the three a values interact is not what I thought I was getting.

And the automatic adjusting by w seems to undo what I was trying to do
with with a values. But maybe it's just becasue I'm not really setting
the three a values correctly or not using them correctly.

Thought. -inf to +inf with a 0 "middle" can be turned into 0 to +inf
with a 1 "middle" with exp(). That seems closer to what I was looking
for because it makes the "idle" inputs 1, and the one that swings a
lot go from near 0 to near infinity giving it plenty of power to swing
the pulses.

But, there is again, the unanswered question about how much volume
should swing the pulses by how much? They will have to be normalized
by some measure. If not by the flow of each link, then by some measure
of the combined averges of flows or something. Let me try a quick
unadjusted exp(). (it's about time to stop for the day and pick this up
again tomorrow)

[11:03 PM] Ok, time to stop. Hacked in some lastGapAvg2 shit, and
siwtched to exp() without much thought. Getting some fun results,
but not correct.

So here's a thought. The combined signal should be amplifeid as large
as possible but keep the average signal the same as the rest, without
cliping. It should be a 0 to N strength signal.

So if one side is doing a 1 to 3 range signal, and the other is doing 4 to
8, the product is 4 to 12, but that should be normalized down to 0 to 8.
So the combined effects of the amplificaiton effect and the q blancing
effect, should get the signal all the way down to zero when the two side
signals are at their typical min.

That's how it should be normalized -- we want to create maximum stenth
signals to drive behavior with! So, probably, the multiply effect
is noramized to crate the zero level, and then the actvity balance w
controls the high end amplitued. Or the average? Yeah whatever.

Ok, tomorros I'll figure out if that  makes sense and how to do it.

================================================================================

2016-02-11 8:34 AM Thursday

Yesterday

1) Caps Lock is turned off! Bugged me enough to find out how to turn
it of. I kept hitting it and doing odd shit in vim.

2) Worked all day on trying to add back in amplification between nodes.
Decided I needed to use flow to node from all neighbors, and created
mean of activity to measure relative flow. Then decided activity trace
was too long an averge. So I worked on measuring gaps of last pulse.
Didn't work much better. Spent hours watching experiments run trying
to understand what I actually needed to do. Didn't get anything working
to the extent I felt it was correct.

3) Got better understadning of what my w balancing is doing, and what
it can and can't do. It's sort of odd becuase it can't balance flow.
It only pushes flow to be more blanced than it would be without the
adjustment.

4) Created TwoNodeOverlapTest() to focus on testing what happens when
the flow of two nodes do when they interact.

5) Did some hacks to allow disabling auto rescale of histogram so I
could better see how things worked when pulse input volumes changed on
this testing.

6) Thoughts from the end of the day -- the interaction should attempt to
MAXIMIZE varriance in the common signal. Which means driving DOWN the
signal to near zero flow volume even if the two neboring signals don't
go to zero. So amplification should focuse on making that zero happen
and let the auto balance worry about how large the max is.

7) Thoughts from bed. I should try using the last pulse time from the
other paths. Not the last gap time I'm using now. So find the last
pulse sent to the common node, by the others paths, igorning anything
I send to it. So it's the max of the lastClock from the neighbors.
That's simple to track, and I thnk it might be the best thing.

Today

Ok, continuing on the two big ideas from yesterady -- amplification
should make the common signal vary from near 0 to Max frequency.
We should try to maximize the variance of gap times, or frequency in
the newly created signal.

So with a fan-out of 3, this means I'll have the last clock for every
path. (or None from a path with no other activity). And I need to bias
activty, to the shortest gap to create amplification.

I could track a mean last clock gap, and compare to that.

For a path with no other activity, should I use my own activity? No,
that would be self amplification feedback.

Ok, but how to map lastClockGap to the correct bias?

exp(-lastGap) might be interesting. A gap of 0 would 1. Larger gaps
would be smaller and smaller numbers.  exp(mean-lastGap) would be 1 for
the mean, larger than 1 for small gap, and smaller than 1 for large gap.
I think that's the last thing I coded last night. Except using preveious
gap instead of current gap to last pulse. Let me check, and switch to
last gap.

[12:13 PM]

Well, still not working. Sort of getting there. Used last gap and
created lastGapAvg, and lastGapVar to calculate the varriance around
the mean, and normalized the offset as (gap-mean)/SD. That gets
us to the ball park. After balancing, the center combined signal
shows higher activity then the other two when both are in their
high state, and lower activity than other two, when they are in
their low state. But yet, the a values, doesn't seem to be
as consistently lower than 1.0 as it should be.

tried to change to freq instead of gap, but that's just showing
crazy. And creates a problem for gaps of zero.

There's a problem here that if average gap is 10, then the smaller
gaps are all 0 to 10, but the larger gaps can go much higher, creating
a very lopsided distribution. This favors a becoming larger than 1
and pushing extra volume on the high side, but hinders the low side
and reducing volume to near zero when both inputs are below average.
And then w balancing only pushes volumes higher. I think. I want to
see the low side dip down to zero, and it's not there. It's like 10%
lower than both inputs, so that's a step in the right dirction but not
the right answer yet. Have to fighure out the complex math of how the
two sides ineract and effect each other, while at the same time how the
w balancing interacts as well.

There might be an issue at work since the two other paths, are set to 1.0
meaning average flow. If they were also interacting with other signals,
the effects of those other signals would be sucking volume out of the
shared side when they were high. Maybe with signals interacting on both
sides, the middle node behavior would look better? I wonder if I cheat
and use a different value othre than 1.0 for the innactive node to
see what I can force?

[12:34 PM]  Turned wLearning off! This allows me to see what the
policy code is doing before it's adjusted by the wLearning! I need
to make the policy code do the right thing on it's own, without
the help of w learning to balance it. And then see if wlearning messes
up what it was doing without it!

So, get the policy multiplication effect working correctly, then
turn w learning back on and see how it interacts.

[2:29 PM] back from lunch at Jerry's with Pig and Julie in Reston.

Go to understand what this code is doing. Need debug so I can see
numbers!

[4:33 PM] Found that that using the lastClock was a mistake. If the one
side was running at 3 hz and the slow side at 1 hz, the fast side would
sample the last clock 3 times of the 1 hz, and see gap times of .33 .66
and 1 (at most) making it look like an average gap of .66 or smaller --
nothing at all like the real gap of 1hz. But maybe there could be a
use for that effect?

Switched to remembering the last actual gap, and throw in some kludges
to deal with zero gap. Switched to avg frequency instead of gap times.

Then realized what it was doing, was setting the a values to 1 1 4 when
the frequency was high, and 1 1 1/4 when low. Seems right until you
combine those two together from both sides and end up with 1 1 8 1 1
vs 1 1 1/2 1 1 in flow volume. And that's BEFORE the system starts to
increase flow due to feedback effects.

So I went back to sigmoid() instead of exp(). Exp produces 1/4, 1, 4,
where as sigmoid() produces .5-x .5 .5+x so all values stay below 1 for
two combined flows. But it would be 1.5 for 3 combined flows that that
same ratio.

With wLearning turned off, the sigmoid() version produced a fairly flat
long term output with amplification effect  causing the common node to
be higer than the two side, at the max, and lower than the two sides at
the min. But turning wLeanring on, pushed the middle higher, causing
it to have slightly more flow than the rest.

And this version of the code is icky feeling and complex and requires
sigmoid() which is expensive.

Not feeling it in the least, but it's in the ball park of what I
envisioned as needed. So lets do some testig with this and see what
happens.

[6:19 PM] Movie tonight! Deadpool Struggled to see what was throwing an
exception. It was a zero variance. Don't know why it was zero. Don't care
at the moment. But I typed Avg instead of Var as I tried to debug it
and didn't see the typo. And Avg was not zero so I couldn't see what
I was cauing a divide by zero error! Took me too long to find it.

Finally get SimpleTest to run without throwing the excpetion by
just preventing the /0. Notice that with the new amplify effect the
secondpulse tends to get sucked into the output with the one being
rewarded. That second pulse gets punished. Not sure why it gets
sucked in. But not every time. Having signals get sucked together
seems a side effect of this amplifcation feature I've sturggled with
for two days. A strong good signal forms due to RL, and a bad signal
next to it, just gets sucked in!

Maybe that's not bad if the other signal wasn't "good" for something else?
Even if it cause the good signal to digrage in value?

I guess that's a good question. If the net combines two signals and
the rewards drop, how is it going to tell that?

The bad signal ends up "stealing" points from the good signal, so it's
rewarded for being a parasite.

I need to experiment using different reward configurations and see how
that effects the issue? If the bad is really bad, and cuases massive
negative rewards, surly it will destroy the link and the good signal
will find a round around?

But also, this is with the 3 fan out toplogy, which has serious crossing
issues (links have to come close to cross unless they wrap around).

Yeah, I need to understand the limits of what I have here better.

[6:35

[10:05 PM] Back from Movie.

Had an interesting thought. I've belived the nodes needed to create
amplification which is why I've worked on this multiplication form of
policy. But, maybe they don't! To create complex behavior I need to
add the global feedback so the network can learn to pick beahvior based
on both recent past actions and sensory inputs. Feedback like that
has the potential to create amplification! A small signal can in theor
trigger shifts in the feedback state to trigger large chagnes in
behavior. So maybe this messy crap of amplification I've been struggling
woith for two days (and years in the past), isn't needed at all!

Maybe all that is needed, is the RL and perception, and the rest is
handled by the feedback!

I'm had this idea of using feedback for 20 years and I've never built
it becuae I never had percpetion working. But now, what I have might
be good enough to make it work! So that's got to be on the todo list
now -- create a feedback network and see if it can learn, and what
it can learn!

I'm not realy happy with the current form of w learning however. Though
I think the theory is basically correct (activity balance) it seems to have
liminations that make me wonder if there's not a better way to implment
it. It is clean and simple as it is, an it does keep the network balanced
though many layers, so it might be good enough even if there is a way 
to do it better.

--

And when I chnage TwoNodeOverplay to a 4 layer deep network, it balances
the 7 outputs at ratios of 14.4%, 14.4% 14.5% 14.1% 14.1% 14.3% 14.5%
ish It was changing slightly as I coppied.

That's with two inputs changing frequency and the amplifiation effect
at play shifting stuff around.

I think it's time to call it a night, and watch a little TV. Figure out
in the morning what direction to head off next. There are lots of
different things that need to be tested and explored now.

Oh, other idea on amplification. I think what the net really needs to do
is to function correctly as a bayes network where one set of believe
in one row, will update the beliefs in the next row based on dependencies
between rows. Each row should be thought of as a probablity distribution,
which has the power to update the probablity distrbituion of the next
row. I should spend some time trying to understand what formula is correct
of probablisty distribution updates, and see if I can justify to myself
that the net is working that way and whether any type of multiplication
of activity levels at a node is needed or if the smple pulse sorting
suming effect is enough? If I can convience myself that the system funcations
as a valid form of belive probigation, I can try to throw out the idea
that this sort of Multiplication is needed. That would be a good thing
to dig into tomorrow.

[10:47 PM]

================================================================================

2016-02-12 10:30? AM Friday

Yesterday

1) Fought with creating the amplification policy. Tried exp(). Went back
to sigmod(). Turned wLeanring off so I could see clearly what the policy
was doing on it's own. Ended up with one that seems to sort of work,
but it's kludge, expensive in CPU and questionable in value.

2) Wondered if amplification policy is even needed. Maybe it can happen
through network feedback instead?

Today

Well, I need to get this figured out. Want to explore more on this old
idea that the network really is a form of bayesian updates?

Ok, so each pulse defines the value of a node as it passes through, and
then gets distributed to the downstream nodes by by the ratio defined
by b*w*?

So we can think of this system operating at the level of numbers
representing flow volumes in pulses per second.  b abd w learning is
intented to be long term and slow, so they define the assocations betwen
nodes, but don't participate in the dynamic operation of the system.
They define the realtionships between nodes, but just end up with fixed
ratios that define a fixed outgoing flow ratio.

So, given a set of flow volumes on one level. The next level is nothing
more than a straight linear mapping from the values of one level, to the
next. Without something more than the b and w terms, there is no dynamic
control at work here. There is no point to having multiple levels becuase
what you can do in 10 levels of linear maps you can reduce down to one.

The value of the nodes, have zero effect on how pulses flow.

Seems like it most certainly needs more.

So what if we think of b*w as a probablity distribution, of a belief over
other beliefs? So the first node and it's flow volume, represents the
systems strength of belief in whatever that node represents. And the
outgoing flow ratios, define the probablities between the beliefs of
one layer and the next? I don't really know what I'm saying.

Consider full layer interconnect. So each node in one layer, fans out
to every node in the next. The fan out is a probablity distribuiton.
If the node was 0/1 valued, we would multiple that probablity distribution
as defined by the fan-out weights, against the current values of the
outgoing layer, then re-normalize everything. Which means, that the
stronger outgoing connecdts would end up increasing the nodes they
connected to and the weaker outgoing connections would DECREASE the
value of the nodes they connected to.

But hey, if we don't send a node enough pulses, it's value DOES decrease!
Interesting. So the effect of constant decay can become a stand in
for the normalizing step? So if all the output nodes are running at
10 pulses per second, and a new node feeds it a pattern of pulses,
some can goup and others can fall.

Ah, and the pulse sorting aspect of this net kicks in. So if the flow
shifts, at a lower level, we are moving pulses form one input node
to another. So we take away the fan out effect of the one and replace
it with the fan out effect of the second.

So one input node say has a fan out of 1 1 2, and we switch a flow
of pulses form that one, to a second node with a fan out of 2 1 1,
That makes the last output node fall, and the first rise but the middle
stay the same. Every pulse we switch, has a little more effect on the
output nodes.

This almost works, but I'm sure there's something missing.

I feel, that the current state of the nodes we send to, must bias the
flow, in a way we are not doing currently with b*w alone. Shit, should it
just be b*w*a where a is the current output level of the downstream nodes?

Ok, so shit, I'm comming right back around to where I started of thinking
I need multiplication when two nodes intersect.

So if one input node sends a value of 4 to a node, and the other common
input node sends a value of 2, then the result needs to be 2*4, BUT
re-normalized across all the output nodes and all the inputs.

Ah, so fuck. Maynbe this is nothign more than a scaling roblem.

We can't take a ratio like .5 .25 .25, and just multiply by 10 10 20
(output pulse rates) because 10 10 20 is not normalized.

Ah, fuck, I see.  it's fine to do that, as long as only ONE of the factors
we multiply together are unnormalized. Multiplying a true (sums to 1)
probablity distribution by an unnormalized set, and then normalizing,
produces the exact same result as if we normalize before the multiply. But
if two numbers are unnormalzied, we can't multiplity them together, and
then normalize after, and get the same result. Let me do some checks
to verity this is true!

Shit, I was wrong! Basic math and I was wrong! You can normalize numbers
before or after takign a product. It doesn't change the results! Cool.
Just taking products of distributions produces valid distributiosn even
if nothing is normalized!

So, OMG. Just b*w*a is a valid probablity distribution product without
ever having to normalize. But if they are not normalized, we must
understand their normalized version is what defiens the probablity
distribution that is in effect.

I'm kinda embarsed I didn't understand this already.

The fact that we only have a limited number of pulses to send at each
step, keeps the whole system nomalized, to the number of puses per second
we are sending through it!

If the number of pulses per second is constant at input, that totalnumber,
divided by the number of nodes at each level, becomes the normalized
sum of that level!

And when the number of pulses per second drops at the input, it just
redefines the current normal level across the entire network!

There's never any need to re-normalize at any point!

So b*w*a is basically correct to turn this into a bayesian probablity
belief probigation network. All the complex shit I was doing to factor
out my own flow from the output net is wrong! right?

Yes, exactly. If I probagate a 1 2 2 probablity due to one pulse, and
it adjusts the downstream PD, and I do it a second time, I do NOT want
to factor out my own effect from the first time!

Beuase I want it to MULTIPLY.  so 2*2*2. So damn, that makes it easy.

But, now I need to more carefully think if b and w are in fact correct
and useful in this context!

But first, I'm going to throw out all this complex shit I've been playing
with for two days becuase it sucks big time!

So, with all all the postive feedback this creates, I now need
to think of a and b are correct.  'a' needs to balance everything to
keep the system stable. Can it do it?

[12:24 PM] Shit, I just wrote bunch of comments, and by not watching
my flying fingers, I deleted them all and lost them. I should take
the time to beeter understand the undo system in VIM. Later.

But what I was writting about is the that I understand the importance of
wLearning not fully faltening the output, and instead, having a "spring"
on the learning factor that pulls w back to no-ajustment level. It's
becaues when the node is in a loop with the top wrapping around to
the bottom, it creates too many degres of freedom for the system to
balance. It can balance the flow by creating 10 80, 10 80, 10 80, or it
can balance it with 50 50, 50 50, 50 50, etc. So it would cuase the net
to constantly drift if wLearning had the power to flatten the balance.
I could remove the loop, and then it would only have one solutio, but
it would still be highly unstable with the nodes on the end "tied down"
and the ones in the middle being less and less stable. Far beter to
make them all more stable, by making wLeanring "springy" as it is now!

So no layer can totaly flaten a non-flat distribution from the previous
layer, but each gets a little beter.

The "spring" effect also no doubt helps in it's fight with b learning.
The w learning can't undo all that b learning is trying to do. It only
bends it back a bit towards flater. Again, creating staiblity as the
two forces work against each other, instead of a constant unstable back
and forth battel (b gets higher, w gets higer to compensate, b gets
higer, etc).

[12:58]  If suddenly feels like the net is done. But yet, I'm not
excited about it becuase I don't have a clue if it works. Running a long
NodeOverlapTest which has two inputs slowly shifting frequencies, and 4
layers of links expanding to 7 wide. Waiting to see if the slow wlearning
can balance it and how it acts when it is balanced. I have accumulated row
conters resetting at every 100 hours of simulated time to see how it goes.

Need to keep trying my different tests with this version of the net to
see how it works and what it can do and can't do.

Can it learn to use a weak signal to make strong changes in outputs?
Can it create negative effects? Aka generating a NOT X signal as output
from an X signal as input?

Would love to see that work! So feed it like all random inputs, but
train it to make outputs alternate (postive and negative) values in
different outputs so some are the amplified version of the x input,
and the rest are the matching NOT X versions.

The net needs the power to learn to do that. I'll try the tests I
already have written to see how this version responds, and then if
nothing strikes my eye to dig into, Ill move on to new tests!

[1:21 PM]  And here's the next big question! Does this do percpetion
learning and map different input configurations into different output
paterns? Does it have memory for sequences?

[1:36 PM]  So with this *a in effect, the speed at which node volume
levels change controls how fast the network can dynamicallyh shift.
So I'm running some tests now with long 10000 halfLife (2.7 hours).
So that might be sloth like in it's ablity to sense and resonse to change!
OR, might the a adjustments rescale all that so that the speed of half
life change only ends up being a units issue and not an actualy real
time based effect? Darn, I don't know. It might not be a speed
issue as much as a memory issue. How much what happaned 30 seconds
ago effects what it does now?

So, this opens the door for endless tests. If the network can learn to
respond based on what it saw 10 seconds ago, we can test it's memory!
So, we train it to resond to a cue, like a signle in put that measn
"answer now". And we train it to send out the answer, which responds to
what it saw on another input, in 10 seconds in the past. Then we test
to see how that memory works for different halfLife settings, and sizes
of nets, and learning speeds etc. And how long it takes to train the net
for different tests at different times into the past, etc etc. All great
ideas. If the network can even learn this beahvior in the first place!

[2:11 PM]  Running lots of tests to see how things work with this new net.

New/old thought, the w learning really shouldn't fight the b learning.
If th b learning says it wants 60 30 10 output, that's what the node
should do if there is no conflicting shared nodes to upset that. So what
the w learning should do, is use the b learning as the taget for what
it will try to blance to. But this won't create node to node conflict
with different tagets, becaues the w learning is "springy" and only
does what it can! So when twoside by side nodes are targeting different
ratios for the b learning, the w won't fight each other to the death,
they will just get as close as they can to the desired b targets with
equal spring blance across each other! I had rejected this in the recent
path because it would cause fighting with the qValues at the link level.
But with the new undrstanding of wlearning being "springy" I understand
this is ok now!

Yes, that sounds correct! I'll have to change w learning to use b ratios
as it's target (yet again). But all my tests are not using RL at the
moment, so I won't stop them to make changes yet. I'll put this on the
todo list for later.

Now I'm really starting to feel good about the design the system is
gravating to. But I'll have to make sure the q leanring doesn't start to
fight each other. Or do I? So lets say X goes to ABC and Y goes to BCD.
There might be a fight over BC. Both X and Y might be trying to send
puless there for different reason. If the net isn't large enough to allow
X and Y to get out of each other's way, there really is no solution to
the problem. But otherwise, one of the two, should find a different
path for their pulses.

I wonder if large fan outs are actually good or not? More options of
how to route and mix pulses, but maybe slower learning? And of course
larger computation per layer. This would have to be tested with learning
ability and cpu load and memory trade offs.

[2:44] OMG, it's working so well! [see screenshot below]  The wLearning
most certaonly can learn to balance this *a network. Here's a simple
two node test with the row output counts over time.

Notice the middle overlap node only gets to 25% over 19% for the others.
Thats the limits of the "spring" power for a single node. The weights
are 442, so it's not like it pushed the weights to their max, that's
just the point where the sping power balances out with the imblance!
Deeper nets have less imbalance!

-------------------------------------------------------------------------------------------------
XNET Fri Feb 12 14:09:23 2016  RunTime: 0:31:40  Real-Time Simulation speed=1500X

p1 freq: 2.00341335589 p2 freq: 2.01758917383
Two Node Overlap Test row1: -1, row5: +1 7 x 2  time: 791:56:40.0000  PPS  6032  RPS 0.000 0.000 
Explore: 1.00%  Q: 0.00001000  B: 0.00001000  W: 0.00001000  halfLife: 100.0
  .         |  . | 
  .         |  4o|    273 =========
 20o442.133 |  7o|    741 =======================
  .         | 15o|   1621 =================================================
 20o244.324 |  5o|    382 ============
  .         | 10o|   1005 ===============================
  .         |  . | 

row counts are:
    249732 19.0%     248290 18.9%     335343 25.5%     246331 18.7%     236424 18.0%
    270175 18.8%     262234 18.3%     357814 25.0%     270215 18.8%     273509 19.1%
    264697 18.4%     263742 18.4%     362588 25.2%     271596 18.9%     273604 19.1%
    275724 19.1%     267498 18.5%     363365 25.1%     262965 18.2%     276225 19.1%
    270967 19.0%     258169 18.1%     363515 25.4%     264348 18.5%     272330 19.1%
    271392 18.6%     271991 18.6%     364665 25.0%     272449 18.7%     279841 19.2%
    272690 18.7%     272041 18.7%     349843 24.0%     276025 18.9%     286169 19.6%
    247431 17.4%     243043 17.1%     427945 30.1%     247172 17.4%     257145 18.1%
-------------------------------------------------------------------------------------------------

So, I took a time laps of the beast with my iPhone! The fucking thing
LOOKS ALIVE! Adding the *a has made the net come to life becuse it gives
it dynamic memory and action! I took the shot above in a state of great
difference just to demonstate how much the outputs are shifting, dispite
the fact that the average levels are so well balanced! And they don't
just shift randomly and instantly, they grow and shink dynamically over
time in response to the dynamics of the network! It's so cool!

This is a short halfLife of 100 seconds. My other tests running have a
10000 half life. I don't yet know if the speed of the dynamic shifting
is controlled by the half life, or if it's more a function of the systems
in puts, or what? The wLearning is very slow so shifts in wLearning
shouldn't be doing it. I don't think. But maybe very small shifts in
w values are having buterfly effects on the system? I'll have to let
it train, then freze w values and see how the net acts? Does it keep
acting the same? Does it become unstable and fall to one side without
the constant juggling of the w values?

After two days of messing with grows "amplification" code I was feeling
depression setting in. That is, I was feeling only dedicated to the
project and anxious to get to work, vs absoltyely manic like I have been.
But now that I think I solved the amplifcation prblem correctl by
just adding *a back into the policy, I'm getting myself hyper again!

Honeslty, the only think on the to be fixed list at the moment, is
changin wLearnign to target the B ratio instead of using b*w*a. And
either that works, as I hope, or it doesn't, and I then figure out why
and go back to b*w*a or I fell the thing is done and working! I feel
lke this could be it! It's clean. It's simple. And I think it has all
the feiatures I've  been trying to build into these networks for
the past 30 years! 

Other than endless testing to see what this can do. The next direction
(other than building robots -- I want so BAD to build robots), is to
see if I can translate these pulse ideas into value caluating network.
The pulses are cool, but if it can be directl translated to a value
calucating network -- or even beter yet, do both at the same time, that
would be super cool -- so that I can inject say frames of pixel data into
the network, without costly conversion to pulse signals first. The idea
would be to count a value input like "112" as if it were 112 pulses being
sent since the last input. And them filter the data through the net.
It would fan out in the same way 112 pulses would fan out. So it it
could be computed layer by layer as we do in typical value based networks.

For applications like learning to play video games from a video input
it would be ideal if I didn't have to convert it to pulse data.

And at first thought, it seems both possible and straight forward.
The value fans out according to b*w*q. The a in the next level is updated
by the amount of the value. The "number of pulse" count would pass
though to each layer, and update it, and come out. The "pulse count"
woldn't have to be an int. It could be just a real. But it would have
to sum to one as it was divided. Or might not need to be normalized
at all but conceptually that is what would be happening (no way, two
puses would have a very different effect on the activity trace from one,
so no, it would most certaonly have to be renormalized at each level to
keep the sum constant. I see no reason at all this can't work like that.
So the net can mix pulses and value updats just as easy. Give 0 inputs
to the pulse inputs where you have no current pulses.

And fuck, we can directly feed data streams of different frequencies into
the net as well. A video feed at one rate cold be sent in, and an audio
feed at a different rate could be send in as packets at a different rate.
Each is just feed in as it's received. And if the whole net is being
recomputed at eacah stop, we just feed 0 values in for inputs that don't
have data at that time step! I think that works.

And pulses are just fed in as data packets of value 1. So if a GPU
can compute this network in parallel super fast, it can be built that
way instead of as a pulse sorting net. And note also, with the value
calucation, there is no stocastic splitting needed (I don't belive).
So all that computational overhad is slaved even

This is getting SO cool!

So, this feels like it's no longer a future reserach direction. It feels
like it's 99% resolved already!

BUT -- thought of one more unresloved issue. That's the issue of secondary
reiforments and TD updates. Still don't know what's right with that.
So, that needs resolution as well. But it seems a minor issue. That
should resolve itself as I see what the network can and can't learn.

This is turning out to be a great Friday! from being stuck in the
morning with no clear direciton out of the mud, to feeling like the
network could be basically done by the end of the day today! [3:20 PM]

[3:56 PM]  Plaing with having wlearning target b ratios.

Seems too much. First thing it does is make Rl learning effects slow
becuaes it puts b learning and w learning in a chain. B learns first,
then w catchs up. That adds a time dealy which can be offset by
increasing w learning speed.

Tried the idea of both putting b factor into policy, AND have w learning
seek the ratio!  that's like double speed q learning. It caused the
netowrk to very quickly pick the wrong solution and lock onto it.
I don't know if it would have freed itself if I gave it enough time,
but the q values were wrong, and the small 1% forceExplore would have
taken a long to time to resolve the invalid qValues.

Another idea -- have w learning seek something like half the b ratio
values. That can be done by adding a constant to the b values before
using them as a ratio target. So adjust b from the normal range of
0 to 1, to a range of something like .25 to .75 before using that as
the target for wLearning. So b could be 0 0 100, but the targe w would
train for would be just 25 25 75. That's like w learning does half the
work of RL, and b does the rest?

I'm not sure if any of this is worth it. or not?

If the b ratio is like 20 20 60 now, the system will not actually
set the output ratio to 20 20 60. It will be less than that due to
w learning trying to reduce it. (which creates a level of conservative
extra exploration) But if the enviornment NEEDS the ratio to be set to 20
20 60, what will happen? But if I do both, i see why that's a problem.
The q values might overlap at 20 20 60, but if both n and w are doing q
learning, the flow ratios will advance do maybe 10 10 80 and highly reeuce
the amount of exploration which could lead to the syste locking onto a
false truth due to a lack of exploration. So if we error, it's probably
wise to error on the side of more exploration, which is what the system
was doing before I started to try and make w values seek to duplicate
the output ratio.  at the 10 10 80 setting, and force q to get better

I'm leaning towards removing this "fix". It not only risks overfitting
and superstitus behaviors, but it puts the learningprocess for q
and w in series, instead of allowign them to operate independently.
Indepent sounds better. Feel I must test more before giving up however..

[4:34PM]  Here's an idea. When b changes, it shifts the flow. and when
that happens, w has to change to "fix it". So the effect of b learning
show up in the w values as they try to "undo" what the b learnin gis
doing. So maybe we should try to factor that out of the w value?  so if
b and w are  50 50 and 50 50 and b shifts to 40 60, then w will shift
to something opposit this to try and "fix" it, reducing the b effect.
This creates a time delay between when b chagnes, and w "catchs up".
So why not factor this "catchup" out of the equation! We can calculate
what b will do to w, and just mathematicallly take it out of the system
instantly so w doesn't have to change at all in response to b changing.
So we have an anti-w factor, in the b*w term??? Yeah, we need to factor
the effects of B out of the target that w is is seeking. Not for the
opint of making w do the learning, but so w doesn't have to do any of
the learning for b! Ah, and that way, the effect of B is felt in full
force in the node, with no delay for w to factor half of it out or for
w to factor it in! Yes, this seems cool. But I'm confused about how
to code it. But I think it's just the opposit, if what I just tried
to code! So let B* create it's full effect instantly when b changes,
and don't make w have to adjust. Let me think though the math and see
if this really works or if this is just insane.

[4:50]  Thought it through. That is exastly what I already coded. So
I changed the wTarget to what b is asking for, but go ahead and implement
it in the policy with *b so w doesn't have to learn it. Didn't work.

And I see now there's a serious problem in my thinking. B learning sets
the LINK flow target, not the output NODE flow volume. And w larning
is adjusting the output NODE flow target. We don't want wLearning
on one set of nodes to fight with wLearning on another bcause they
have diferent flow targers for the same nodes! And just becuase the
b learning shifted is no gurantedd how the w learing will shift due
to it's interaction with the rest of the network -- not to mention
it's inteaction with the *a term of the policy!

So, ok this is all crap. The b learning needs to be separate because
it's target is separate from what wLearning is using for alearning target!
I got confused and was thinking they were setting teh same flow
targets since they were both CONTROLING the same flow volumes out of the
node!

Ok, throw it way and move on. I see my test of the code is suffering
as well -- It lost convergence after running for an hour, and had
to regain it. Not stable. Not good.

================================================================================

2016-02-13 10:51 AM Saturday

Yesterday

1) Resolved the amplificaiton question. Just use *a in the policy and
don't mess around with trying to factor out our own link contribuiton.
That makes it a correct probablity distribution multiplication from
layer to layer! The *w term learns to adjust the flows to account for it!

2) Deleted all the gross code from the previous two days of messing
around.

3) Explored the idea of switching from a pulse/event network to a value
caluclating net. It looks on the surface that it will tranlate directly
with ease. Just use values as "number of pulses", and otherwise the
processing is much the same. Code would change from chasing a single
pulse through the net updating as we go, to processing the full network,
layer by layer node by node, keeping a "pulse count" for each node.
The pulse could would be a real, not an int.

4) Explored the idea of changing w learning to track target defined
by b (again), but then through out all the idea when I realised that b
learning sets flow volumes in the links, but w learning is optimizing
flow volume in the downstream nodes, so they really are optimizing two
different things shouldn't be mixed together in that way.

5) Started lots of testing on various networks. Found that in fact the
*a factor didn't prevent the *w code from balancing network activity,
but that the *a code did make the output highly chaotic and very much
"life-like".

6) One long running test on a binary wired network seemed at first
take to show that the binary network has lots more trouble learning
than the 3-way fan out network. And now understanding the importance
of the network  using the w* learning to create new signals from
old ones, I'm being to think a wide fan-out is important for allowing
the net to produce the best quality internal signals. When it can
only mix two signal at a time, it's choices are highly limited and
could take a very deep net to get the same results.

Today

Well, now that the excitement of a breakthough is slowing down, and the
future looks like a long path of slow testing of a working network my
energy is slowing down. Need to find something to excite and energize me.

On the binary net vs large fan out -- if the optimal mapping from 10
input signal to 10 output singals requiles a partial mix of every input
with every outout, how hard will it be for a binary net to find that
same solution? It feels to me, like if it mixes XYZ to produce ABC if
we mix AB first, then X to each of the two output that won't get the same
result. We will have to keep mixing them many times to get the full set of
signal to stabalize on would have happened in one mix, of XYZ together.
So the wider the fan out the better I think. But this would have to be
tested to verify.

So, maybe I need to try a full cross connect network wiring options and
see how that works? Then it becomes a compuational speed question as to
how wide is optimal. And once we talk about wide fan outs, the single
pulse approach losses it's advangage -- no wait, it doesn't. Becuase even
though I have to pick and calcuate N links, I only choose one path, and
only one path gets it's at updated. So actually, I think it's just as
effecent to sick with the same algorithm until the ned arriases to process
things like video data that are already in the form of synchronus samples.

[11:23 AM] Hey, so binary wiring seemed like a good idea for pulse
routing speed and flexiblity, but yet it's seeming to be a bad idea for
percpetion learning. So maybe the network toplogy will work better with
wide fan out in the early laters, and less fan out in later layers?

[12:04 PM] added wireFanOut paramater to Network()

[3:25 PM] took a break to do pizza and meet John so he could drop off
some ingress gear for me. Back experimenting again.

Playing with TwoNodeOverlap with a fan out of 5 instead of 3. Playing
with halfLife to see how that changes it's life-like behaviors.

Increasing halfLife most certainly does slow down the systems behavior.
It's not just a question of units as I wondered about the other day.

So how fast the node states can change, and in turn, how fast the system's
changes outputs is controlled by half life. But behavior is a direct
effect of changing inputs as well, so when inputs change fast, ouptuts
change fast as well. But the configuration of the nets internal state
chagnes as a function of halflife. So a long half lifke like 10000
moves very slowly, and a short half life shifts very quickly.

The system is clearly very chaotic in how the different shifting nodes
cause each other to rise and fall in strength. And this is before I've
added the feedback the system needs so nodes will control their own
shifting as well indirectly!

If the nodes represent the state of the world (on the percpetion side
of the problem), then they must be able to shift as fast as the world
state changes. How fast is that I wonder? What is correct for a human
level robot in t4erms of how fast his world changes?
enviornment. Such as running, and chasing prey, and being chased. It
would need to change as fast as the environment was changing.

But the faster we make the system shift (shorter halfLife) the less long
term memory we have -- but the more short term resolutoin we end up with.
So do we want eally short tem memory, and very high reslution awarness
of the current state of the evnironment, which would make us very good at
acting quickly, or long term memory which makes us slower to respond, but
good at understanding slower longer term events (languge for exmaple).
So do  you want the sytem to use it's power to reconize short term
signals in high spatial resolution or long term sequences.

It's a trade off of how the memory and resolution of the network is used
-- more for gspatial paterns or more for  temporal patterns.

I've suspected for a long time now that the reason we have langauge powers
is because our language center is tuned for longer temporal memory and
weaker spatial memory.

So, does a deeper network create even slower beahviors or what?
I'm testing that now.

[4:51 PM]  Modified TwoNodeOverlap test to only save last 5 row
counts. Added standard Deviaion to it. Added code to start decreasing
wLearningSpeed at a set time and rate.

So far, deeper nets seem to create more chaotic varriance in the
output, but not faster or slower -- but wider swings in values.
But I got distrated on trying to understand wlearning better in this
context. 

Turned off wLearning after training, made the net become less blanced
indicating that w learnig is an active force to keep the net balanced,
not just a long term learning. Don't know if that was becuaes w learning
was just not done optimizing the settings or if it's just true it can't
optimize the settings. So I've set up the code to show sd to see how
well it's doing, and then I can make it w learning decine in value over
time giving the power to fine tune the settings. But will it fine tune
and make it better, or will solwing it down just make it less able to
dynamically balance allowing the net to destablize.

Also, I'm testing with two inputs, cycling though 20 or so different
pre-defined frequency pairs feeding a 7 output net with 5 fan out.
So there's lots more degrees of freedom in the net, then then inputs
really contain raw state data. The input is only 20 different states
really. So can the network learn 20 different output configurations to
support those states? Doesn't feel like it's doing it now. This is all
about undersanding what percpetion learning is doing, and what it can
do, etc.

I would think, that in theory, w learning wouldn't need to be an active
stabalizing system and that once well trained on data I could turn the
w learning off, and the system will stay basiclaly stable.

But that may only work well with high information content data as
input being mapped to lower information contant state space. and it may
require multiple levels to really decode and repesent the state well?
In the inputs are too low information, the system might not be able to
stay stable.

I'll have to experment by adding some extra random data inputs as well
to see how that effects it all.

[8:15 PM]  testing below:
================================================================================================================================================================
XNET Sat Feb 13 17:12:15 2016  RunTime: 3:02:24  Real-Time Simulation speed=1500X
p1 freq:  1.0 =========
p2 freq:  2.0 ===================
Two Node Overlap Test row1: -1, row5: +1 7 x 2  time: 4560:16:40.0000  PPS  3016  RPS 0.000 0.000 
Explore: 1.00%  Q: 0.00001000  B: 0.00001000  W: 0.00000000  halfLife: 100.0
  .             |  2o|    198 ======
  .             |  3o|    315 ==========
 10o33111.11010 |  3o|    337 ===========
  .             |  4o|    295 =========
 10o11133.00011 |  3o|    340 ===========
  .             |  2o|    190 ======
  .             |  3o|    336 ===========
Node [2][0] 0.99938
 at:   0.20539902        0.32372236        0.18405785        0.12382567        0.16237104     
  q:   1.00000000        1.00000000        1.00000000        1.00000000        1.00000000     
  b:   0.33334310  20    0.33334230  20    0.33334138  20    0.33333686  19    0.33333486  19 
  w:   0.29207218  29    0.29043178  29    0.13764681  13    0.14361136  14    0.13627789  13 
  a:   0.20539902  13    0.32372236  21    0.34697921  22    0.35142052  23    0.29094729  19 
bwa:   0.01999770  20    0.03134060  32    0.01592058  16    0.01682284  17    0.01321662  13 
Node [4][0] 1.00723
 at:   0.16292136        0.22759485        0.12857625        0.21124726        0.27688612     
  q:   1.00000000        1.00000000        1.00000000        1.00000000        1.00000000     
  b:   0.33334175  20    0.33333620  19    0.33333413  19    0.33333868  20    0.33333662  19 
  w:   0.14346975  14    0.13830618  13    0.14437748  14    0.28713071  28    0.28675590  28 
  a:   0.34697921  23    0.35142052  23    0.29094729  19    0.21124726  14    0.27688612  18 
bwa:   0.01659409  17    0.01620135  17    0.01400211  14    0.02021885  21    0.02646650  28 

row counts are:
     14646  9.8%      24936 16.6%      22632 15.1%      24814 16.6%      22836 15.2%      22809 15.2%      17197 11.5% [sd:24.911 w:4.5e-48]
     96015 13.3%     100007 13.9%     116717 16.2%     114291 15.9%      99189 13.8%     101209 14.1%      92366 12.8% [sd:26.461 w:7.0e-48]
    101352 14.1%      91879 12.8%     117228 16.3%     110733 15.4%     107293 14.9%      95675 13.3%      95945 13.3% [sd:26.622 w:2.1e-47]
    108364 15.1%      83589 11.6%     113587 15.8%     111154 15.4%     111374 15.5%      99373 13.8%      92017 12.8% [sd:33.088 w:6.5e-47]
    100646 14.0%      98820 13.7%     103584 14.4%     104166 14.5%     115168 16.0%     104063 14.5%      93501 13.0% [sd:19.148 w:2.0e-46]
    108435 15.1%      83304 11.6%     114807 16.0%     115251 16.0%     109813 15.3%      99750 13.9%      88362 12.3% [sd:36.862 w:6.1e-46]
    104638 14.5%      89113 12.4%     126222 17.5%     112090 15.6%      96895 13.5%      98707 13.7%      92525 12.8% [sd:36.940 w:1.8e-45]
    104783 14.5%      93659 13.0%     104722 14.5%     108228 15.0%     111969 15.5%      95246 13.2%     101741 14.1% [sd:19.101 w:5.6e-45]
     85156 11.8%     105637 14.7%     113635 15.8%     116953 16.2%     109429 15.2%      95924 13.3%      93121 12.9% [sd:33.791 w:1.7e-44]
    100462 14.0%      97194 13.5%     109767 15.3%     112450 15.6%     104216 14.5%      99370 13.8%      96163 13.4% [sd:18.124 w:5.3e-44]
    104829 14.6%      89174 12.4%     107311 14.9%     113978 15.8%     111930 15.5%      94826 13.2%      97814 13.6% [sd:26.535 w:1.6e-43]

================================================================================================================================================================

It's running a simple 2 node network, and wLearningSpeed is being slowly
reuded from it's starting point of around 1e-5 down to -48 as if this
point and the standard deviaion of the output pulse counts is in the
20's which is typical of what it was doing when w was much higher.

Other test runs were doing even better with lower sd numbers like this:

row counts are:
     12276 11.3%      16546 15.2%      18442 16.9%      12330 11.3%      17114 15.7%      13720 12.6%      18618 17.1% [sd:20.459 w:2.0e-49]
    102983 14.3%      90284 12.5%     104602 14.5%     109340 15.2%     112603 15.6%     100731 14.0%      99509 13.8% [sd:20.879 w:2.5e-49]
     84043 11.7%     118311 16.4%     101128 14.0%     102785 14.3%     103956 14.4%     100887 14.0%     108710 15.1% [sd:29.704 w:7.5e-49]
     78904 11.0%     115030 16.0%     109468 15.2%     111916 15.5%     103765 14.4%      97133 13.5%     103744 14.4% [sd:35.002 w:2.3e-48]
     97402 13.5%      92971 12.9%     109533 15.2%     105768 14.7%     117094 16.3%     105093 14.6%      92211 12.8% [sd:26.392 w:7.0e-48]
     96551 13.4%      99077 13.8%     114070 15.8%     105414 14.6%     100582 14.0%     105643 14.7%      98676 13.7% [sd:17.350 w:2.1e-47]


This is what it looked like when it starts:

row counts are:
     87335 12.1%      89106 12.4%     120075 16.7%     115653 16.1%     110993 15.4%      89388 12.4%     107250 14.9% [sd:40.094 w:1.0e-05]
     92214 12.8%      85429 11.9%     103895 14.4%     114781 15.9%     122223 17.0%      91480 12.7%     110094 15.3% [sd:39.387 w:1.0e-05]
     87423 12.1%      96638 13.4%     122526 17.0%     108400 15.1%     107116 14.9%      94775 13.2%     102822 14.3% [sd:32.877 w:1.0e-05]
     90662 12.6%      93850 13.0%     120030 16.7%     109373 15.2%     111730 15.5%      96450 13.4%      97979 13.6% [sd:31.418 w:1.0e-05]
     94653 13.1%      97450 13.5%     117079 16.3%     109268 15.2%     111827 15.5%      87611 12.2%     102415 14.2% [sd:30.109 w:1.0e-05]
     99541 13.8%      95469 13.3%     108341 15.0%     116150 16.1%     111158 15.4%      98648 13.7%      90986 12.6% [sd:26.344 w:1.0e-05]
     87327 12.1%      92518 12.9%     114552 15.9%     111436 15.5%     109874 15.3%      94105 13.1%     109936 15.3% [sd:31.943 w:1.0e-05]
     78768 11.0%      82266 11.5%     116332 16.2%     140966 19.7%     124542 17.4%      90257 12.6%      82943 11.6% [sd:71.507 w:1.0e-05]

It stars at SD around 60 and drops quickly to the 20's. So for this
simple 2 node single layer network, it seems that once trained wLearning
can be basically turned off, and the net statys about the same, meaning
wLearning is not acting as a important dynamic balancing force. But I
really need to suddenly drop it to zero to realy confirm it's not needed
for dynamic balancing. It might just be dynamicly balancing at the 44th
deceial place! Maybe this chaotic system is that senstive?

Ah, but that data I posted above, started to drop the wValue off the cliff
at that last tick at the top, It's down to e-8 already and falling fast.
And the net doesn't seem to be responding well --  high SD! Kept cutting
and pasting as it continues...

Seems to keep about the same level of SD down to w=0.

row counts are:

    121376 16.9%      80841 11.2%     109479 15.2%     111756 15.5%      97723 13.6%      90969 12.6%     107653 15.0% [sd:39.821 w:0.0e+00]
    141076 19.6%      67984  9.4%      92313 12.8%     104865 14.6%     105591 14.7%     100576 14.0%     107803 15.0% [sd:62.676 w:0.0e+00]
    130447 18.1%      74968 10.4%      96406 13.4%     107731 15.0%     106077 14.7%     101143 14.0%     103532 14.4% [sd:47.397 w:0.0e+00]
    135448 18.8%      65369  9.1%      97665 13.6%     114850 15.9%     107563 14.9%      76742 10.7%     122454 17.0% [sd:71.938 w:0.0e+00]
    132483 18.4%      71422  9.9%      92167 12.8%     112644 15.6%     106231 14.8%      91135 12.7%     113837 15.8% [sd:57.076 w:0.0e+00]
    135198 18.8%      67506  9.4%     102724 14.3%     108915 15.1%     104668 14.5%      89870 12.5%     111275 15.5% [sd:59.798 w:0.0e+00]
    124462 17.3%      79525 11.1%      99507 13.8%     111337 15.5%     102037 14.2%      85760 11.9%     117037 16.3% [sd:46.979 w:0.0e+00]
    136078 18.9%      67850  9.4%      97023 13.5%     107162 14.9%     111091 15.4%      96592 13.4%     104087 14.5% [sd:58.815 w:0.0e+00]
    126951 17.6%      77369 10.8%      97670 13.6%     109480 15.2%     105954 14.7%      97813 13.6%     104471 14.5% [sd:43.126 w:0.0e+00]
    130064 18.1%      78957 11.0%      90814 12.6%     110766 15.4%     100421 13.9%      93310 13.0%     115552 16.1% [sd:49.687 w:0.0e+00]

    119466 16.6%      77517 10.8%     109473 15.2%     111173 15.4%     109879 15.2%      80864 11.2%     112231 15.6% [sd:47.876 w:1.0e-130]
    140863 19.5%      72921 10.1%     100069 13.9%     104040 14.4%      91951 12.8%     102948 14.3%     107763 15.0% [sd:58.818 w:1.0e-125]
    131933 18.3%      66867  9.3%     103516 14.4%     116172 16.1%     104289 14.5%      85292 11.8%     111782 15.5% [sd:61.342 w:1.0e-120]
    130708 18.2%      75394 10.5%      93316 13.0%     113567 15.8%     100176 13.9%      91563 12.7%     115073 16.0% [sd:52.970 w:1.0e-115]
    122633 17.0%      75946 10.5%      98223 13.6%     123178 17.1%     100643 14.0%      86416 12.0%     113195 15.7% [sd:51.790 w:1.0e-110]
    126422 17.6%      75287 10.5%      99674 13.8%     115954 16.1%      99864 13.9%     100618 14.0%     102282 14.2% [sd:45.813 w:1.0e-105]
    139218 19.3%      67050  9.3%      95407 13.2%     110846 15.4%     102626 14.3%      94524 13.1%     110475 15.3% [sd:62.921 w:1.0e-100]
    132914 18.5%      73012 10.1%     100515 14.0%     108938 15.1%     100376 13.9%      84027 11.7%     120502 16.7% [sd:59.016 w:1.0e-95]
    139628 19.4%      68894  9.6%      92135 12.8%     113501 15.8%      96052 13.3%      94489 13.1%     115182 16.0% [sd:64.577 w:1.0e-90]
    117358 16.3%      76293 10.6%      96225 13.4%     122648 17.0%     110656 15.4%      86562 12.0%     110378 15.3% [sd:49.078 w:1.0e-85]
    130232 18.1%      75767 10.5%      91535 12.7%     112608 15.6%     103733 14.4%      93355 13.0%     112705 15.7% [sd:51.299 w:1.0e-80]
    120661 16.8%      81988 11.4%     105220 14.6%     103432 14.4%     109988 15.3%      90406 12.6%     108412 15.1% [sd:37.174 w:1.0e-75]
    131202 18.2%      72999 10.1%     100055 13.9%     110787 15.4%     102165 14.2%      86851 12.1%     116152 16.1% [sd:55.258 w:1.0e-70]
    121890 16.9%      77116 10.7%      96472 13.4%     122064 16.9%     100497 14.0%      74591 10.4%     127539 17.7% [sd:62.889 w:1.0e-65]
    131305 18.2%      71690 10.0%      92022 12.8%     115047 16.0%     105345 14.6%      95346 13.2%     109127 15.2% [sd:54.632 w:1.0e-60]
    131158 18.2%      70120  9.7%      95119 13.2%     116786 16.2%     105692 14.7%      94096 13.1%     107389 14.9% [sd:55.662 w:1.0e-55]
    137083 19.0%      67010  9.3%      94871 13.2%     116680 16.2%     100061 13.9%      90656 12.6%     114300 15.9% [sd:64.508 w:1.0e-50]
    135317 18.8%      65094  9.0%      96988 13.5%     119688 16.6%     101833 14.1%      92607 12.9%     108871 15.1% [sd:63.880 w:1.0e-45]
    126860 17.6%      78065 10.8%      97423 13.5%     115957 16.1%      97966 13.6%      84121 11.7%     119828 16.6% [sd:53.415 w:1.0e-40]

    128754 17.9%      73977 10.3%      90676 12.6%     116079 16.1%     107539 14.9%      83483 11.6%     119458 16.6% [sd:58.936 w:1.0e-25]
    123963 17.2%      82640 11.5%      95627 13.3%     108760 15.1%     103784 14.4%      87858 12.2%     117280 16.3% [sd:43.706 w:1.0e-20]
    131385 18.2%      70876  9.8%      97827 13.6%     110575 15.4%     108769 15.1%     103982 14.4%      96798 13.4% [sd:52.623 w:1.0e-15]
    149312 20.7%      66629  9.3%      89074 12.4%     103099 14.3%      97361 13.5%      82105 11.4%     132191 18.4% [sd:83.196 w:1.0e-10]
     87335 12.1%      89106 12.4%     120075 16.7%     115653 16.1%     110993 15.4%      89388 12.4%     107250 14.9% [sd:40.094 w:1.0e-05]

It was in the 30's before the wLearning fast drop off stated, and it
moved up to the 50's and 60's almost as bad as it started at in the first
section (at 71). But looking at the distribution of pulses, (10 to 17%)
even in the worse, really doesn't look bad at all. What was bad was
when I turned w off, and it went to like 40% in one 35% in another, and
less than 5% in the rest. Most output piled up on two outputs basically.

waiting for it to actually hit zero and see if something big changes (e-300 I think)

Next I need to try it with a two layer net and see if it works as well
once wLearning is turned off.

[9:00 PM] Wow, the below is a binary tree run from 24 hours ago. It is
rewarded for sending all P1 to output 4, and for not sending P2 to 4,
and rewarded for P2 going to all other paths It was working fine for most
th etime, and just colapsed and lost it's ability to only send pulses to
output 4. This has constant inputs -- and those constant inputs likely
cause the system stabilty problems. But still, this b inary net I think
is the real problem. Percpetion does't seem to work well in a simple
inary (fan out of 2) network.
================================================================================================================================================================
XNET Fri Feb 12 20:52:10 2016  RunTime: 24:07:44
  p1 freq:  0.8    p2 freq:  1.0 =
SimpleTest p1 +1 out4, p2 0 for out4 10 x 8  time: 16492:09:59.6601  PPS  1198  RPS 0.096 0.863 
Explore: 5.00%  Q: 0.00010000  B: 0.00001000  W: 0.00001000  halfLife: 10.0
  .          |  .          |  .          |  .o55.64.01 |  .o37.64.10 |  1o46.64.11 |  2o55.55.11 |  2o|    104 =====
  .          |  .          |  .          |  1o73.46.12 |  1o37.73.01 |  .o73.37.00 |  0o64.37.10 |  3o|    110 =====
  .          |  .          |  .          |  0o55.73.00 |  0o82.37.10 |  2o28.82.10 |  2o37.73.02 |  1o|     86 ====
  .          |  .          |  .          |  1o46.64.01 |  .o55.55.00 |  .o64.55.00 |  .o55.46.30 |  5o|    125 ======
  8o55.55.04 |  1o55.73.01 |  .o55.73.01 |  .o64.46.00 |  .o46.64.00 |  .o46.55.02 |  .o82.28.10 |  1o|    151 =======
  .          |  7o55.73.50 |  6o55.73.22 |  2o64.37.10 |  2o37.73.02 |  1o46.73.01 |  .o64.55.00 |  .o|    280 =============
 10o55.55.32 |  6o46.46.13 |  2o64.55.10 |  2o73.37.20 |  5o73.46.22 |  4o73.37.20 |  4o73.37.21 |  4o|    225 ===========
  .          |  4o55.37.02 |  1o46.64.00 |  .o55.46.11 |  3o73.46.30 |  5o64.46.13 |  3o46.73.01 |  .o|     65 ===
  .          |  .          |  4o64.37.10 |  4o37.73.12 |  2o37.64.02 |  1o64.46.01 |  .o73.46.00 |  . | 
  .          |  .          |  3o55.37.30 |  7o64.46.21 |  4o64.46.21 |  4o46.64.30 |  6o46.64.13 |  1o|     56 ===
p1 percent for output 4 is:  43.71
P1
 0     45 =====
 1     53 ======
 2     33 ====
 3     42 =====
 4     66 =======
 5    141 ==============
 6     77 ========
 7     33 ====
 8      .
 9     33 ====
P2
 0     59 =====
 1     57 =====
 2     53 =====
 3     83 =======
 4     85 =======
 5    139 ===========
 6    148 ============
 7     32 ===
 8      .
 9     23 ==
================================================================================================================================================================

Two layer network output testing the same dropping in wLearningSpeed fast after the first few cycles:

The middle output is seriously depressed in volume.

row counts are:
      7697 20.2%       6648 17.5%       3615  9.5%       4307 11.3%       4501 11.8%       3956 10.4%       7321 19.2% [sd:21.624 w:1.0e-26]
    117587 16.3%     120067 16.7%      90970 12.6%      38717  5.4%      74175 10.3%     134890 18.7%     143308 19.9% [sd:107.087 w:1.0e-25]
    114283 15.9%     140123 19.5%      76850 10.7%      56541  7.9%      67686  9.4%     125360 17.4%     138941 19.3% [sd:101.301 w:1.0e-20]
    112420 15.6%     114179 15.8%      88437 12.3%      59031  8.2%      81086 11.3%     124117 17.2%     141158 19.6% [sd:81.124 w:1.0e-15]
    114617 15.9%      99416 13.8%     104584 14.5%      54014  7.5%      95895 13.3%     123982 17.2%     127602 17.7% [sd:71.154 w:1.0e-10]
     87871 12.2%      90748 12.6%     141742 19.7%      97375 13.5%     105288 14.6%      97428 13.5%      99778 13.9% [sd:52.142 w:1.0e-05]
     95754 13.3%     118494 16.5%     123724 17.2%      98114 13.6%     104239 14.5%     104992 14.6%      74793 10.4% [sd:46.349 w:1.0e-05]
     98536 13.7%     106823 14.8%     122438 17.0%      95263 13.2%     119395 16.6%      91350 12.7%      86218 12.0% [sd:40.108 w:1.0e-05]
     89782 12.5%     116119 16.1%     128234 17.8%      95532 13.3%     113179 15.7%      95172 13.2%      82216 11.4% [sd:47.662 w:1.0e-05]
     89915 12.5%     104521 14.5%     134419 18.7%      98878 13.7%     111789 15.5%      94968 13.2%      85142 11.8% [sd:47.710 w:1.0e-05]
    112237 15.6%      88367 12.3%     132050 18.3%      85187 11.8%     115468 16.0%      98809 13.7%      87767 12.2% [sd:50.906 w:1.0e-05]

[9:46 PM] Thought. Maybe it's key that the system pulse volume remain
relatively constant?  increases in volume, cause increase in flows
being sucked to one node or another (high values causes more sucking
action). Maybe it's much harder for the system to learn a set of w values,
that make the system stable, when overall flow is jumping up and down.
On this test I'm running it goes from two inputs at 1 hz (2 pulses a
second) to two at 3 hz (6 pulses a second) on the high end.

Maybe if I made sure the sum of all pulses per second were costant, it
could better stablize the system when the w learning is turned off?

I'l have to experment wtih that.

Other Thought -- w learning, as the w speed is reduced, might just
be adding more and more resolution to the w values. Each time it
steps down, it might have to fill in more digits of the precision.
And if it steps down too quckly, it fails to fill in.

So when still turned on, the wLarning system is acting as active
stabalizing effect for the lower resolution details it can't fill in
due to the high level of variations?

[12:50 AM]  Time for bed

Here's a 2 level run with reducing w Value:

    114734 15.9%     121548 16.9%     103212 14.3%      51896  7.2%      98504 13.7%     129311 18.0%     100510 14.0% [sd:72.776 w:0.0e+00]
    101349 14.1%     118613 16.5%     101430 14.1%      55045  7.6%     101191 14.0%      99020 13.7%     143591 19.9% [sd:76.503 w:0.0e+00]
    111762 15.5%     126408 17.6%      95557 13.3%      57703  8.0%      87161 12.1%     124324 17.3%     117038 16.3% [sd:71.050 w:0.0e+00]
    127590 17.7%     117325 16.3%      91678 12.7%      56127  7.8%      72991 10.1%     137920 19.2%     116140 16.1% [sd:86.837 w:0.0e+00]
     92089 12.8%     165394 23.0%      79266 11.0%      48367  6.7%      85252 11.8%      87977 12.2%     161970 22.5% [sd:126.783 w:0.0e+00]
    112417 15.6%     136006 18.9%      95671 13.3%      53491  7.4%      78625 10.9%     114776 15.9%     128910 17.9% [sd:84.060 w:0.0e+00]

And a second one with slower step down (20h instead of 10h per step down)

     48860  8.7%      63940 11.4%     117641 21.0%      74019 13.2%     122100 21.8%      82049 14.6%      52311  9.3% [sd:96.500 w:1.0e-207]
     61147  8.5%      77339 10.7%     143600 19.9%     118109 16.4%     157964 21.9%      84700 11.8%      77346 10.7% [sd:107.357 w:1.0e-205]
     61837  8.6%      82455 11.5%     139082 19.3%     116218 16.1%     149837 20.8%     109658 15.2%      60817  8.4% [sd:102.817 w:1.0e-203]
     50053  7.0%      99755 13.9%     136129 18.9%     122978 17.1%     157044 21.8%      86511 12.0%      67480  9.4% [sd:110.336 w:1.0e-200]
     63833  8.9%      93137 12.9%     138048 19.2%     103990 14.4%     149393 20.7%      94005 13.1%      77673 10.8% [sd:89.245 w:1.0e-198]
     68129  9.5%      84754 11.8%     145758 20.2%      96970 13.5%     168230 23.4%      82458 11.4%      74027 10.3% [sd:111.341 w:1.0e-195]
     70939  9.9%      81965 11.4%     135109 18.8%     114406 15.9%     150481 20.9%     102826 14.3%      64205  8.9% [sd:94.218 w:1.0e-193]
     53268  7.4%      81244 11.3%     149757 20.8%      96431 13.4%     162715 22.6%     104399 14.5%      71988 10.0% [sd:116.118 w:1.0e-190]
     57262  8.0%      83528 11.6%     153784 21.4%     102573 14.2%     160398 22.3%      87202 12.1%      75212 10.4% [sd:114.060 w:1.0e-188]

Seems by random chance that te one did a bit better than the other.

[12:50 AM]  Time for bed

================================================================================

2016-02-14 8:54 AM Sunday

(Valentines day)

Yesterday

1) Added fan-out paramater to network to set the node fan out.

2) lots of long testing runs with TwoNodeOverLap to understand how
learning parameters work with halfLife and w learning speed.

3) Learned that longer halfLife does slow down how fast the net shifts
internal state.

4) Learned that w learning can act as a dynamic stabalizing effect so
when you turn it off, the net does suddenly become lese balanced (like
with most output going to two of 5 nodes).

5) Slowly reducing w learning speed to zero can help the net stay balanced
when w is turned off. Reducing too fast however is like turning it off
all at once.

6) Added code to TwoNodeOverlap to show standard devision of pulse
volumes from nodes, to track only the last 10 periods of saved
results, to automatically start to deline w speed at a given point
and a given rate to learn more about how that works.

7) learned that deeper nets tend to have large chaotic varriance
in the amount of data flowing to each node. They "spike" more
then a single level net with the same other paramaters. 

8) An idea from bed. Bias the fan-out with factors that create a
non-symetry in the system. Create an artifical "distance" bias
in the system.

Today

On the distance bias idea...

Thought of this before but decided it was pointless becuase it would just
be undone by w elarning. But I now relaize that's not true.  w learning is
springly and can't totalkly undo anything. So a distance bias to linke
wieghts would create a perminate bias in the system. Good for when two
ouptut nodes are shared by more than on set of input nodes (which happens
a lot in 3 or more fan out). Without that bias, all nodes are equal and
the system has no bias on how nodes are assigned roles. So the roles are
free to drift endlessly. Percpetion learning must first assign "roles"
to the nodes, and then qLearning evalutes the usefulness of that node in
different beahviors. But if the roles assigned by percpetio nlearning
is constantly drtiftig, then qLearning won't be able to converge.

For example, two shared nodes, one is .3x + .7y, and the other is .7x
+ .3y. So qLearing starts to evaluate the valueo fthose two signals.
But before it's collected must statistics, it has drifted all the way to
become .7x .3y in the first, and .3x .7y in the secod -- the two signals
have moved places! And now qLearning is all wrong. the qvalue learned
for the firstis not being assigned to the second.

Whatever "beahviors' the first was being used for in the downstream
network, is now switched. with the second. Everything is messed up adn
the qValues and the entire downstream network, now as to reconfigure
itself to the new "roles" of these bottom nodes. And as the rest of
the netowrk tries to do that, the bottom noees then drift again.

So percpetion learning has to be a highly stable system on which the
rest can be built. A system that ends up converging on the same network
condiguartion ever time! Not randomly endingup with different solutions.
It must be over-constained, which is why this springy version of w
elarning is so important.

And why this distance value on links I now see is important.

But what values to use?

[1:33 pm] This is what I randomly picked to code:

	## 1.0/((i-middle)^2 + 10):
	## fanout 0: 
	## fanout 1:  1.000
	## fanout 2:  0.500 0.500
	## fanout 3:  0.323 0.355 0.323
	## fanout 4:  0.228 0.272 0.272 0.228
	## fanout 5:  0.168 0.214 0.235 0.214 0.168
	## fanout 6:  0.128 0.170 0.203 0.203 0.170 0.128
	## fanout 7:  0.099 0.135 0.172 0.189 0.172 0.135 0.099
	## fanout 8:  0.079 0.108 0.143 0.171 0.171 0.143 0.108 0.079
	## fanout 9:  0.063 0.087 0.118 0.150 0.165 0.150 0.118 0.087 0.063

It's not linear, which I think would help reduce the odds that it
failed to do what I wanted it to do. So with full fan out, ever node
ends up being a different linear combination of the nodes in the previous
level.

Doing test runs with TwoNodeOverlap to see if I can tell if there's any
difference. At first glance, two runs, that ran for an hour plus, showed
that the old system had slightly less standard devation measures. This
is whith a 3 level deep network BTW. But generaly about the same.

Watching it run, I notice that the one without distanceBias tends to
have high peak activty -- the pulse count bar graph shoots out futhr,
more often,m for individual nodes. I don't measure that effect with my
sd score -- I only masure long term differences in flow from output
to output. This is a short term effect I seem to be spotting. But I
don't know if it was a side effect of this sinle run, or something that
could be seen consistently.

I'm running 3 more runs with distanceBiase to see what the w values of
the first two nodes (fan out of 5) settle on. In theory, this distance
bias should make the w values more consistent from run to run.

I'll do the same without distance bias, and see what the consistency
is like. Maybe I'll add more statistics to try and track this.

[2:49 PM]

Testing results...

no distance bias:

#1
w:   0.28948693  28    0.28953930  28    0.13260786  13    0.14259954  14    0.14581993  14 
w:   0.14836970  14    0.13887107  13    0.13614862  13    0.28815171  28    0.28851254  28 
#2
w:   0.28949383  28    0.29017198  29    0.14076955  14    0.14014760  14    0.13947062  13 
w:   0.14077546  14    0.14084605  14    0.14150661  14    0.28832901  28    0.28859649  28 
#3
w:   0.28701537  28    0.28872348  28    0.14367989  14    0.14119000  14    0.13944486  13 
w:   0.13778947  13    0.14005957  14    0.14214545  14    0.29052283  29    0.28953627  28 

With distance bias:

#1
w:   0.33532063  33    0.26566363  26    0.12380110  12    0.12986902  12    0.14539923  14 
w:   0.15217107  15    0.12664844  12    0.12824702  12    0.26125341  26    0.33173366  33 
#2
w:   0.33292117  33    0.26258190  26    0.12468034  12    0.13295749  13    0.14691267  14 
w:   0.15240084  15    0.12264917  12    0.12721739  12    0.26395355  26    0.33383269  33 
#3
w:   0.33456883  33    0.26417307  26    0.12430015  12    0.12973896  12    0.14727256  14 
w:   0.15138612  15    0.12662382  12    0.12743563  12    0.26243993  26    0.33216807  33 

Without doing any formal statistics, it does look like the three runs
with distance bias have converged on w values that seem to be slightly
more consitent than the three runs with no distance bias. Such as no
distance: 148 140 137 and distance for the same numbners 152 152 151.

The difference is small. It's like the distance bias added 1/2 digit
of more consistency to what the system converged. With distance bias
convering on about 3 digits of consistency and non distance at about 2
1/2 digits of consistency. But that's only a routh subjective measure.

I'm not seeing any big advangage to using the distance bias on this test,
but the logic sems sound, and the numbers are consitent with imporved
stablity in this data even if not overwhelming.

Speed of convergence on RL tests will also be interesting to see with
distanceBias and without.

Though there is plenty more testing I could do with this, I'm inclinded
to just turn it on and leave it on and assume it's a step in the right
direction, if maybe only a small one. And the cost is insignificant.

Oh, and also, these are only numbers for the first input layers. Not
the following 2 layers beyond that. The next two layers could show
more consistency with the distanceBias in effect.

Darn, I think I need to test that.

Two more runs with distanceBias on, dump of all the w values:
Also reduced half life from 100.0 to 10.0 since the last
runs above.

[ 2, 0] 0.33495 0.26568 0.13939 0.12638 0.13365
[ 4, 0] 0.13166 0.13043 0.13760 0.26608 0.33428
[ 0, 1] 0.22234 0.21367 0.22688 0.21005 0.20983
[ 1, 1] 0.22518 0.22553 0.20949 0.19409 0.19081
[ 2, 1] 0.25992 0.20246 0.18094 0.17450 0.19764
[ 3, 1] 0.24280 0.18634 0.17412 0.18028 0.22684
[ 4, 1] 0.19043 0.17553 0.17817 0.20443 0.26238
[ 5, 1] 0.19374 0.18602 0.20318 0.22978 0.23345
[ 6, 1] 0.21565 0.21506 0.22562 0.21954 0.20554
[ 0, 2] 0.23242 0.19478 0.19074 0.19136 0.23662
[ 1, 2] 0.25032 0.18603 0.18614 0.18629 0.22510
[ 2, 2] 0.23506 0.18638 0.17071 0.18719 0.24231
[ 3, 2] 0.24946 0.18591 0.17930 0.18072 0.22747
[ 4, 2] 0.24084 0.18753 0.17551 0.18308 0.23272
[ 5, 2] 0.21878 0.18509 0.18396 0.19033 0.25951
[ 6, 2] 0.24363 0.19770 0.19088 0.19049 0.22040

[ 2, 0] 0.33192 0.26430 0.13465 0.13049 0.13869
[ 4, 0] 0.13782 0.12662 0.13445 0.26690 0.33427
[ 0, 1] 0.22224 0.22223 0.22211 0.20708 0.20712
[ 1, 1] 0.22830 0.22805 0.20571 0.19297 0.19633
[ 2, 1] 0.25409 0.20526 0.17894 0.17818 0.19676
[ 3, 1] 0.23471 0.18852 0.17741 0.18254 0.22501
[ 4, 1] 0.19617 0.17684 0.18061 0.20961 0.25203
[ 5, 1] 0.19147 0.19169 0.19881 0.22767 0.23556
[ 6, 1] 0.20396 0.20323 0.23471 0.22307 0.21462
[ 0, 2] 0.23682 0.19377 0.19230 0.19324 0.23375
[ 1, 2] 0.25386 0.18853 0.18821 0.18274 0.22337
[ 2, 2] 0.22616 0.18087 0.17742 0.19286 0.24063
[ 3, 2] 0.23197 0.18705 0.18637 0.18784 0.22836
[ 4, 2] 0.24081 0.18381 0.18045 0.18264 0.23279
[ 5, 2] 0.22276 0.18281 0.18806 0.18616 0.25350
[ 6, 2] 0.22984 0.19546 0.20103 0.19514 0.22919


Now runnning with distance Biase turned off

[ 2, 0] 0.28946 0.28926 0.14112 0.13805 0.14216
[ 4, 0] 0.13990 0.14220 0.13787 0.28979 0.29029
[ 0, 1] 0.20894 0.22452 0.20996 0.21446 0.20570
[ 1, 1] 0.22735 0.22538 0.20813 0.19731 0.20036
[ 2, 1] 0.23813 0.21032 0.18139 0.18738 0.19534
[ 3, 1] 0.21648 0.19558 0.19251 0.18658 0.22167
[ 4, 1] 0.19754 0.18962 0.18454 0.20411 0.23685
[ 5, 1] 0.19875 0.20121 0.20357 0.22191 0.22869
[ 6, 1] 0.20383 0.20361 0.21895 0.22882 0.20489
[ 0, 2] 0.21645 0.19818 0.20723 0.20374 0.21995
[ 1, 2] 0.22565 0.19389 0.20119 0.20025 0.21888
[ 2, 2] 0.20995 0.18986 0.20055 0.20188 0.21768
[ 3, 2] 0.22137 0.19406 0.20716 0.19537 0.21099
[ 4, 2] 0.22065 0.19047 0.19745 0.19439 0.21352
[ 5, 2] 0.21149 0.19733 0.19467 0.19393 0.23767
[ 6, 2] 0.21889 0.19815 0.20507 0.19644 0.22216

[ 2, 0] 0.28975 0.28957 0.13900 0.13531 0.14643
[ 4, 0] 0.14160 0.14443 0.13483 0.28947 0.28973
[ 0, 1] 0.19246 0.22339 0.22247 0.20463 0.20643
[ 1, 1] 0.23609 0.22399 0.20711 0.20790 0.19014
[ 2, 1] 0.23560 0.21317 0.18564 0.18782 0.19085
[ 3, 1] 0.20938 0.18791 0.19776 0.19525 0.22232
[ 4, 1] 0.19660 0.19142 0.18571 0.20804 0.23062
[ 5, 1] 0.20731 0.20165 0.20599 0.22154 0.22317
[ 6, 1] 0.20613 0.20941 0.22096 0.22065 0.20554
[ 0, 2] 0.21847 0.19223 0.20648 0.20264 0.22202
[ 1, 2] 0.22657 0.19682 0.20742 0.19241 0.21553
[ 2, 2] 0.20746 0.18861 0.20025 0.20220 0.21566
[ 3, 2] 0.22274 0.19214 0.19895 0.19078 0.22383
[ 4, 2] 0.21710 0.20179 0.19992 0.19402 0.21173
[ 5, 2] 0.21324 0.19972 0.19805 0.19639 0.22739
[ 6, 2] 0.21779 0.19548 0.20576 0.20356 0.22143

Too many numbers. I can't really say that either
is more constant. Looking at numbers like this
is not asnwering the question.

================================================================================

2016-02-15 7:20 AM Monday

Yesterday

1) Implemented distanceBias idea and tested. The idea is that it will
make the net more stable for perception learning which should improve
the systems ability to do RL without having the lower levels shifting
roles randomly. Saw small signs that it allows the net to better
train to the same w values from run to run but mostly my testing
has not shown any real differences yet. But larger nets with RL
testing should show the improvement if this idea is valud.

2) Got called "an embarsement" for aruging that human intelinece
is created by RL on AI Safty Facebook group.

Today

Working at Mount Vernon for free day today with Nicholas. It snowed a
few inchs over night and there's risk of ice today. They have delayed
opening until 10 am and will close at 3pm. Have a few minutes
to kill since they are opening an hour later than normal.

Need to test how well perception learning works and find it what
it can do or can't do.

================================================================================

2016-02-16 8:13 AM Tuesday

Yesterday

1) Worked at Mount Vermon on Free day. Snow, and cold made the day
short and light. They were only open 10 to 2:30 intead of the normal
9 to 4 for this time of year. So, very little real work on AI got done.

2) Coded a training test with TwoNodeOverlapTest to see what percpetion
would do. Mapped 4 different pulse frquency combinations for 2 inputs,
to 4 different output nodes, trying to train the system to send a
stream of pulses to each output during each frequency combination.
Input frequencies were 1.0 and 2.0. They cycled thorugh 4 pairs
of 11 12 21 22., and the hope was to see outputs like 1000 0100
0010 and 0001 repeating. No sign of working. Using a 7x6 (5 deep)
network. No sign of lerning this trick at all.

3) added two more pulse inputs as volume equlizing inputs. So the
sum of all pulses per second would stay equal as the two main signal
inputs went up and down. These extras sort of act as negative versions
of the two signal inputs to give the net work to work with. No
improvment with learning this trick.

4) Exciting idea for improved w learning! Was thinking about how w
learning might not be information maximising as it should. Was thinking
about the simple update formula Hinton came up with for RBMs that
trained the system to create hidden layers that were good predictors of
the input layers and in thinking about how to make my training do the
same -- i.e. train the output layer to be a predictor of the inputs,
I came up with this idea:  The system should be sorting each pulse
to the currently lowest active output node! So train it to do that!
Train the system, so that the state of the output nodes act as PREDCTORS
of what pulse will be sorted where next! The output nodes are basically
saying "here is where I think the next plse will be sorted" (by whichever
node is the lowest in volume). So to do that, the training rule is ver
close to what I'm already using, but "better". I find the lowest ouptut
node O(low), and then I note the sorting policy node selection O(sort).
If they are the same, then the system did exactl what it should do --
no training needed. But if they are different, then increase the weight
going to the low node, and decrease the weight going to the node that
was selected! Leave all the other weights alone. This approach is a
direct anonlogy to the deep learning update. But simpler because it
requires no cycling between the two layers. But the end result is the
same -- we train the system to be the best precitor possible!

Today

I'm up at 7:30 becuase I was so excited about this new idea I couldn't
sleep! Came up with it right before going to bed. Had to take one
capsule of the sleeping aid and I hardly felt it's effect -- but I did
sleep well, until I woke up at 7:30ish and had to get up.

Wrote a note to myself about this idea on scrap paper on my desk.
Just took a picture of it with the phone becuase I feel this is so obovius
it's going to be great -- and could be the last major missing piece of
the puzzle? (last - ha ha ha - it always feels that way but there is
always something else missing). I feel like I"m making history here
and need to record it as it happens!

Coding this is trivally simple. So I'll go do that.

What to collect statistics to see how it compares to the old way.

The old way is: decrease the weight of the lowest active node and increase
all others. So that would reduce the w even if the system sorted to the
lowest and even if the lowest was no longer the lowest after the sort!
It would decrease all others no matter if they were right or wrong,
making it result with the w values becoming a measure of how likely it
was for each node to be the low one. But the update is towards 0.0001
or towardsw 1.0, which makes it springy towards the middle which means
they can never adjust to the point of perfectly a predition.

That was working well for many reasons talked about above. BUT, with this
new update it might not be needed. The old update was trying to equalize
volume, not maximize predictive power and there were good reasons not
to totally equalize volume becuase it would create too many degrees of
freedoms and allow the network to randomly drift though paths of all
possible solutions. That problem is also why I added the the distance
Bias. But, with this time based system, using all past updates to predict
what comes next, (that's what this will do), it will never be able to
pefectly solve the problem). So the solution is inheratly "springy"
to start with! This is there ren't an infinit number of solutions --
there's not even one that is "perfect". But there might be only one that
is "the best we can do given what we have to work with". So maybe thw
egith updates will need to change from the 0 to 1 to something else --
or how they work might need to change.

So I'll need to track some statistics to compare how this update is
different from the one I'm using. Like calculate a ratio of how many
times the net correctly predicted the sort (the lowest output was where
it was sorted to) using the old learning update vs the new to see if the
new is actually making it a better predictor or not. This could be all
nonense and the old update if low only might do just as well as the new
one! (but I'm betting the new one is correct).

From the first time I implented the current w update, I was suspecious it
wasn't as good as it should be. It was a quick "copy"/"mirror" of what
I came up with for the b value updates and though the v value updates
seemed briliant, the w updates didn't see briliant, they seemed lame.
Now, this idea is feeling briliant for the w updates! The b updates are
just measuring the odds of whether a q value is in the lead. And the
old w update is measuring which output was falling behind. But this
new update makes the the output nodes the best predictors they can be,
which is exactly the foudnational power of deeplearning and what makes
for a good perception system.

Enogh talk, lets do some checkpoints and write and test code...

[9:28 AM] First test results. 2 nodes, TwoNodeOverlapTest. One layer
deep.  rewards() turned off. New updates are blancing outputs but when
I look at the sd measure of how well balanced the outputs are for each
test period, the old algoirthm is working better! The old algorithm gets
sd of 22 to 34 range. The new algorithms has Sd of 33 to 48 range.
But this is not surprising. That measure was to measure how well the
system was equalizing volume, not how well it was doing at maximizing
predictive power.

So, now, I need to trace predictive power and see if the new updates
are able to make it predict any better than the old! In other words can
it even learn to do what I'm asking it to do! or is the old update just
as good?

[10:55 AM]  Went out to starbucks. Ice everywhere this morning, but it's
40 deg ish and raining so it's melting fast. Got to put my new LLBean
STABILicers to the test that I got as a present from Margaret for xmas.
Strap on rubber crampons with round steel tips. Works create! Though
they would be dangerous to any flooring they are great outside on the
ice!

Left 4 test runs going. They have run about an hour now. Added a
count of %rigth and %wrong for how many times pulses get sorted to the
lowest value output. Very cool statistic beucase it's a highly stable
measure of progress. Much better than my attempt at measure SD of output
pulse blance!

And yes, it does work better for the new algoirthm, but only slightly!
So the old one was on the right track, but the new algorihm does better!
Or at least, the new one has converged to a higher number faster.

The old one after an hour has converged to 16.3% right and 16.4% right.
But they might both be still creeping upwards! The new algorithm,
running the same time, with all other paramaters the same, has reached
18.2% right on both runs! This is with 2 nodes feeding a fan out of 5,
into a 7 wide network, so they have 3 overlapping output nodes and 2
nodes on each side feed only by one node.

I'm going to disbale the row pulse output counting and calculating
of SD becasue that's not nearly as good a measure as this new one.

And do more testing to see how these two algoirthms compare. It could
be that even the old algorithm, will converge on the same answer given
enough time, or given a slowly decaying learning speed to help it but
that it takes longer becuse it's constantly making changes that are
incorrect and adds a lot more noise to the learning process than it
needs to! Or maybe, the old algorithm just can't match the new at all?

Fun fun.

[11:30 AM] more tests running.  3 layer net this time.  so far new w
prediction algorithm is only doing slightly better. 12.77 vs 12.37.

I wonder. If a lower level is a good predictor, might it be harder for
later levels to be good predictors because the lower level starts to look
more random and harder to predict? I should try tracking prediction stats
per level! Or per node even to see what nodes are better predoctgors
looking back or looking forward (two different ways to track).

But, the intent here, is for each layer to maximize it's information
about the previous inputs (but over the entire halfLife history), and
if one layer is a better predictor due to the better update algorithm
it should beter represent the full truth about the prvious level, and
in turn, be less random (less determined by the stocastic random picks)
and easier to predect in the next layer. So maybe each layer does get
harder to predict, but with the imporved update algoirthm each layer
should be better than the bad algoirhtm. Maybe. Letting tests run to
learn more about this approach...

[12:16 PM] The single layer test seems to be mostly converged after
2 1/2 hours of running. The new algorithm hist 18.2% correct. The
old was 16.4%. That's what it was after an hour as well so no
real change in the last 1.5 hours. Going to try different
half life now. That was with 300 sec half life. Going to try
3 secon half life and see what happens to both of those with single
layer network!

So far, the new algoirthm has always beat the old one in this %correct
stat.

[3:21 PM] Took a two hour nap. :)  Got up too early and didn't sleep
well enougn.

The 3 layers network (2 layers of links), got 12.63 right for the old
algorithm and 13.27 for the new.  200 halfLife.  and 13.21 for a second
run with new algorithm.

-------------------------------------------------------------------------------------------------
With halfLife of 3.0 a 5 layers of links gives this:

XNET Tue Feb 16 12:49:34 2016  RunTime: 2:35:21  Real-Time Simulation speed=2000X  [ on short half life ]
p1 freq:  1.0 ==========
p2 freq:  1.0 ==========
Two Node Overlap Test row1: -1, row5: +1 7 x 6  time: 5178:20:00.0000  PPS  7001  RPS 0.000 0.000 
Explore: 1.00%  Q: 0.00001000  B: 0.00001000  W: 0.00001000  halfLife: 3.0
  .             |  2o55654.02100 |  3o65555.61000 |  .o65555.40300 | 19o65456.31300 | 15o|    193 ===============
  .             |  .o66544.21001 |  2o65545.20000 |  1o65455.04000 |  .o65455.23000 |  .o|    353 ===========================
 19o86333.10301 | 23o65445.10014 |  0o65445.00000 |  .o65455.40000 |  .o65456.30000 |  .o|     65 =====
  .             |  3o65456.00130 |  7o65456.00006 |  .o65456.00004 |  .o65456.00004 |  .o|     15 ==
 19o33368.30100 |  9o54456.01302 | 18o54456.00062 |  2o65456.00040 |  .o65456.00032 |  .o|    268 =====================
  .             |  .o44555.13021 |  1o54556.00510 | 29o55456.00304 | 19o65456.00313 | 15o|    247 ===================
  .             |  .o55555.30210 |  7o55556.05100 |  7o55555.04040 |  .o65456.03130 |  8o|    959 ========================================================================

Percent right by column: predict: on
 1.16%  1.11%  1.11%  1.11%  1.11%
 1.16%  1.11%  1.11%  1.11%  1.11%
 1.16%  1.11%  1.11%  1.11%  1.11%
[more of same]
-------------------------------------------------------------------------------------------------
And with predict off:
-------------------------------------------------------------------------------------------------
Percent right by column: predict: off
 1.16%  1.09%  1.10%  1.10%  1.11%
 1.16%  1.09%  1.10%  1.10%  1.11%
 1.16%  1.09%  1.10%  1.10%  1.11%
 1.16%  1.09%  1.10%  1.10%  1.11%
 1.16%  1.09%  1.10%  1.10%  1.11%
-------------------------------------------------------------------------------------------------

The predict off  (old algorithm) was ahead at the start for a long time,
but after letting it long the new algoirthm seems to have converged on
the same (bad) answer but the new algorithm did better on deeper layers.

Need to test with more values of half life.

with 30 half life, the prediction is startting off in the 6.x range.

I think if i make w range from 0 to infinity and make the adjustment
towards zero, and away from zero, this will allow the algoirthm to set
better values for w and produce higher results.

But it will have done it by removing the spring from the w values,
and will allow the w I believe to totally override the b values and
q learning and in effect, disable the q learning. I should test and
verify if both or etiher of these ideas are correct.

If it does help it predict better, it might be good to try and make
it work.

To preventit from overriding the b values, there might be a way to bias
the w to prevent that. I've tried the idea of using b as the target for
the w to optimize for. But that was inherently wrong beduse b controls
flow volumes, not total balance (or prediction power) so doing that sets
invalid node activity targets.

And I used the +.0001 trick to limit the imblance that w can create.
But this offset kludge always felt wrong.

The idea will need to be, maximize predictive power, given the flow
balance as defined by q. And beucase flow balance will mess with node
volumes, using "sort to low" will get broken. I think.

Is there a way to make the system predict something other than the low
value predicting next sort that would not interfer with with the b values?

I'm wondering about biasing learning weights with b? If b sets ratio
to 10:1 for otputs A and B, then if the node sorts to A/10, it will be
"wrong" a lot more often, and if it sorts to B/1 it will right a lot
more often. Can we bias how reward learning rates happen based on the
bias the offset expects? Make the updates for A a lot slower, than
the updates for B for exmaple? So all the "mistakes" made for sorting
to A are not pentalized as much as a mistake made for sorting to B?
Or that rewards for A are not over rewarded compared to the rewards
for B? Maybe. Doesn't feel instantly correct, but maybe... ???

it might at least allow b to overpower the effects of A trying to push
outputs back to balance?

But the problem is that the other nodes can send pulses to these same
nodes at high volumes due to other q value reasons, so the balance
of the outputs really will have nothing to do with the b values for
only our node.

I guess another idea, is let w values seek what they want, per the
update rules, but do something else to allow the b values to dominate
like the more unbalanced the b values become, the more power they have
over the w values?  b**2 * w?

Ok, for now, I'll keep looking at w updates and effects, and worry about
q learning after I'm more confienced these updates are good.

[4:52 PM]  here's an idea.  w learning balances sort "errors" based on
this idea that the next layer is predicting the sort with it's lowest
volume node. We can think of the b varible as setting an error balance
target for w to sort to. Instead of adjusting w to making equal number
of error between two nodes, a 10:1 sort ratio by y says the 10 output
should have 10 times more prediction errors than the 1 output. So q
learning will increase the prediction errors. Maybe, b can just define
the error target for w, and w does all the work adjusting flows to hit
that error target?  soo 100:0 means 100 errors to first output (it's
never the the lowest sort path). so an error to the 10 output, is not
punished only 1/10th as much as an error to the 1: outputs? If we get
it right, it's not updated now (not punished). If we get it wrong, we do
a punish update. So the idea would be to weaken the learnign update by
the b value. So every time a pulse goes to the low volume and is wrong,
it's punished 10 times as much. This is in line with what I was sort
of thinking above, but seems more rational when described this way.

But, I'm not sure if this alone could push the sorting balance as strongly
as the b learning needs to -- 100%? I'll have to test later to see.

[5:12 PM] Ok, so we push w for not sorting to low. But if we can use
the b offset to bias that punishment, so that sorting to the one
b does want, is a much weaker punishment, then maybe we an make
w values control all the sort. Or at least, not worry about the
w values undoing the work of the b value?

So lets free the w to seek any value needed from 0 to +inf to
adjust outputs and see waht happens. Sort from and away from
zero to do this.

[8:58 PM]  30 halfLife  fanout 5 - testing 0-inf vs 0-1

7.44% right 30 halfLife 0-inf vars 32256 w values

7.59 first colum with 0-1 vars 33367 w values
7.59 on a second run same

7.36 andor 7.37 same as above 0-inf with 10x slower w learning

5.15% fanout 7 - full width
4.86% fanout 7 same as above I think

Larger fanout has smaller % correct. I guess the odds of picking the
wrong output are just higher. Smaller halfLife makes %picked go down
as well.

The new 0-inf w values show lower percent correct as well. Which strick
me as odd and almost impossible. What on earth is happening?

Need to do more of the same runs to see how much variation there is from
run to run.

================================================================================

2016-02-17 9:18 AM Wednesday

Yesterday

1) Coded the new w update rule of punish for not sorting to lowest output.
Do nothing when sorted to the lowest. When sorted to something other
than the lowest, - that path, and + the lowest path. The idea is that
this trains the sytem to predict where the next pulse will go.

2) Added statistical tracking of number pulses sorted to the lowest
output vs number sorted elsewhere.

3) Switched w values from 0 to 1 range to 0 to infinity and tested.
Seems to produce slightly worse 0 sort statistics for reasons I don't
understand. I thought it would do better or just the same. Likely breaks
q learning, but I've not tested that.

4) Did lots of long testing runs. The new w udpate algorithm does
beter than the old in terms of producing better 0 sort error statistics.
Not surprising since that's what it's traning the net to do. But is it
better for our application or not? I don't know. And the improvement
is small. In the rage of 10% better statistics.

Today

Left 4 tests running for the past 12 hours. TwoNodeOverlapTest.
2 input nodes cycling through 4 dual frequency paris. Fan out of 5.
Net width of 7.  2 layers.  halfLife 30.

-------------------------------------------------------------------------------------------------
XNET Tue Feb 16 21:09:29 2016  RunTime: 12:19:56  Real-Time Simulation speed=2000X  [ on 0-inf w ]
p1 freq:  1.0 ==========
p2 freq:  2.0 ====================
Two Node Overlap Test row1: -1, row5: +1 7 x 3  time: 24664:50:00.0000  PPS  4999  RPS 0.000 0.000 
Explore: 1.00%  Q: 0.00001000  B: 0.00001000  W: 0.00000100  halfLife: 30.0
  .             | 10o33335.00023 |  1o|     42 ====
  .             |  0o43343.00131 |  8o|    401 ===============================
 20o65223.40111 |  6o53333.02211 | 12o|    418 ================================
  .             |  7o43334.22111 |  3o|    357 ===========================
 10o32256.11000 |  4o33335.21100 |  4o|    116 =========
  .             |  1o34334.11000 |  2o|    124 ==========
  .             |  1o53333.11001 |  0o|     42 ====

Percent right by column: predict: on
 7.45%  4.80%
 7.45%  4.80%
 7.45%  4.80%
-------------------------------------------------------------------------------------------------

They all converged on nearly identical numbers which is nice. That was
the point of the test to see how close the convering was. from run to run.

The first number for the first column was 7.45 or 7.46, and the second
was 4.80 or 4.81. And these were all with the 0 to inf w values.

The old algorithm would converge on w values only accurate to about 2
places when the learning rate was slowly dropped to 0. I wonder if this
update converges more consistenly?

And, how long does it take to reach these values? Starting another run
over to see how long it takes to get to 7.45 ish.

Anyhow, from yesterday, I had high hopes that this new update rule would
perform some great improvement in the network due to it being based on a
prediction idea. But I don't think it has made much real difference at
all. I do feel it's better and I like it, but I don't know if it really
is better or not. The old update was reward the smallest value and punish
all the others and ignore what sorting was done this time (of course
the size of output node values is a measure of past sorts). Because all
updates were the same, and on a blanced net, each path would be rewarded
once and puinshed fanOut-1 times the w values were lower than the new
update, where the reward was an balanced number of updates for each path.
I need to test using the old algorithm, but adjusting the size of the
rewards to create blance, and see if it converges on the same solutions
as the new update rule! They might in fact produce identical results.

Another possible update rule to explore is punish for sorting to the highest
and do nothing for all others. This is sort of a mirror of the same idea
as punish for not sorting to the lowest. It might work just as well
or maybe it will produce a slightly different answer? It would give
the system more freedom to sort to other nodes.

One balances the system from the bottom, the other balances it from the top.

The larger question to be answered, is does this system do what it
needs to do? It was not solving the binary to 1 of N training problem.
Could it even hope to solve that>  What should the perception system be
forming in the way of nodes to make traning that possible?

So first, I need to test these different sorting system. Or maybe not.
Get some tests running, then think abot whether this entire approach
has the power I needs to have!

[10:20 AM] back from Starbucks and Ingress run.

Another update rule possiblity. Punish the highest and reward the
smallest output for every update. This creates a balanced update and
optimizes for equal amount of time high and low. Seems better balanced
that way. The other two balance the time low, or balance the time high,
but not both. Equal time high could create a configuration where some
nodes spent more time in the high state than others. And equal time
high could alow for varrying times in the low state.

It seems to me, that nodes in the low state are in effect "unimportant"
and nodes in the high state have the most power and importance, so maybe
due to this, balancing time in the high state is what we should be doing?

But maybe, all these end up with the same solutions (or nearly the
same) anyway because they all balance the number of pulses being sent
to different paths?

[12:06 PM] Coded a slightly different version of the update from above.
I punish the highest and reward the lowest, but only do it each time a
pulse is sorted to the highest. So this is both a system to equalize
the high states (which seems the most important since high has the most
power to effect behavior), and I only do it, when pulses are sorted to
the high, so I'm traning it to not sort to high (which offsets the fact
that policy biases behavior to sort to the highest output).

It's probably not so important to only do this when pulses are sorted to
the high side, but it seems more valid, and slightly reduced the amount
of traning happening making it a slight be more CPU effecent.

Trying to do a little testing to see how this is different from the
other variations I've coded and thought of. But the tests takes hours
to stabalize so it's slow.

Also switched back to the 0-1 w values becuse for reasons unknown, then
seem to produce beter statistical results, and more important becuase
they are know to play well with q learning.

I would like to see a ton of tests done on all these different w
algorithms and get to the bototm of how they are are different, but that's
going to take a long time since all the tests take so long to converge.
So I'll try to do more of tha tover time. But for now I want to swtich
my attention to whether this entire appraoch of this combined RL, and w
and policy really do what it needs to do to solve the problems it should
be able to solve!

So, I tried to get it to learn to decode from two binary signals, to for 1
of 4 outputs. It showed no ability to learn that. Why? Should it be able
to do it? Does it need feedback to learn that? Was I asking it to learn
something it can't learn but maybe it can learn something equivelant to
that? Maybe the net wasn't large enough for what it needed to learn?

Roughly, my thought was that the perception learning would have created
nodes that were basically alrady decoding the state to 1 of N. And if
it was a 7 wide network, it would have decoded the state to 1 of 7. And
since the input was cyclig between only 4 states, the nodes would have
taken on values that represet those 4 states as 7 different overlaping
state signals. After all, the point of the nodes, is to reprsent the
tempral state of the environment, and my environment only had 4 different
states that were repeating over and over -- that could be chopped up
into as many smaller temporal slices as one would like to the limit of
the pulse frequency.

So, what is the percpetion learning actually doing? Is it really creating
nodes that represent state?

Or have I created a random number generator where most of what the
nodes are doing, are only amplification of the random noise generated
by my random() picking of paths? So that which nodes go high, is just a
matter of luck as to which nodes get sorted where vs a true measure of
the enviornments state?

I feel the need to draw pictures and use those to think with. Can't do
that here.

[12:58 PM] Oh, just edited all notes to remove double spaces after periods
and and ! and ?. Trying to learn not to do that anymore. Even though in
this fixed with file it might will be valid. In variable with fonts it's
not valid anymore so I'm trying to train myself to not do it anymore. But
it's so deeply trained into my typing habits it will be hard not to.

Anyhow, on the pictues, I realizesd the problem I have is one of
negatives. If I tried to make an output go high, when X and Y inputs
are low, my net is not really able to do that today. Or at least not
well. The approach will require that the system sum together a lot of
other random signals at low levels, and modulate that with negative of
the signal I want. Thats should be possible in theory. Since signals
sum if I create a nutrual signal by summing lots of noise, then regulate
one path as amplification of the control signal, the other path should
become it's negative, and be routed to where it's needed.

So, things to verify with my network design. Amplificaiton! One signal's
effect should be amplified in the network by the b value settings. And
second, negative signals should be easy to create.

I'm wondering if w values need to go negative to help this? With the
current appraoch, outputs fade from not being used. But there is reason
to wonder if a given input signal, should have the power to make an
output fade faster than that? So that no pulses routed to a node will
cause it to fade slowerl with a long halfLife but that there's a way
to route negative pulses to the node in effect to drive it down fast
(aka this pulse needs to be under stood as postive evidence of NOT a CAT?)

When using Bayes rule and multiplying probablity distributions, all
that is needed is a zero value input to indicate zero probablity of
that option. Zero times anything is zero. So a zero output (no pulses)
is all that should be beeded if the system is in fact doing probablity
distributions correctly). Are we doing it correctly?

If we are, that means the halfLife we pick limtis how fast it can forget
the old short term state. Suddenly sorting all pulses to A will cause
B to drop and A to rise just as if we were sending a negative to B
(in a sense).

Ok, but wait, I shouldn't think of the nodes as snap shot of the current
state. It should be a short term memory of history. So a box opens,
then shuts. But I need to remember it was just open. That's the point
of the slow fade. To retain the memory that it was just recently open.
So the sensory data coming in is endicating "closed", so the "closed"
state is quickly being registered. But the the system still has memory
that it was recently open. So if I have to inputs X and Y, and X is
active and Y no, it is registering this effect stronger in some nodes
than others that the state is X. And when X goes quite and Y turns on,
states switch to a Y state, but some faster than others. So if we had a
bit net, the volume could swing left and right across the net to indicate
which of the two inputs was stronger or where it was in the sequence? I
think i need to build a biger net and show output values only and see
how data flows through the net as inputs change.

But I sense it's going to have trouble detecing 00 as different from
11. I sense these two things reprsent the same state not two different
states. It needs other inputs to create a difference. 1001 is different
than 1221 but otherwise, 00 and 22 would look the same, one just just
twice as powerful belief in the same thing.

So I don't think we need negative w values now. But to detect 00 01
10 11 the net needs more inputs to tell 00 is different from 11. And I
tried taht with adding a volume equalizig inputs, and that didn't allow
the network to learn either. So that's not the only issue at play here.

The amplificaiton needs to work as well. b values and q learning needs
to detemine not just simple pulse routing, but pulse amplification
effects. But I think the net does that. Becuase if b values causes
all pulses from X to route to A, than A will "suck" lots of data from
other shared inputs due to *a policy. And w learning should mix all data
together to create what is in effect a grey goo from which the q values
can amplify the signals that are important.

Ok, let me code a new test for a big net set up to watch how output
values flow through the net.

[6:55 PM]  Went to pizza, played some ingress. I have 7 test runs soaking
up 100% of the CPU of my 4 processor box. Need more CPU! Need bigger and
or more screens! Been waiting for them to stabalize.

Three 2 input single layer tests running. Testing the new w update
rule of punish all sorts to max output. Dec max path and increment
min path.

After 7 hours the first two are showing:

Sorted to min: 7.75% and 7.73% alone with 63.97% and 63.96% not sorted
to high.

The one test of the push not sorted to low, is 7.65% sorted to low,
and 64.10 not sorted to high.

This is an odd result becuase it's backwards. The one trying to train
the system to sort to low, got a lower sort-to-low score than the one
tryign to train to not sort to high. And the two trying to train to not
sort to high, got lower not-sort-to-high scores. This stuff is so weird.

And, before, the one using full 0-inf with no "spring" limitations got
lower scores than the ones capped to 0-1 with "spring" got backwards
results from expected. The one with more freedom to overfit a solution
got a worse result (it failed to overfit).

I have no understanding of why these are working this way. Maybe a bias
created by the real time training? I'm thinking that it gets close but
then constantly makes it worse with eaca training update for the next
pulses so it bounces around the optimal answer or something? They seem
to take forever to converge and I can't justfiy running these for like
24 hours each with very slowly decaying w learning speeds to try and
find out what really happens. Nor does it really matter.

Both of these results are in the same ballpark, and both are better
than the old algoirthm I started with of always increasing the weakest
node and reducing all the others. But they are all so close, I honestly
don't think any of these are really wrong. I bet they all bascially work
the same in the long term since it's the RL training that fine tunes the
ultimate "solutions" and this percpetion learning is only needed to get
the data shared across all the nodes -- and all these algoirthms do a
good job of that basic task.

I'm going to leave these three running to see if they drift to diferent
values over night.

I also created TwoNodeBigNet to test larger nets. Have a 10x10 and a
21x20 sized net running. Only displaying node values to make it fit fine
in the window. Very intersting results so far.

They data flows through the net in rivers that sway and whip around
and sometimes split into multiple flows. Very cool. Was hoping it would
do that.

BUT, with a half life of 30 seconds, the rivers are changing path
amazingly slowly. Sections of the network can remain idle with no
activity for for simluation times of an hour or more on the larger
21x20 net. The larger the net, to longer it takes the "river" to swing
around. This just means that the later parts of the net have no hope of
representing in put state -- the swining is just random. My intput state
is cycling through 4 frequency configurations changing frequency every
300 seconds (5 minutes) so making a full cycle in 20 minutes. But the
net is taking hours to drift through different "river" configurations on
the output side.  But then again, this is what I was asking for! Longer
term temporal activity deeper into the network! But, this means that the
halfLife might be too large even for the input side of the network. It
might not be able to really create the different state representations
when the halfLife is this large? Since in a very large network, I wwould
expect there to be lots of empty inactive space deeper in the network,
that would work great to create a sparse distribited reprsentation so
as long as activityh was sparse, a short halfLife might not really limit
long term learning? If rivers move slowly but stay in the same area for a
long time, then outputs will be high in those areas and remain in memory
for a long time later to be reinforced even with a short halfLife. Maybe.

Also, I was wondering if the node halfLife that controls this behavior
patterns might need to be short for behavior creation and fast decay
of state and that the link halfLifes could be longer, since their ownly
purpose is reward back probigation? The link half life might need to be
tuned to match the expected evironmental delay between actions and real
rewards where as the node halfLife might need to be tuned based on desired
beavior reaction times and speed? to where the rivers have recently been.

Oh, wider fan out cuases rivers to spread out more. Don't have one running
long term to test how it converges. And of course, maybe a different
network toplogy that causes data to spread out in a spreading tree would
make the network work better. But these river effects I'm sure are more
pronounced in this side by side type network. The effect may still be
there in a branching tree network but just not show up vidually the same
way. More testing required later. Stil just trying to get a feel for how
these networks work. But I'm loving what I'm seeing. Looks good. Just
as it should.

[10:58 PM] Activted workspace switcher on Lynx to give me more room.
System however locked up in non responsive mode I couldn't recover from
and I lost a few terms. But letting it time out and log out forced it
to recover.

Lots of playing with different size nets and different fan-outs and
different half lifes. All seem to be fine.  Small halfLife with small
fan out (3) makes it very hard for net to learn to swing back and
forth! Takes a long time to swing far enough up and down to be able
to train w values to allow more swinging! I sense it will take some
experimenting to understand what values work well together for different
problems. Starting to test RL with big networks. First think I noticed
is that rewards are VERY expensive and slow it down a lot. So I coded
it to only reward randomly every so often.

================================================================================

2016-02-18 9:19 AM Thursday

Yesterday

1) Tested the new w update rule of punishing not sorting to low.
This updates for all pulses not sent to the lowest current output.

2) Came up with a third new w update rule of punishing sorting to high.
This only updates when pulses sort to the highest output and punishes
the highest w value path and rewards the path to the lowest (sort less
to the highest, more the the lowest). Did lots of long testing runs with
this update.

3) So I now have 3 different w update rules I've been playing with and
two different w value ranges of 0-1 and 0-inf.  And each test takes hours
to converge to figure out if it's any better or worse. And in truth,
I'm not seeing substabtial differences in any of them.  But so far I
like the last one the best.  It only punishes the high output and only
rewards the low output. It updates less offten than the others so it's
a lower CPU overhead.  Both the two new rules seem to work better than
the older one so I'll stick with this new one. All probably work well
enough to get the job done of roughly balancing the network outputs.

4) I'll stick with 0-1 value w values for now becuase I think that's
better for q learning.  Maybe I'll test that some to verify today.

5) Added more statistics to test to show number of pulses sorted to
not high (correct count for 3rd w update rule). Oddly, the new update
rule that trains not to sort to high, gets a better sort-to-low score,
and the sort to low update rule, gets a better score for not sorting to
high. This stuff can be so counter intunitive at times.

6) Created TwoNodeBigNet for testing nets 11x10 and 21x20 size range.
Only displays output activity level. All other net.show() output has been
tuned into method arguments and turned off. Got to see for the first time
how this net operates when larger. Rivers of activity form and wander back
and forth, spilt, and merge back together.  The wandering of the rivers
are far far slower than the halfLife would suspect, making me think I
need to use very small halfLife values (like 1 second). Experimented
with different fanOut settings -- large fanout makes the rivers spread
out more.

6b) Trying to understand why the network could not learn the binary
decoding problem and what it needs to be able to learn that.

7) Set up on RL tests on the big nets and let them run overnoght.
None of them worked!  RL isn't working on the big big net for this
simple test. Need to test more and understand what is and sin't
working? Does RL become very hard and slow as the net gets larger?
RL is very CPU expensive for the big nets -- did see that in action.
a 21x20 net with a fanout of 7 has 2940 links to update for each call
to reward(). I slowed down rewards using random to only 1% of the pulses
to combat the CPU load -- but did I break the learning in the process?

8) Removed double spaces after sentence puncuation. Trying to learn to
not type two spaces after periods! It's very hard to kill such an old
habit. But I'll keep practicing. Yes I will. Just keep practicing. Nothing
to do with AI of course.

9) Turned on workspace support on the computer. Didn't know it was an
option. Then, instantly it got confused and locked up when I tried to
move big terminal windows to other screens. It's got bugs with termninal
windows that are larger than the screen. It was stuck in a mode that
wouldn't allow me to finish moving the termal, but still had control
of the keyboard and screen. Nothing would get it out. But letting the
auto lock time out and lock the screen reset it.  I need a second biger
screen for all these displays.

Today

1) Have an all staff meeting at Mount Vernon for 4:30 today.

I had high hopes the improved w update rules would be magically better.
So far I'm not seeing anything huge. The old rules got the job done.
The new rules I think work better and converge faster with less updates,
but still end up converging to the same ballpark solutions of roughly
balancing the activity by "fighting" agsinst the "sort to most active"
policy rule. Since the sort to most active policy is inherently unstable
trying to counterblance that seems to be a juggling act with no perfect
solutions. But the balance from the top by rewarding the bottom feels
like the best of the lot so I'll stick wtih that for now.

Have to go back and test some simple RL stuff to see if it's still
working and how it works with the new rules. And then test with the
larger networks.

Still on the trail of trying to see what these networks can learn and
why it couldn't lean the binary decoding problem.

Net to create sipmler tests to work up to the binary decoding problem
to understand what it can learn.

Already been out to Starbucks today!

Found one of my lost terminals (they were oddly lost in the lock up
problems with the workspace manager.  Made them come back by killing
terminals to find out what was still eating up CPU!

And look, one of them actualy did RL learning! (though maybe not correctly?)
WRONG -- WRONG -- did not learn RL was turnned off!  Just very slow moving rivers.

-------------------------------------------------------------------------------------------------
XNET Wed Feb 17 22:34:57 2016  RunTime: 11:30:45  Real-Time Simulation speed=1000X
p1 freq:  2.0 ====================
p2 freq:  0.1 =
Two Node Big Net 21 x 20  time: 2498:32:30.0000  PPS  456  RPS 0.000 0.000 
Explore: 1.00%  Q: 0.00001000  B: 0.00001000  W: 0.00000100  halfLife: 1000.0
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   0       3 =
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   2   1   4   3      42 ====
  .   .   .   .   .   .   .   .   .   .   .   .   .   2   1   4   2   3   1   1      15 ==
  .   .   .   .   .   .   .   .   .   .   0   .   3   1   3   0   1   .   .   .  
  .   .   .   .   .   .   .   .   .   1   0   4   1   2   .   0   .   .   .   .  
  .   .   .   .   .   .   1   .   3   0   4   0   1   .   .   .   .   .   .   .  
  .   .   .   0   0   2   0   4   0   3   .   0   .   .   .   .   .   .   .   .  
  .   .   2   1   3   0   4   0   1   .   .   .   .   .   .   .   .   .   .   .  
  .   4   3   4   0   4   0   1   .   .   .   .   .   0   .   1   .   2   1   7      49 ====
 11   4   4   2   6   1   5   0   2   .   2   .   7   0  12   4  14   8  13   7      42 ====
  .   4   3   5   2   6   0   8   0   9   1  13   1  14   1  10   1   5   1   1       7 =
 10   4   3   2   4   0   6   0  12   1  12   1   6   0   1   .   0   .   .   .  
  .   4   4   3   1   4   0   5   0   4   .   1   .   0   .   .   .   .   .   .  
  .   .   2   3   4   1   4   .   1   .   0   .   .   .   .   .   .   .   .   .  
  .   .   .   .   1   3   0   2   .   0   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   1   .   2   0   1   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   0   .   1   1   1   0   0   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   0   0   1   0   1   0   1   .   0   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   1   0   2   1   2   0   2  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   0   .   1   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
-------------------------------------------------------------------------------------------------

[EDIT LATER WRONG WRONG WRONG -- no RL]
The "three rivers" were locked into this configuration by Rl. All it
was being trained to do (as far as I remember) was a reward for sorting
P1 pulses (the top of the two input pulses) to output 1 (second from
top), and punished for sorting p1 pulses anywhere else. P2 pulses were
not punished.

The other tests had extra inputs to do flow blancing that probably made
the learning harder. But this was also a very large halfLife of 1000 vs
30 or 1 on the other tests. And this has a small fanOut of 3 instead of
7 or 11 sort of numbers.

Yes, checking the code, it was +1 for sorting to the correct place and -1
for sorting to the wrong place. The RPS is showing 0, which is odd that
it's exactl 0! But that implies that half the pulses were being sorted
to the wrong place but that 50% were going out the correct row 1 output!

So a nework with 20 outputs, can't balance across 20 outputs and still
sort 50% of the input pulses to a single output. I wonder if that's an
inherent issue here? That is, if you try to train it to do something
like that it ends up doing what we see above of reducing all flow down
to a few paths? Maybe the trick is only traning the net to shift the
balance to adjust ratios and not try to push anything to 100%?

And maybe the long halfLife for the links is important to qLearning
where as we might be able to use short halfLifes in the node for behavior?

Oh, shit, I was totally wrong!

here's the same run a few miutes later:

-------------------------------------------------------------------------------------------------
XNET Wed Feb 17 22:34:57 2016  RunTime: 11:58:33  Real-Time Simulation speed=1000X
p1 freq:  0.1 =
p2 freq:  2.0 ====================
Two Node Big Net 21 x 20  time: 2516:37:30.0000  PPS  456  RPS 0.000 0.000 
Explore: 1.00%  Q: 0.00001000  B: 0.00001000  W: 0.00000100  halfLife: 1000.0
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   0   .   2   .  
  .   .   .   0   1   .   .   .   .   .   .   .   .   0   .   5   1  10   3  12      44 ====
  .   .   2   3   0   2   .   0   .   0   1   1   6   4  11   4  12   1   8   1       3 =
  .   4   2   2   6   2   6   4   6   8   9  10   6   8   1   4   0   1   .   0  
 10   3   4   4   2   8   5   8   6   4   3   1   1   0   0   .   .   .   .   .  
  .   6   3   2   3   0   1   0   0   0   .   .   .   .   .   .   .   .   .   .  
 11   3   4   3   1   2   .   0   .   .   .   .   .   .   .   .   .   .   .   .  
  .   5   3   2   5   1   4   1   1   1   0   0   .   1   .   1   .   3   0   6      85 =======
  .   .   3   3   2   6   4   8   6   5   7   2   7   1   7   2   8   2   8   1      17 ==
  .   .   .   1   1   .   1   .   1   3   1   6   1   6   1   5   0   3   0   1       9 =
  .   .   .   .   .   .   .   .   .   .   .   .   0   .   0   .   0   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  
-------------------------------------------------------------------------------------------------

The rivers are not fixed. They are just very slow to move becuse of the large halfLife!

And there seems to be no traning at all. That's why PRS is zero!

[12:29 PM] watching 2 layer bignet train. Very slow and painful due to
the fact that it has learned the wrong thing, due to superstition. That
is, one path was stronger, but wrong, and then the rest of the the net,
found a good answers unrelated, and average RPS jumps us. But now the
path with the strongest flow is moving to the new higher average faster
than the rest, causing it to falsly look better -- and it stays that
way for a very very long time as all the values slowly converge on the
new higher average.

But, idea struck me! Within a fanOut, we shouldn't be training one
link faster than the others! And I've thought about this seeking to new
average problem, but I see there's a fucking solution! Within one node,
adjust training rates based on EXCPECTD FLOW -- aka b*w*a! So the fanOut
combined as one average, will train at a rate relative to their combined
activity, the individual links will train at a rate adjusted for their
expected average! So the random variation away from expected average will
be what controls how fast one trains vs another! And that should better
correlate with actual results! Factoring out the effects of drifting to
higher and lower averages unrelated to behavior all together!

I'm excited about this! Had to write it down before coding it!

So reward update speed is a(link)/b*w*a(output). But, minor issue. If
b*w*a*() is not a true normalized probablity distribtuion over the links,
then q values will get an update bias that might be harmful making
one set of qvalues move faster than another node's q values? Not sure
if that's an issue since we don't do much back prop though the net.
Lets code and test!

[2:04 PM] Coded and tested the new reward adjustments. Decided I went too
far by including all the policy adjustments! a(output) is a short term
flow bias that has important effect on creating the variance in the system
that helps traning. By factoring it out with this it makes it too hard to
learn! So I reduced the policy to only include b*w*distanceBias. Oh, and
in the above I forgot about the distanceBias. Had to include that. Added
a policy var to links to record the last policy used. Then stopped using
that when I decided to not use a(output). But maybe I should use it and
remove a(output)? Or just change the code that records it to factor out
a(output)? So far it's looking very promising but still needs adjusting
and experimentation to see what works best. But it really does help to
factor out the superstion learning effects that cause the net to get stuck
on false beliefs. One run using old code got stuck on false beliefs
and took 171 hours to reach 3.0 in RPS after getting itself unstock,
and the newest code, did it 125 hours using a qLeanringSpeed that is
1/10 the rate!

Need to see if this suffers from a colapse problem as it converges
however. Waiting for long runs to converge now. Will have to take off
for MountVernon soon for the meeting. Maybe it's time for lunch now?

[8:12 PM] Back from staff meeting at Mount Vernon.

All three of the RL tests I had running failed to converge and stay
converged. All had converged to the correct answer to some extent,
but all also then failed to stay converged.

This idea of factoring out flow bias differences in links seems like a
good one. But it's not working well.

This larger problem of all q values tending to converge on the average
qValues seems like a much bigger problem at work here.

But now, changing halfLife down to 1.0, made it all work better. And
I'm remembering now how I decided the real cause of the colapse was
having a long halfLife setting for a problem, with instant feedback,
which calls for a very short half life setting. Trying to solve it with
a long half life creates the problem that the solution only lies hidden
down in the 3rd or 4th decimal place and if the answer is hidden there,
then we must use very slow qLearning to find it.

Maybe, long halfLife is never the answer. And the way these problems are
solved is not a long delay between actions and rewards, but with strong
perception systems that correctly identify the context for the learning
in a sparse network. So reward effects are always strong but sparse.
It quickly lights up the part of the network that is active when the
reward happens, and then through secondary reinforcment the knowlege is
passed back in time with experience to the things that lead to the bad.

That actually sounds like a poential answer. So the network is always
driven with a short half life -- which answers how the world state is
allowed to shift quickly. But it forms complex rivers of activation
to define state, and reards are burned into those rivers at the time of
the rewards, and those rivers keep back proping reward to others through
secondary reinforcement.

I conceptually like the new reward adjustments for bias to reduce learning
speed errors within a single fanout, but it might not be that important if
we keep half life short. So to learn more complex longer term thigns we
need larger networks that create long term sparse represenations of state.
We need to test more complex stimuls inputs and see if the large nets
can in fact represent long term state with more accuracy.

And then there's the feedback effecs we have not even opened the door
on yet!

Let me try testing the large networks that were not training with
the short halflife and the new updated reward trick.

================================================================================

2016-02-19 10:49 AM Friday

Yesterday

1) Tested RL with new algoirthms on larger nets. Was having problems
until I reduced halfLife down to 1. Remembered that too long a
half life makes learning and convergence very dificult -- requiring
very slow learning rates to reduce noise to below the very small
signal levels.

2) Came up with new idea to help the problem of average reward level
drifting and creating superstitious behavior in links that happen to
have the most activity because they drift towards the new mean faster.
Making a bad link seem very good, but only because it's adjusting to the
new high average far faster. I adjust the speed of learning for links,
based on their long term policy factors now (b, w, and distanceBias).
So the least used link by policy learns far faster than the most used
link. The short term policy bias which is the activity level of the
output node, I leave out of the adjustment. This helps all links
in the same set, drift to new q values at close to the same speed,

This may just be more important when a network is first learning, to
speed up inital convergence on the evornments general average reward
level, and might not be so impotant for long running networks.

Today

Continue testing ability of larger nets to converge.

The 2 layer nets I got to converge fine once I swtiched to a halfLife
of 1 for this instant reeard problem (sort all P1 to out 1, sort all
others to anything but out 1). The short halfLife means the difference
in QW values becomes far larger between good and bad actions, even after
convergence, so it's easy for the network to both learn, and not "forget"
as q values converge close together.

Longer q values will work, but requrie far slower learning speeds I
think, and take far longer to learn. Raising the question of whether
the optimal learning speeds can be set based on halfLife? If it takes
a month to learn something, it's going to be ungodly to try and
optimize learning rates by trial and error testing.

The 9 layer deep version didn't converge after 12 hours. But I restrated
one with 3 layers deep and it learned in about an hour! But checking
it again just now I see it's swtiched to a different solution.  Did it
loes convergence and regain it, or did it just swtich internal network
paths????

The path for p1 pulses s3eems to go down to the 10 node, then up to the
15 and 15 to make it out path 1. It was totally different an hour agao.

-------------------------------------------------------------------------------------------------
XNET Fri Feb 19 09:31:32 2016  RunTime: 1:33:07  Real-Time Simulation speed=1000X
p1 freq:  2.0 ====================
p2 freq:  2.0 ====================
Two Node Big Net 8 x 4  time: 397:15:00.0000  PPS  1267  RPS 4.254 3.730 
Explore: 1.00%  Q: 0.00010000  B: 0.00010000  W: 0.00001642  halfLife: 1.0  fanOut: 5
  .   .  29  29     276 =====================
  .   .   .  15      78 ======
  .   .   .   .       7 =
 10   0  15   .       1 =
  .   5   .   .       3 =
 17  10   .   1       3 =
  .  29   .   .       3 =
 17   .   .   .       4 =
p1
 0      2 =
 1     68 ==========
 2      1 =
 3      1 =
 4      1 =
 5      2 =
 6      .
 7      .
p2
 0    136 =========================
 1      7 ==
 2      3 =
 3      .
 4      2 =
 5      1 =
 6      .
 7      1 =
p3
 0    138 ===============
 1      3 =
 2      3 =
 3      .
 4      .
 5      .
 6      3 =
 7      3 =
-------------------------------------------------------------------------------------------------

Want to try that again with a slightly longer half life. Started one
with halfLife of 10 instead of 1.

Need to test deeper nets and see where it fails or if it fails.

I also need to do more testing this new equal learning rate per fan out
rule to see if it works well or not. Is it really needed or just a waste
of CPU and memory? As I go to larger nets the extra CPU is really going
to add up.

Drat, I need to have like 100 tests running in parallel here and my 4
CPU box only does 4 without slowing them down. And it's hard to remember
what I'm testing and looking for as I get too many running at once.

I've been wondering if the way I do Q values is really the best? It is
correctly calculating expected rewards per second, which seems cool. And
that allows the system to converge on correct values for all links. But I
don't really need to know correct q values. I only need to know relative
q values for each 3 way fan-out.

And I'm still not doing secondaryReinforcement correctly. And I'm still
not sure what correct IS!

Is there a way to do relative measures of qValues that would be better
and more stable than these absolute measures? Are absolute measure needed
to do secondary reinforcement correctly maybe?

Maybe nodes could be assigned absolute qValues and links use only relative
values somehow?

SecondaryReinforcement.. In eall world problems, only with the help of
percpetion learning will the net create internal nodes, that correate with
external rewards. And that's more of a "node" thing than a link thing.

Percpetion creates useful nodes. RL builds useful links. So it seems
perception creates the signal as a node. Then RL links it to the correct
place with links. This is why w learning and polocy routing, uses node
values to create percpetion. But RL must track the usefulness (value)
of links. If we were to track node Q values we can back prop link Q
values to nodes to create it. The back prop values of nodes would equal
the qValue of the source node. But we can't back prob nodes to the links
that feed it, and get the correct qValues for the links. One link could
be highly useful like a q of 10, and the other two links could be of
little use, like a q of 0. The forward prop of those three links times
the probablity of each would predict the q value of the downstream node,
but you couldn't reverse it and get the right answer.

But what if we tracked node qValues? Could link qValues be predicted if we
knew the qVlaues of al the nodes? Nope, a simple exmaple makes it clear
the nodes might have all values of 9, but some links could be 10's and
others's 1. That would not be possible to calcuate from the node values
alone. Link q values have to be tracked separate from the node q values.

But link q values could be relative to the source node if it were useful
to do that and possible to calculate them that way?

If we just change the update formula for links couldn't we do
that? Caluate the q value of nodes the same we do for links now, but
the for links make it source_nodeQ+linkQ as the formula? So then link
q values become relative values? And as the average drifts, the source
node will drift preventing the link q values from having to drift? Also
allowing for finer resolution between link values? So if the link true
q values are like 10.001 vs 10.003 then the node might be 10.002 and
the links become -.001 vs +.001? With the noise level of the learning
updates being +- .00001 for the relative values maybe?

Ok, that would be even more memory and CPU, but maybe it would make
thing work better in an important way. Will keep it on the todo list.
Might be a better and cleaner solution than the adjusting I'm tryign to
do with the learning rates yesterday.

So, with the help of percepetion, parts in the middle of the net can
end up with high q values becaues they correlate with good stuff in the
enviroment and others low because they correlate with bad stuff. And
this is true for Nodes. But maybe not for links?

I can't stop typing two spaces. It's so fucking hardwired into my thumb's
behavior. I guess I just need to practice. By not typing to spaces. Darn I
keep doing it even as I write about not doing it. So tst. And again, new
idea. Start a new idea. Start again. This is not two spaces. So neither
is this, okd. So never type two spaces is that so hard mr thumb? Just
learn to keep typing one.

Ah, my halfLife=10 8x4 net 3 layers of links, converged on the correct
answer in some time less than1.5 hours. It's showing 2.770 RPS and
counting up slowly at the moment. Was it better than the 1 halfLife? I
have no Idea. I need reward over time graphs damn it! Maybe it's time
to break out the python graphics code and write some graphing tools?

Drawing graphs and numbers, looking at how q values work out for nodes
vs links. Yes, as I was thinking, back prop the link q values will tell
you the node q values. But you can't back prob beyond that. There's no
way to back prop nodes to the feed in links, or links to links or nodes
to nodes. It's only done by back proping rewards from the reward to all
the links and/or nodes like I'm doing. And only if activity is sparse
and dynamic can this do a good job of assigning rewards to actions. When
all actions are all busy at the same time the credit assignment broblem
becomes very dificult and would take a lot of data to separate signal
from noise.

But secondary reinforcement is important and it needs to be done. But
how? My accumulation of td errors is wrong. A -8 link can sort to a +9.8
link in my exmaple, and that could "seem" like a good thing creating
a postive TD error the back prop. But it's not, becuaes that sort is a
bad sort, and it's what makes the link 9.8 instead of 10. But that can
not be measured by the link qValues.

I am thinking a running average of qValues the pulses pass through
would be the measure of the systems estimate of the current state of
the enviornment. What I was doing before I changed to "real" calcuation
of rewards.

Ah, so when a real reward shows up, I use the next link (one of the
first links in the net) as the systems prediction of future qValues. And
the difference between all the current qVlues, and this discounted
future prediction, is the error I send back in time. The links are
using each other to prdict the future. But, the running mean would be
the prediction made by all the recent past links and their qvalues. So
instead of using the one node as the target, maybe that running mean,
should be the target? Instead of the next input node? And we then back
prop the difference betweeen what the running mean predicted, and what
the reward was?

Drat. This seems correct. But isn't this what we were doing and what I
gave up on as invalid in order to create the real q values I'm using now?

I'm using some time based discounting now on the current node to calculate
the error which is needed to make it correctl calculate rewards per
second I think. I don't think I was doing that before. So maybe the
only problem with before was a very subtle problem of not discounting
correctly in the formula?

So, all q values are disconted future estimates of rewards, that are
scaled to measure esimated total RPS. We average these estimates, using
the same halfLife curve to get the systems expected future rewards. When
we get a reward, we use this average as the systems estimate instead
of using the next link. Yeah, the average will be the correct seconary
reinformcnet signal to measure the nets belief about the future. But
we must calcuate not a REWARD! but a Rewards Per Second! So when we
get a reward, we must turn it into a rewards points per second since
the last reward and compare that to the current mean! Beucse q values
don't estimate reward values! They estimate RPS! If we don't do that the
system won't know the difference between a +1 reward 10 times a second
or a +1 reward once a year! I don't think my old code was doing that so
it was measuring value of next expected reard, not RPS!

Ok, so dman. Back to the old system, with improvements. And then every
time we process a reward, we ARE back proping secondary reinforment
learning! Ah, and sending 0 values rewards doens't change RPS
values! Wait, fuck, 0 has special meaning this this case and I thought
it didn't. has the switch to RPS changed everything? A reward of 2 once a
second, is now the same as a reward of 1, twice a second. A reward of 0,
is a non-reward becuase we are adding up reward points over time!

Shit, a reward of -1 has a negative meaning now! Is a rewards of 0 and
-1, the same as rewards of 10 and 11? As I've been thining it was? I
guess a reward of 10 10 times a second creates q values of 100. And a
rewward of 11 10 times a second creates q values of 110. and 0 and -1
create q of 0 and and -10. So they are creating a value of 10 difference
in q values. Yeah, I guess pretty much they are working as expected. But
the key is sending no rewards is identical to send a reward of 0. Except
when I send a zero it will force the back prop of current estimates and
when I don't secondary reinfoment is not taking place! And this will
work the same as it is now except the mean will be the systems running
prediction instead of the current input link. So if input links were good
predcitors that would be ok to use but when input links are not good
predictors it makes the systems ability to predict rewards very weak.
And we don't expect the input links to be good predictors. But if last
leavel perception links are the best predictors, and they get averaged
with all these other bad prediction links what will that create? If the
average rewards is zero for the system, and most links are zero because
they are bad predictors, but on link always correlates with a spceal +1
reward and another with a -1 reward, the q value of that link won't just
become +1 as it does not. It will become a value that is large enough to
push the mean, to +1! It could become +100! No, wait, it won't. Because
we compare the systems mean (0), with the reward 1, and make that the
taget the qvalues are intended to predict -- so +1 for that links! Yes
this is very different than what I was doing with the mean before. Shit,
no wait. If the q value is predicing rewaerds reative to mean, -- ah,
rewards relative to discounted mean. Ah, yes the mean is not really
a prediciton. The q values are the predictions. The mean is just the
mean of past predicitons. So the q value is a prediction of the reward
as calculated with back propigated real rewards and dicounsted. Ok,
let me look at the code we are using now.

Looked at code. Yes I'm using the first link q value as the target
but I discount that and convert units of rewards to inc units to match
the halfLife to create units per second. So a q value in a node that
estimates 10, will discount to the future of the next reward, to some
smaller value, like 8, and have the current reward added, say 2, to get
back to 10, if time and value was right to be 10 rewards per second --
Then adjust the link q value, by how far away it is from that estimate
of 10 -- zero in this case.

This all looks correct. So we just susbstitue qMean for the first node
links value we were using. And we don't need to buffer and wait to apply
rewards if we don't want to. But we can if we think multiple rewards will
show up before the next pulse. So secondary reinforment gets added into
to every rewards update, and rewards of 0 is how you force secondary
reinforment when there havne't been rewards!

I think this solves the secondary reinforment issue that has been hanging
open! The same learning speed ajustment added yesterday to balance learning
speed of links in the same fan out should still work just fine.

And it takes us back to what we were doing for the past few years using
mean! Except fixed now so that it discounts qValues as needed to make
them become rewards per second. Without discounting, and how we were
updating, they would have been trying to guess reward values, but not
total rewards. So it had no concept of learnign that more rewards was
better before. Or actually, something odd like how much the mean had to
be adjusted to make the mean estimate the correct reward value. Yeah
so if the system as a whole predicted .5, and the reward _value_ was
1.0 always correlated with a given link, that link's value would seek
the value needed to push the mean to 1.0.

In the old system, the q values were not compared to the estimate
so the errors probigated would make all values change so as to make the
mean measure the corrct mean estimated reward causing the q values
to varry widely and be a function of what other q values were being
used a the same time linking all their values together in an odd
dance. That odd linking should come back, the q values will now
just be estimated rps numbers.

Ok, been thinking more. I have no clue what I'm doing. :)

If I want qValues to be good estimates of future rewards independely,
then all I should need to do, is back proping rewards/second numbers.
So I get a reward, divide by the amount of time since the last reward to get rps,
and use that number as the target to compare to each q value to
create a training error. There is no need or point in picking a single
q value as the systems estimate of the future, and then comparing
the current reward to that. I wasn't even doing that correctly.
or was I? Wait. So the link q value was a future estimate. If I assume
it's correct, I then combine the current reward, with the future
estimate..... ah...

Wait, so a q of 10 means the discounted sum of all future rewards as
measured with inc values sums to 10. So if the link q is 10, that's a
disconted measure of all future inc values from that point forward in
time. And I have a reward, which is turned into an inc value -- but not
correctly discounted back to the point in time of the last qValue update.

So if my last estimate was 11 rps for the future, we went 1 second so
I should have decayed by 11*inc, and we got a 20 unit reward, and the
link I'm using for reference says 10, I need to discount that furture
10 as the correct part of my old discount and average it with what we
just actually saw, to be my target of what I should be.

So I'm using the last real reward, with the systems best estimate for
the future, to update my estimate.

Ok, so if the time discount is .8, then 20% of my number should represent
the real reward we just got. and 80% should be the systems estimate
of the future at that point. So I discount the future estimate by .8.
And I should discount the real rewards per second by .2, add the two,
And that should be my target.

d = .8 discount

target = (1-d) * reward/sec + d * q
target = r/s - d*r/s + d * q
target = r/s + dq - dr/s
target = r/s + d(q - r/s)

Any of those seem correct.

d is measured in q/sec? inc is how much a q of 1 discounts in a
second? (.5 for a 1 sec half life)

Ah, so 1-d is inc*seconds. So how much a unit q value will discount
in our time period. So if the reward was 1 and we miultiply by inc,
we have discounted ...

Yeah, so it looks like the current formula is about right. But I'm
not sure if it converges as fast as it might be bale to do if done
differently.

So it's using the first links as the systems estimate for the future to
update all other links.

Should we use mean instead? Would that be any better? I guess it's
crticial in terms of back propigating secondary reinforcement.

So if wMean is the systems current estimate for the future, then we need
to mix that with the 1-d discounted actual rewards (r(1-d)) to create
the target. Which is CLOSE to r*inc, but I suspect not the same. Right!
r*inc is the correct discount for ONE SECOND, but not for the actual time
needed! So now I'm thinking it's surprising this works at all! I guess
it doesn't! It's lower than it should be most the time becuase I end
up doing rewards for less than a second a lot!

I have to run to BGOP meeting tonight. And tomorrow it's bike riding
all day with Dave!

[7:57 PM] Left for BGOP. Onkly made it to Five Guys. Came home. :)

So, reward*inc makes the target correct in that it produces the correct
q value IF the q alue was correct to start with, and if the reward
is correct for that prediction, and if the future prediction matches.
If reward is too high, it will make q increase. And if reward is too
low it iwll make q decrease. And same for the future projection.

But, if q is say 10, but future prediciton is 5, and reward is 20 rps,
how these get added together I don't think is correct. I don't think
it correctly makes the q value seek what it should. discount * futureq
is correct. I think. But I don't think the current reward is calculated
correctly.

[8:23 PM] Well. I'll be. The activity trace can't actually measure pulses
per second correctly. 10 pulses per second becomes 9.69481928716 instead
of 10, and 1/100 pulses per second is 0.0670324698595 instead of .01.
It gets 1 pps correctly but the more we move away from that the more
error there is. This explains why these activity numbers were always a
little off -- it wasn't just display error or something.

Well, shit. The foundation of so much of this code is wrong. Or more
correctly my understanding of it was wrong.

Ok, so there is nothing wrong with the ActivityTrace. It's just not
measuring events per second or rewards per second. It would have needed to
be a 1/x decay or something like that to do that. So we can use Activity
trace to define a discount function to apply rewards to past events. But
the back prop equation is not what I thought it was because the discount
wasn't correctly turning time into pulses per second. Which only make
all my confusion above more confused! So discounted sum of future rewards
does not translate to RPS -- but something similar to it.

So rewards needs to be muiltiplied by inc to change the scale and units
so as to make the sum in the ball park of RPS, but not exactl RPS. SO inc
is not just some sort of magical discount or something, it's just a unit
conversion factor. And needed to keep the units in the balpark of RPS.

Ah, so I'm NOT actually calculating rewards per second! So dividing
rewards by time is NOT correct!

Ok, so the past q Value, is disconted ot the "current" time, where the
estimated SUM of future discounted rewards, is added to the current reward
(unit converted).

Ahm no, I must add the unit converted current reward to the estimate of
future discounted rewards, and then discount the SUM, and make that the
target of the past q value. so old_q = d(r*inc+future_q).

If old is 1.0, and discount is for one second, and future_q is estimating
a steady 1 reward per second, then the amount that old_q decays should be
inc. Right, so not d*r*inc, just (1-d)*r because inc is the same as 1-d*1
where d is the discount for 1 second! So r*inc + d*future_q is correct
if time is one second, but wrong for eveything else. So we don't need
to convert r to inc? Right! But we need to add r to estimated future
rewards, and THEN discount the sum to find the correct target! No use
of inc needed! It will just end up being the same as if we did r*inc +
d*future if the time is one second! And not tons off for times near
1 second.

OMG this has got me so confused. Yes, we still need to convert units
to inc before summing. So d*(r*inc + future_q). So this converts the
discounted sum of future rewards into a number that is in the ballpark
of being rewards per second. But inc is not calculated to be used like
this? It is the one second discount amount (1-t1).

But we were using inc as the 1 second disccount to adjust r by.

Ah, I see swomething that is confusing me! If you have one unit per
second, the activity trace jumps up, and then decays, and then jumps up
back to what it was. Inc is set so that the amount it jumps up to is 1.0,
and then fades down to .93 etc. But it could also be configured to make
it jump up to something higher, than fade backj down to 1.0. So it's a
question of where we want that sawtooth waveform to be relative to 1.0
when it's representing 1 unit a second! Or, if a unit of 1 repreesnts
a stream of 1 units once per second into infinity, where does the first
pulse start in time? Right now, or 1 second in the future?

Ok, so when doing the reward update, the q values, all reprsent a forward
looking estimate of discounted FUTURE rewards -- stuff that's discounted
from the future, back to NOW. So, using a given q value a the systems
estimate of NOW, and a reward I just received, I have to add the reward
to the estimate (converting to special units),m and then INFLATE it back
in time to the previous q value as it's target.

prevous_q_target = (r*inc + now_q) / discount

That sounds totally logical, but is totally wrong.

Right, rewards fade INTO THE PAST. so now_q is the estiamte of future
discounted rewards. I've been drawing the discounting backwards all
this time and that's what got me so amazingly confused even though I
was writting the formula correctly.

So past_q = d * (r*inc + now_q)

Oh, looking at the test I had running, it's converged on the right answer
after 2 1/2 hours even though the q values are wrong -- they are higher
than they should be ...

-------------------------------------------------------------------------------------------------
XNET Fri Feb 19 19:50:30 2016  RunTime: 2:25:14  Real-Time Simulation speed=1000X
p1 freq:  2.0 ====================
p2 freq:  1.0 ==========
Two Node Big Net 8 x 3  time: 942:26:15.0000  PPS  1988  RPS 4.648 4.499 
Explore: 1.00%  Q: 0.00010000  B: 0.00010000  W: 0.00000100  halfLife: 1.0  fanOut: 5
  .                              |  .  5.68  5.64  5.64  5.52  4.91| 23|    204 ================
  .                              |  .  5.46  5.46  5.75  5.46  5.47| 16|    144 ===========
  .                              | 17  5.67  5.91  5.67  5.67  5.67|  .|      1 =
 17  5.90  5.90  5.90  5.66  5.67|  .  5.89  5.67  5.67  5.67  5.66|  .| 
  .                              |  .  5.54  5.80  5.80  5.82  5.83|  .|      1 =
 10  5.66  5.89  5.89  5.90  5.90|  .  5.78  5.78  5.77  5.79  5.77|  .|      3 =
  .                              | 22  5.80  5.80  5.79  5.79  5.78|  .|     19 ==
 12  5.70  5.70  5.70  5.63  5.52|  0  5.79  5.79  5.78  5.79  5.52|  .|      3 =
qMean:          5.8264
-------------------------------------------------------------------------------------------------

So let me go fix the formula. But still use qMean as the estimated future
reward measure.

================================================================================

2016-02-20 8:57 AM Saturday

Yesterday

1) Looked at rewards, did lots of thinking about how they work so as
to add secondary reinforcement. Decided to go back to using qMean in
the update formula but using it as the future prediction instead of
using first link sorted. That adds secondary reinforcement to ever
update. reward(0) then becomes how you do secondary reinforcemetn when
there are no rewards to send.

2) Figured out that ActivityTrace() does not actually measure events
per second correctly. Works correctly if rate is 1 per second but
gets further ahd further away from correct as the rate gets away from
that. Not a problem, but good to know. I was thinking it did which has
caused me confusion. So AT is a discount function that defines how
rewards are applied to the past, which means it defines how qValues
predict future rewards.

3) Figured out that my back prop formula was not correct either. Target
was r*inc + d*predict_q. It should have been d(r*inc + predict_q). the
inc factor is just a unit converter, not a valid discount.

4) Speed up the second RPS measure to be able to see it converge faster
as debug output.

5) Lots of testing still ongoing to see how these new ideas work in real
life. So far, they seem to work, maybe a little better. But maybe only
about the same?

6) Seraching for why larger nets are not learning -- is it just a time
problem or somthing more serious? Working up. x4 level net was able
to learn but haven't had time to test lager yet. 11x10 range did not
yet learn.

7) changed inc. It was the value needed to make inc go to 1, and fade
below for 1 per second pattern. Now it makes the bottom of the wave 1
instead of the peek. But I'm about to change again to make the center of
the wave 0. Should inegrate the curve and calculate the weighted mean --
too lazy, the center should be close enough considering it's all just
an estimate and not real units per second measure.

Today

About to leave for bike ride up W&OD and down Fairfax County Parkway
with Dave. It's been cold, but will be up to 60 deg today. It's been
mostly in the 40's lately. Meeting Dave at 11:30 at bridge over parkway.
Will be able to work more tonight.

Adjusted inc to put it in the middle of the waveform inc = 2(1-d)/(1+d)

More ideas on how to use the net to predict future rewards. qMean is
one way. But we are training the q values to be the best predictors on
their own. When we average them together, we get a number that is close
to the average but not a great predictor of how things are changing. If
the average is 5, and a q value predicts 10, then the 10 is a far better
predictor than the average. It's an indication of something special
happening. Maybe this is just another point to be made about short half
lifes. Short half lifes will make the qMean be heavilly weighted with
recent qValues so we get a bunch of 10's it will make the net expectation
rise up. Ah, but again, input and output nodes are expected to have q
values near the mean and have very little correlation with anything so
they will just weigh down the average.

Ah, here's a simple idea. Give q values further away from the mean more
power to move the average faster. Average (q-qMean)^2! To calculate a
variance and then use the systems oh, no, we need to maintain sign not
erease it. Then I guess that forces us to (q-qMean)^3 then use the 3rd
root of that added to the mean to produce the systems current reward
estimate that gives the q values futher away from the mean more power
to move the mean! Could experiment with that later. Might make the net
unstable as large values self amplify themselves. Sounds good by some
measure but risky by others. A large q vaiance can move the mean quickly
with this, but likewise, the average values will have power to pull it
back down quickly. So if he numbers are 5 10 10 5, where will the mean
stabalize at with the ^3 system? Would not still be in the middle at 7.5?
What if it were 5 5 10 5 5 where would it be? How different would it be
from just the normal mean?

Oh, right, I was not thinking of moving the mean, with ^3. I was thinking
of maintaining the mean separate from the ^3 skew measure. Need to write
some test code that loop though values like that and see what results.

Time to get ready to bike.. [9:53 AM]

Oh, need to create reward graph system so I can see long running changes
in rewards to compare systems to see which do better, and to spot problems
with the system colapsing.

Just noticed the new code colapsig on a long run with a halfLife of 10
-- the larger the halfLife the more likely these instant reward tests
are to colapse like that and have to start over. If seems like there
should be some way to force the system to converge and not colaspe. Was
exploring the idea using relative q values as a though on how to do
things different to fix the colapse issue. Not sure if that would help.
But something must be posible. I think the issue is that the difference
between q values needed to maintain convergence falls below the noise
level of the learning so the system loses all track of what is better. It
becomes blind to the differences and starts doing stupid stuff, which
casues all hard won q values to be erased. Using relative measure within a
fan-out might reduce the learning noise below the needed resoluition. So
if the q us 5.0001 vs 5.0003, then the learnign noise as applied to the
5 could swamp the .0001 difference. But if the links had relative values
of -.0001 and +.0001 and learning noise was three decmal places below
that, then there would not be a problem. Maybe this just means we need
to adjust q values relative not to their absolute values but rellative
to their difference from the mean for a fan out? That sounds like an
idea that needs to be investigated! So noise from leanring drops as q
values get close together in value. Or beter yet, relative to the mean
of the two best q values? Don't want on really bad one from draging down
the mean. 5.0001 5.0002 vs -10 for exmaple..

Ok, getting late now gota run. [10:06 AM]

[6:07 PM] back home from the bike ride! 39.1 miles. Snow piles were
only a minor problem at times. Lots of mud from having to ride off the
trail to go around melting snow however! Temp got all the up to the high
60's. Very unsual for this time of year. Back to the 40's later this week.

Need to test larger nets and find out what the current algorithm can do.

The halfLife 1 nets seems to work and seem to be steady. I have not
noticed them colapse. But the halfLife 10 nets will converge, then colapse
after a few hours, and keep having to restart. Need to collect reward
rate data series and polt it so I can get a good look at what's happening.

[8:06 PM] got a simple data plot basically working to plot rewards
over time. The window controls don't work on their own but it's able
to update as the xnet runs so that's cool! Using matplotlib. Will
learn more and improve over time! Got a 3 layers of link BigNet test
running. It doesn't seem to find a path for P1. One run I saw it work.
But all others have not worked. I'll leave it running and see if it
finds a solution later. Time for some rest and TV.

================================================================================

2016-02-21 11:02 AM Sunday

Yesterday

1) Changed formula for inc again to try and place the 1 PPS in the middle
of the sawtooth waveform. Not important really since it's only a units
convervsion factor and has nothing to do with how the network operates.

2) Added graphing of rewards! Can now for the first time really see
clearly how the colapse problem works. The only network that doesn't
colapse is a single layer network. All others colapse. But at least the
2 layer network with a halfLife of 1 seems to have stopped colapsing
and stabalized over night.

3) Spent a good part of yesterday bike riding with Dave. Odd warm weather
with the temp topping out in the high 60's yesterday. Slept in late. The
bike ride really wore me out. It is good to feel tired like that and
get a good sleep.

Today

Need to explore how all the different learning paramaters effect network
behavior now that I have the graph to make it so very clear what is
happeing for the first time. Maybe b learning needs to be slower to
pevent colapse for example to give larger network smore time to
converge. Maybe I'll even need to automate testing of runs?

Results from runs.

All these use 3 pulse inputs with blanced pulse volume. p1 and p2
alternate between 1 and 2 hz in a binary count. p3 is the volume
balancing outputs. Net is rewarded +1 for sending p1 to out 2, punished
for sending it anywhere else. p2 and p3 are rewarded for not sending
to out 2 all other outputs are +1.

Using the TwoNodeBigNet test for these.

They all learn to not send p2 and p3 to the output fairly quickly
which gets rewards up to around 2 per second. But they struggle to
learn to send P1 to out 2 and that's what keeps colapsing when it
falls apart. The rewards approach 5 PPS when it's all working, and
when the p1 colapses it falls down twoards 2. But tends to bounce
around between 2 and 5 with a slow exploneital appraoch to 5, and
then a sudden and quick drop downards when it colapses until it
figures out how to sort p1 again, and heads upwards suddenly.

8x4 net with 3 layers of links. halfLife of 2.0 15 hours of real run
time, 3608 hours of simulated time. Constant sawtooth of learnfing
then forgetting, 50 ish times. It can't converge without colappsing.
Max rewards are around 4.5 with cloplase down to 2.5 range. deeper nets
and higher halfLife causes more colapse. q: 0.0001 B: 0.0001 W starts
high, goes down to 0.000001. halfLIfe 2.

Next:

8x3 net 2 layers. halfLife 1. Colaplsed a few times, but has run for most
the night without a colapse. RPS 4.78 -- seems to be the best possile
for this net. BTW, exploer is 1%

next:

8x3 net 2 layers halfLife 10. Constant colapse but not as fast or as
bad as the deeper 8x4. The adding of the extra layer seems to have more
effect on colapse than the higher halfLife.

next:

8x3 half life 2.0. Colapses a few times by has stablzied. Colpased a
bit more than the halflife 1 net but otherwise looks very similar to
halfLife 1.

----

[12:01 PM]

Watched the 8x4 network colapse. The first layer input nodes for p1 got so
close together in value, that one of the bad options was endup equal to
the good options, and the second the network started to send some pulses
to a bad second layer net, the PPS started to dive. The good nodes, being
used the most, started to dive faster, making them seem like bad nodes. At
least I think they started to dive faster. But suddenly, after the values
being highly stable, they all started to jump around our of control.

My new qMean based target for secondary reinforcement might be makeing
the learning process even more complex and chaostic here.

The problem with the longer half life, is that even bad paths, look every
good. And sending only one pulse to a bad path, only has a very tiny
small effect on the q value. So the system needs to be able to measure
and use that tiny difference to correctly understand that a path is good
or bad. And as it is now, a 4.6701 vs 4.6702 difference is beyond what
the network can spot without dropping the leanring rates down to very
small amounts. But the key is that the r3elative diffence of q values
in a fan out is what the system needs to focuse on, not the absolute
difference. Let me look at this idea of adjusting learning speed by q
value difference. And or, creating a q value for the feeding node to
act as the relative measure, and the q values of links being feednode +
link to get it's actual value. So we update the feednoe +link relative to
the size of each? Hey, that sounds correct and sort of straight forward!
So if the learning target update is .0001, then most the .0001 is applied
to the feed node q value and only a realtively tiny part is applied to
the relative link! So the relative values move very slowey, but the
common value can move very quickly! Let me check the code.

----

[2:06 PM]

Ok, I swtiched code to make link q values relative to the node q values.
Thought I was doing it the right way, but all I did, was make it act
exactl as it would, even if the values were not relative, but just use
relative coding. This does nothing. When coded like this. And what I
saw happening was that the node q value drifted upwards faster due to
higher total volume than link q values so most the link q values were
negative in value indicating the link q values were trying to catch up
to the node q value. This is not what I was going for. :) But there's an
answer to be found here. I also took out the speed of learning adjusts
per link to account for the long term bias factors of q and and w and
distanceBias. So I need to figure out how to do this to implment the
sort of idea I was going for instead of what I actually just coded.

----

Ok, here's a flaw in my thinking. I was thinking that the relative q
vales would be small and have less learning noise. But the learning
noise is cuases by the difference between q values and targets, but by
the size of the q value. Targets jump up and down because real rewards
jump up and own, like -1 +1 rewards. So the real error is caused by
real rewards. Like a bumch of +1, and a rare few -1. That averge to
+4.8. But the planw as still good here. The plan was to have the common
node absorb the bulk of the learning noise so the relative links didn't
have to. But as I've coded it, they all are aborbing the learnign noise
and I've solved nothing yet. Most important, the common node, at the
high volume, absorbs more than the others.

Oh, but I forgot. B values are now learning, using relative link
values. But I guess that makes no difference yet now that I think abot
it since b learning is all just based on which is the greatest.

If I made the common node absorb the average of the learning noise from
the links, would that even help? If I calculate links as if they were
full valued, then it does nothing to help the core problem of learning
noise causing individual nodes to move relative to each other.

Ok, wait. The +1 -1 shouldn't be a problem. That noise is absorned
the same by all of the links making them all move up and down in step
and not causing a good one to move ahead of a bad one. Well, except the
drift problem of different voumes in links making high volume links move
faster. But I had fixed that with the long term adjustment which I now
took out. But the long term adjustment addressed that.

Ok, there's a problem with my thinking here. I was thinking learning
noise was making bad links jump randomly ahead of good links. But now
that I think about this, I don't have a clue how that could happen.
Is there a problem that I was calculating q values incorrectly at some
small level and this when this calcuation error gets large enough that's
when the network colapses?

Ok, so no matter what the target is, all links move towards it. Ok and
if the use of the link swas the CAUSE of the reward, then the links will
all be trained correctly. A good link is piced at random, it leads to a
higher value reward, and all inks move up even with long half life. But
the good link activity will be higher so it will move slightly more. And
the amount of moiton by speed shouldn't be a question.

The problem must all come in when something ELSE causes the reward so a
bad link is selected, rewards are done correctly, learning is correct,
but them in another part of the net, something happens, and rewards
get better. But the randomly chosen bad action, gets updated more for
the good stuff some other links did. So, part of the net is doing good
stuff, to sort P2 and P3, and whatever we happen to be doing to P1, gets
rewarded, even if it's not good. Becuse the good stuff for p2 and p3 is
overpowering the bad stuff that is beong done for P1. Which means the net
first learns to do the goods tuff for P2 and P3 (because shared nodes that
do good for p2 and p3 are being shared by p2 and p3 so they are improving
together twice as fast). But once p2 and p3 learning starts to slow down,
then p1 learning should not be confused by that anymore and it whould
learn on it's own. Which is what I saw happening. The net would learn
the easy and identical p2 and p3 solution, and it would take longer then
to learn p1. But what then causes the later colapse?

I guess the problem must be that p2 and p3 behavior randomly jumps rewards
up and own based on random exploring and the like. So a few random bad
luck choices by the p2 p3 part of the net, cuses anverage rewards to
fall. And when p3 learning has just randomly picked good beahviors, the
bad mistake of p2 and p3 get applied to the good behaviors of p1. So, p2
and p3 learning noise, is confusing p1 learning. And that's the source
of the noise. And when that noise becomes larger than the difference
in p1 q values, that's when p1 loses track of what's good and and bad
and colapses.

So we can look at this as if p1 learning, has constant random rewards
applied that have mothing to do with that it's trying to learn. We could
do the same thing, just by adding random rewards to the system. And the
learning noise in moving q values for the random noise, causes q values
to move too fast, we see this colapse problem.

So learning speed, must slow down, as q values get closer and closer
together, so as to average out the unrelated learning noise. But this
is only relative to each fan-out not for the entire net? But if q values
need to switch places we can't just keep slowing them more and more and
prevent them from swtiching!

Ah, so maybe it's not an issue of the two best q values and how close they
are, but the relative value of all the q values in a fan-out. Beuase that
is what I saw in the colapse. All q values in the node were aproaching
the same value, like 4.65, becuase with the high half life the difference
between bad and good was in fact very very small.

So, just setting learning speed lower, will allow the network to learn
these finer detail issues. Both qlearning and b leanring. qlearning speed
controls how much of the false reward noise shows up in q values. and b
learning speed controls how much of that show up in b values and behavior.

Ok, so the problem is not learning noies for valid rewards. It's
learning noise for false rewards from other behaviors. It's the cross
polution of learning rewards from one part of the network, to the other.
And that's just a normal part of the problem the system must deal with,
not something that can be eliminated. Ok, but how in general, with a stream
of reward data, does the system know what is false noise and what is valid
noise? Because it's trying to correlate rewards, with random varaitions
in link selection. So it's the link activity, that is correlated with
reward data. And how much q values are allowed to change, must be related
to how much link activity is changing I guess? Ah, wait, what's key
is not link volume for the fan out. Because all links are having the
reward training data biased by that equally. But the key, is how much
the links vary from each other in activity level. Or how much links
vary from themselves from update to update. Ok, so total volume to
the fanout is valid for assigning value to the fan-out. high volume
means the rewards are more relevent to us, low volume me they are less
relevent. But how that update is applied from link to link,
should be bias by how the links changed activity level relative to
each other. So activity might have been 4 4 8 last time, now it's 4 3 9.

We want the node common value to drift with it's activity level it
seems to me But the relative values need to be more subtle I think.

----

Maybe one answer is just to use slower learning speed for the relative
value than the common node value?

----

[4:54 PM]

I added back the bias of b w and distanceBias -- the link values are now
staying much closer together and much closer to the feed node value so
that there are more + and - values in the link relative q values.

For making sort decisions, the system only needs good relative q values
for the links in a given fan-out. So the problem of learning good relative
q values is the most important for not colapsing. But for doing secondary
reinforcement, real q values are needed. It feels as if calculating
feed-in node q values as the real q values, and the links as relative to
that still has important potential in solving all this colapse problem
without just setting really small q and b learning speed values.

Slowing down q learning by a factor of 10, caused the system to have a
very wobbly learning line with inconsistent growth even though it seemed
to be heading in the right direction. I killed it and reduced b to the
same value by dividing it by 10 to see if this might be a problem of
q being slow and b was faster. Maybe it's just a problem of slowing
it down to the oint I can see in the graph the noise that was hidden
before? I'll have to let it run longer to see what happens.

================================================================================

2016-02-22 9:24 AM Monday

Yesterday

1) Make link values relative to node. Idea was to have link values
update slowly and node to absorb most the learning noise. But the code
ended up making no difference because I coded it as if they were updated
separately!

2) Added back in the relative speed adjustments using b and w and
distanceBias. But just now realised I didn't cap the speed up when b
gets near zero to match the policy!

3) Improved how graph interaction works so it stays interactive by
more frequent calls. And made it rescale constantly. It would stop auto
rescale on growth once you manually changed it. Now it autorescales but
that means you can't pan and zoom to look at data becuase it rescales
every few seconds.

4) Multiple long runs with different learning values on 8x4 network to
see if slower learning makes it more stable. for this net with hl 1,
.0001 learning had constant colapses. .00001 learning converged quickly,
but seems very ustable with lots of noise in the reward signal and a few
small dips. But that could be caused by my error in 2) above I'll need
to test. .000001 (5 zeros) learning works, but is as not so surprising
far slower to converge but seems to be staying converged.

5) Oddly, the slow learning doesn't seem to have close together q
values. The faste learning colapse when the q values get too close
together, but the slow learning doesn't solve it with more precision, but
with totally different q values it seems. I think. More testing needed.

6) also the tests used the code that started w high, and slowly made it
decline to .000001. That made the slow learning act very different while
w was higher. Two tests I ran without w declining and they were very
similar in curve shape but different from the one with the declining w.

Today

Lots of testing needed to find out what chagnes. But the testing takes
hours for every run just to converge and hours more to see if it starts to
colapse. Very slow process. Lots of waiting for tests with nothing to do.

Need to understand the colapse better and see if I can make the code
work other than just slowing learning down to a snals pace.

But first, fix the relative speed adjustment that made learning far too
fast when b = 0.

----

[10:27 AM]

Went out for Starbucks and Ingress. Only 6 days left to Sojourner 360 day
onyx badge! I can finally stop worring about remembering to play every
day starting Sunday! Well, Friday after that really because I want to
take it to a full 365 days.

Fixed the use of speed equalizing without capping maximum speedup by
adding .01 to the factor capping it at 100 speed up. Running the same
3x4 test from above with fast learning of .0001 for all three speed
variables. This is what was colapsing badly before. Now it's looking
amazingly smooth and perfect. It's converged on the 4.5 range best
answer as the other slow learning tests and it did it in only 250 sim
hours vs 2000 for the slow learning. And the slow learning is full of
reward noise that isn't in this one. Clearly my testing yesterday was
all wasted due to not correcting for this.

Now that hl 1 at fast learning is smooth and working nicely I need
to try hl 10 and see how the q values converge.

----

[12:14 PM]

Started a few runs to test.  two with 10 hf and two with 100 hf. With
the same .000 level learning that works for 1 hl, both the 10 and
100 level show some learning but lots of colaspe problems preventing
the learning from getting very far.  Relative q values all hang around
0 on the system that is colapsing.

Two have learning rates of .0000 followed by 1. 4x0 learning. e-5 learning
speeds.  They are struggling to get over 2.0 so they are just starting
to show signs of learning the p1 sort. Don't know if more time will
allow them to learn to use p1 and have the graph take off or if they
are just stuck. Time will tell.

it or if the system is 

----

[2:28 PM]

Been adding network parameter display to graph and fine tuning how
it rescales.

The hl 10 and hl 100 tests are either colapsing or just not learning.

But I noticed I had learning at 4 zeros but w learning at 3 zeros by
accident! That could prevent it from learning maybe. Testing with
w learning set the same as q and b, and will test with slower w
larning than the other two to see how it all effects things.

----

[5:06 PM]

Watching very slow tests run.

Doing lots of hl 10 tests with 8x4 nets with different learning
paramaters.

q:e-4 b:e-5 w:e-6 aka "456" was working very well, until it almost
converged, then colapsed all the way down to almost the 2.0 leve and is
now coverting again.

345 test (10x faster than the good one for all speeds), started cleanly
and nicely and faster but them colapsed before converging very far.

A 556 test had a dip riught at the beginning before hitting the 2.0
range plateu, then took off at 300 hour mark, an dstarted to slide a
few times but recovered and kept going nice. And oddly, the slide back
was not a sudden sharp drop but a rounded sin wave sort of affair. It's
now converged at the max around 4.6 and seems stable for the moment.

I have this idea that w learning needs to be slower tham b learning so
b learning can make some gains ahead of w learning cancel out what b
learning is trying to learn. Need more testing to verify.

A 555 test converged to 4.71 (high) but continues to colapse and get
worse.

On these hl 10 test that have converged, the heavy used paths have all
converged onto +-0.00 relative q values.  So clearly the learning noise
has to be kept small to keep them from colapsing. I don't dispaly high
enough resolution to know what the real differences are.

Started a hl1 8x5 test to see if the system can learn to converge on that
with 345 params (figure the hl1 might work with the faster 345 setting).

Current network is not only doing relative link values, but is using
both link and NODE values in the qMean calcultion.  Those extra node
q values will make the qMean more stable. That might likely reduce
learning noise and help the system converge with faster learning rates.

So don't know how slow the learning has to be to converge for the 8x4
hl 10 test.

The 8x4 hl 1 test, converged quickly and is dead stable with 444 learning
paramaters.  This was the first test I ran after putting back the cap
on rate equalizing based b w and distanceBias that made eveything work
better again.

I still don't have a clue how to take advantage of using the relative
link q values.  Though it's coded that way, they will work exactly as if
they were not relative.  I could try just making link learning be 10x
or 100x slower than node learning and see what happens. It might allow
the system to converge faster by using faster learning for the nodes,
and the slower learning for the links.  But how I have it coded right
now, they don't operate together -- they are independent, which means
the nodes with higher volume will move much faster than the links if
the link learning is slower.

----

[7:33 PM]

Thought. Does it matter what the speed of learning is from node to
node? That is, from fan-out to fan-out?  Should the speed of learning
be auto-adjusted for each fan-out relative to how close together the
q values are? It seems to me this is possible and likely a best case
solution.  Keep links as relative q values and adjust speed based on
learning noise rates and q value spacing.  But keep the node q values
with their own general learning shared across all of them to create
the qMean and secondary reinforment learning? It might require a lot
of extra overhead and work to do the tracking, but if a simple way to
track it could figured out that would be cool.

Maybe reducing q, b, and w all down to one value that is adjusted? I've
had this idea before but never made it work. Maybe in the future I can
figure that out but lets just ignore that idea for now. And move on to
making q learning work better.

I'm thinking links may not need to be real q values at all.  That node
qValues might be all that's needed.  In a large network, there will be
lots of fine resolution at the node level, bringing into question the
need for the more accurate link level q values.  So maybe we maintin the
b values but not the q values?  Can we know which way to adjust b values
directly from reward targets reltiave to the node q value and not track
the actual q values of each link?

So, reward target relative to our node qValue defines if we move links
up or down.  Current link traffic relative to link policy defines which
moves the most.  Node volume and node value reltiave to target defines
how much error is applied to our fan-out.

We could track a node change variance to get a magnatude of the error
updates.  How much is applied to links could be determined by the ratio
of link spread to qd varriance so average upates are like 1/100th of
the spread or something.  So fan-outs with a small spread will change
very slowerly and fan-outs with a large spread will change quicker.

Or, can we keep all the fan-outs normalized in a way so then the same
error update speed can be applied to eveything?  If they are not real
q Values but rather just normalized spread of some type that might be
possible.  Humn, if they are normalized in some way could we then map
the spread values to sort ratios in some automatic way without tracking
the b values?

Well, maybe we should ignore all that at first, and just go ahead and
use real q values in the links like we have been, but adjust learning
speed relative to the spead -- and realtive to the varriance of the error
difference we have to apply.  Sounds like a direction to head off in.

I feel this is all a job for tomorrow.  Tonight, I feel TV in the works.
Pig and I just got back from Outback.  I'm tired.  A sore throat is
developing for some reason.  And I've got 7 test runs on my 4 processor
box I need to let keep running over night to firmly establish trends.

If I can figure out how to to do it as real qValues, then mayube I can see
a path to combine q values and b values into one simplified update rule?

On the idea of not using link q values, maybe I should try not using
link qValues at all for qMean calculation as a start?

I wonder how much cloud computing resources cost?

----

[10:13 PM]

Set up a free Amazon AWS account started up a ubuntu image copied xnet.py
and got it running with a remote X connection back to my server over
ssh!

The CPU is faster than mine, running more like 3000 cps when mine runs
2000 on the same code.  But grapical access back though SSH is very slow.

Had to do an apt-get to install numpy and matplotlib but that was easy.

750 hours of server time a month for free for the first year.  Wait
that's one server full time forever for a year!  Need to verity fhat.

It acts like a single CPU so I can only run one job at a time without
slowing down the system.  Don't know if I can afford it after it's free.
Have to look at pricing. But they do have lots of high power large CPU
options to pick from if I ever need to fire up a 1000 cpu big net one day!
Cool stuff.

----

[12:56 AM]

Running this on AWS

-------------------------------------------------------------------------------------------------
XNET Tue Feb 23 03:19:13 2016  RunTime: 2:36:19  Real-Time Simulation speed=2000X
p1 freq:  1.0 ==========
p2 freq:  2.0 ====================
Two Node Big Net 8 x 6  time: 234:30:00.0000  PPS  158  RPS 3.978 3.331 
Explore: 1.00%  Q: 0.00010000  B: 0.00010000  W: 0.00001000  halfLife: 0.5  fanOut: 5
  .   .   .   .   .   .       4 =
  .   .  24   .   .  23     186 ==============
  .  24   .  24  24   .       2 =
 24   .   .   .   .   .       2 =
  .   .   .   .   2   1       4 =
 16   .  28  28   .  28     290 ======================
  .   .   .   .  26   1      11 =
 12  28   0   .   .   .       1 =
qMean:          4.7965  2.5429
p1
 0      4 =
 1    171 ===============
 2      1 =
 3      2 =
 4      1 =
 5     16 ==
 6      5 =
 7      .
p2
 0      .
 1      8 =
 2      1 =
 3      .
 4      2 =
 5     89 ========
 6      .
 7      .
p3
 0      .
 1      7 =
 2      .
 3      .
 4      1 =
 5    185 ================
 6      6 =
 7      1 =
-------------------------------------------------------------------------------------------------

It got past 2.0 RPS so it was trying to converge but as of this snapshot, it was running
very slow for some reason.  Maybe memory too large for machine?  But wanted to save it
to that an 8x6 net had converged to an answer! --- NOPE that was the speed it ran at
start -- just slow because it was a big network!

================================================================================

2016-02-23 10:41 AM Tuesday

Yesterday

1) Added back the use of policy in equalizing the learning speed across
links in the same fan-out to adjust for expected volume differences.
This was lost when I wrote the link-relative code.  That made learning
work much better again.  The hl 1 networks were learning and converging
nicely after this fix.  And not colapsing.

2) Started a lot of testing with hl 10 networks (known to be hard due
to the small difference in q values created after it converges to the
good policy cuases by only one small sorting mistake to the bad side).
Some worked much better than others, but not a single one converged
and stay converged without any colapses.  Slower learning works better,
but none that I tried solved the 8x4 network with hl 10 totally correctly.

3) Graphing improvements. Lots more little changes and improvement.
Addded all the network paramaters into the title so they become better
at self documenting.  Improved interactvity.

4) Amazon AWS account set up.  It gives a free year of access to for
one server instance.  Set up a server and got python libs loaded and
was even able to run xnet with X server connection back to the local
box though ssh!  Very cool.  The cpu is faster than my sinner box by
a little bit but that's for a single CPU.  That access is about $10 a
month for full time access but you can use it as you want on demand
and start up as many servers as you need.  MIght be useful in time.
Could set up a big remote distributed network at some point.

5) Ideas for improving network by using the links separate from the nodes.
Maybe adjust speed by difference in link values?

Today

Feeling seek. Comming down with a cold.  Not sure how much I can work
today.

Ideas from last night in bed!  Should use the nodes to calcuate qMean
and not the links.  That might make qmean more stable and help reduce
learning noise a bit.  And shouldn't be an issue in the long term becuase
even if one links if 10 and the other is -10, we don't need to know that
becuase the 10 link will be the one used mostly causing the feed node to
take on the 10 value as well.  So the feed node in time will represent
the mean of the links. Or at least close to it. So just usig the feed
nodes for the qMean should work fine.

Then for learning updates the target difference between the feed node and
the target, becomes the link learning target.  Then the difference between
that and the link values is the link training signal.  I'm actually
doing that already.  But didn't fully grasp the implications.

Then the idea here is to use fairly fast learning with the node values --
so the network can drift to new policy levels fairly quickly, but using
slower learning with the links to allow for finer resolution learning.
And that slower link learning doesn't slow down how fast the nodes and
mean can drift.  Even the idea of doupling the learning seems like it
might be resonable. So e-4 learning for the node, and e-8 learning for the
links for exmaple. We don't have to worry about how fast the link values
drift to represen the new q values and have that slow down how long it
takes the whole net to converge on a new policy level, beucae the node q
values will drift very quickly to take care of that, and links will only
be very slow relative measures.  And for b leanring all we care about
is which link value is larger we don't care about their absolute value.
So having them move slowy to find the data burried deep in the learning
noise shouldn't be an issue.  But it can effecthow long an old dog takes
to learn a new trick when thigns change.

So I want to try coding that and see what it does.  But first a run to
Starbucks and Ingress while I'm still feeling strong enough.

[11:03 AM]

----

[12:01 AM] Back from Starbucks and Ingress.  Updated my Facebook profile
to show the end of newsreader and the beginning of full time AI work!
Skinner needed a reboot.

Ready to start coding these split node/link ideas.

----

[12:56 PM]

Added display of node q value to net.show() and added reverse video
display of max q.  Saw something I failed to grasp happening before I
displayed this.  node+link quickly grows to average q values of around 2.
But they both grow at the same rate, so the node grows to around 1 and
the links grow to around 1. Then they stop growing very fast at all
since qMean is the correct average links stop growing.  But the node
keeps growing slowly as the links then slowly shrink in value.  This is
the problem of double counting the node alone, and also mixed in with
the links.  The links dominate the qMean.  Wait, no.  The learning will
Still be for qMean +- 1, which is 2+-1 which is 1 or 3.

AH, no, failed to understand correctly.  qValue is set to 1.0 at start
in the BigNet test.  That's why they started at 1.0.  Ok, ignore that
Lets code what I was thinking of.

----

[4:42 PM]

Changed code, and have been experimenting.  Changed policy to include
forceExploation value so it doesn't have to be added in for the updateQ.

Made qLearningSpeed control node learning.  Only nodes are used for
calculating qMean.  Link learning I'm expermenting with .1 less than node,
and .01 to see how it does.

This approach works great on the easy hl 1 test.  But I'm just now
starting to test with the hl 10 test where the q values get so much
closer together to see how it works.  It's starting to converge quickly,
but it's colapsing and struggleing, which might just be normal vs
a problem.  The test is whether this can converge and hold steady on
the right answers. This will just take more testing time to figure out
but I'm liking the feel of how it's working so far.

Need to add command line comments to graph still!

----

[6:58 PM]

It nailed the hl10 test as well with 8x4 net, and 334 with .02 less for
links (aka e-5). At e-4 for links, it found the answer for p1 getting
above 2.0 but it couldn't hold it and just cycled without learning
and colaplsing.

And it did it all in minutes becuase this allowed the links learning to
be slow, since link q values start at zero, and converge to near zero,
there was no long drawn out waiting for link q values to run all the
way up to 4.0 etc, for for some to chagne fater than other losing the
learning on the way there.

With fast q learning on nodes, that keeps the slow q learning on links
working far better.  It's just a matter of making sure the link q learning
is slow enough to capture the needed differences for the problem I think.

Lets add command line comments to the graphs first.  Kill the p1-p3
histogram and replace with a simple right/wrong counts.  And add printouts
for say first level q values so I can see in detail what the values are.

================================================================================

2016-02-24 1:00 PM Wednesday

Yesterday

1) Added forceExploration factor into link.policy instead of hard coding
it as +.01 in reward processing. But yet it's still not totally correct
because it had to be added in after *A since A can be zero so there
remains a slight difference between what the expected volumes really
are and what policy defines.  This could be a problem at times.  Maybe.
Or maybe I'm wrong and it is correct?

2) update net.show() to include node q value, as well as links. Added
arguments to control display of those.  Added reverse video display of
largest qValue.

3) Graph display added command line arguments.

4) Changed qMean to only use node q values and not link q values.

5) Changed link q value learning to be straight forward learning against
the target defined as target-nodeQ.  Then made link q learning slower
than node learning.  So qLearningSpeed defines node learning speeed, but
qLearnkingSpeed * 0.1, or 0.01,m or qLearningpSpeed ^2 were all tried as
the link learning speed.  All worked reall well becuase it allowed the
network to converge on answers far far far faster.  So we can use fast
q learning speed for the nodes, so they change very quickly as the net
learns and reward chagne. But very slow learning for the link values
that control actual policy that allows the system to learn very small
differences in link values as needed to converge on hard problems.

Today

I'm sick.  Slow to get going.  Took NyQuil and I'm in a daze.  Just a
cold but the drugs really slowed me down. And it's a dull and dreary
rainy day outside which isn't motivating me.

The changes to use different speeds for node vs link learning and only
use nodes for qMean (secondary reinforcement feedback) seems to be a
great step in the the right direction.  Nets that were not learning the
day before learn quickly now.

But, I left a hl 100 net 8x3 (two layer) running yesterday to see it it
could learn.  It failed to learn, more than the 2.0, but worse yet, after
many hours of steady running at 2.0 level, it colapsed way down to -3.8!
It is showing the key nodes and the q values seem to be changing too fast.
They are like .000005 range and the link learning is q^2 which is e-3
or e-6 for links.  I think for a hl of 100 the link learning needs to
be evey slower than that.

But, a big net, of 11x10 334 haveLife 2 net, is now converging after
running for almost 15 hours! It's looking smooth so far, but will have
to wait to see how it stabalizes and if it colapses.  But with a hl 2
odds are good it will remain stable.

So it's cool that these changes have moved up to larger nets.  But not
so cool that it still takes 14 hours for a 11x10 net to converge.

Also not cool that the hl 100 net provied so totally unstable after
hours of running. If learning is too hard, the idea is that the net
should remain stable, but just be "blind" to the harder problem and
never learn it.  Hard learnign shouldn't make the net unstable.

I'm thinking that the net as it works with the q b and w learning tends to
lock itself into answers and flip VERY SLOWLY AT TIMEs, from one answer
to another. Creating these flip points that cause the net to suddenly
shift and colapse and the like.  What I would like to see instead,
is slow convergence on the good answers, and never having it colapse.

I'm thinking the entire way I use the b value to accumulate evidence might
be at fault. And I don't like how w and b fight each other.  I would like
to see a system where the w percpetion was the default foundation of the
sytem, and that q learning would push it away from that foundation slowly.
The fact that w learning is trying to undo the q learning is an issue
that needs addressing.  I really like the b learning, and was very exited
when I relized how simple it would be to create the b values But now I
question if it was fixing other problems that I have now fixed with the
separate node and link q values.  The simpler the system the better and
more stable it's likely to be.

I'm thinking the great difference in flow from 0 to 100% or 1% to 99% with
forceExploration is an issue.  I'm thinking maybe the net flows should
be forced to be far more noisy?  Like 20 80 even maybe?  Or something.

Though this network is learning to send p1 to out 2, and not send p2
and p3 to out 2, it ends to send all p2/p3 pulses to the same place.
And though that place may chagne from time to time to blance in the long
term, it never goes back to the more random behavior seen at the start
where it's using all outputs equally. It should go back to that once it
figures out that all those beahviors are equal but it doesn't feel
like this net will do that very well.  Without b lerrning, there is no
sort policy, so I don't know how to make it work without b learning.  Need
to think about that.  But the idea would be to just sort ot the highest
qValue very time. Or something simple like that?

What if we bias output flow based on highest qValue but not bias it
to 100 0 0? But rather, say 80 20 20 or something?  Then w learning
can apply to that to pick the actual path each time?

W learning seems ok as it is. Assuming this even works well for percpeiton
which still has never been confirmed! It calcuates a w value based on down
stream node activity level.  The question the is how q learning overides
that bias.

On the idea of too many parameters...  Maybe qLearningSpeed should define
the link learning speed, and the node learning speed should just be
the sqrt(qLarningSpeed)?  So nodes learn the first N digits of perception
and links learn the second N digits?  That sounds like a possible
approach. So then we are back down to one qLearning speed to set based
on the nature of the problem, and things like half life. It's a trade
off of precision and speed of learning.  The net is blind to all things
below the speed of learning noise level.  This is such a good idea I'm
going to code that now.

----

[3:27 PM]

Added sqrt(qLearningSpeed) to create qNodeLearningSpeed and qLearningSpeed
is used for links now.

Watching activity on different nets as they learn.

Is there a need for b learning?  Now, when one q value has been in
the lead long enough, be will become 0 0 100 in favor of that q value.
As other q values jump ahead, the values will slowly start to shift. But
the b leanring delays how long it will take for activity to shift when
one q value jumps ahead of another.  Is that delay good?  Is it correct?
The q values change slowly alreday due to slow learning updates so they
represent accumuilated knowledge.  Do we need even more accumulated
knowledge in the form of the b learning?

It's this whole question of what is the correct way to map q values
to behavior. We certainly need slowly shifting beahvior at one level,
to get the net time to learn if the chagnes really are good or not.

Could it be an issue of levels as well?  That the first columns of the
network need to change slower thean the rest maybe? So if the first
nodes change their mind, it doesn't suddenly cause the others to shift?
Or to make it more stable, and prevent osscilations, the first may need
to change the slowest or something?  Or is it just the opposit with the
last changing the slowest and the first changing the fastest?

In the sort of test I'm doing now, where the system is learning which
output to sort pulses to, I can end up thinking about the problem
differently, about speed of elarning exct, vs how it might end up with
the hard problems in the future, where the issue is creating the right
behaviors from what is learned in the middle of thet net?  With more
percption at work in the first layers and more RL at work in the later?

So maybe the balance of power between w and b learning needs to shift
from column to column aka layer to layer?  Or w is the same in all
layeers but b's power over w increases as it gets deeper and deeper?

Is there a problem with using low b values and forcing data to flood out
if what we actually need is to sort all p1 to output 2 like in this test?
How can we do that if all p2 get mixed up with other signals?  We can't.
But maybe that's part of the problem.  The specific test I'm doing here
(all p1 to a given output) is beyond what we SHOULD use the network to do?
What we should do it make p1 bve the major control, of what output 2 is
doing and not worry about what pulses are actually used?  That seems
more realistic.  If p1 is cycling up and down (as it does in the test
I'm using), the network needs to make output 2 cycle up and down in the
same way, but not worry about what pulses actually get sent there.

Then, since we balance volume input, and send all other output to all
other paths, all other paths should learn to be the negative of p1,
but becuase it's blanced across all other 10 outputs, the effect will be
minimal making all othe routputs look mostly steady (random and chaotic
as well, but steady in general).

But how do I reward the net for doing this if I don't reward actual
pulse sorting?

And if I reward actual pulse sorting, is it wrong that the network can
learn to that that?

Watching b values drift, with this large network using e-3 b learning
it sure seems to me they are drifting very fast once one q value gets
ahead of another. It's not feeling like the use of b learning is really
locking the network into a solution. It's feeling like it's simply making
the transition from one state to another smoother.

What's locking the network into solutions is the actual q values. Or
the fact that b is allowed to range from 0 to 100.

BTW, I've still not really tested the effects of fanOut, distanceBias
and topology on all this. These things are likely more important as we
move to larger networks.  That's still on the todo list in time.

I increased explore to 10% to see what that does for this large 11x10
network.  It spreads activity though the entire net more which is not
surprising.

A thought.  Maybe I'm putting to much thought into startup issues verses
long term learning issues?  If it takes a long time to stablize maybe
that's ok?  Because the long term is a question of how it responds after
it has stabalized vs how long it takes to get there?  Maybe?

On this big net test, the P1 pulse, gets strongly programmed to send
down two rows and merge in with the p2 and p3 pulses.  But that's bad
becuase we don't want p1 to go the same place as p2.

Why is it learning this?  Why is P1 so strongly biased to follow the
wrong path?

BTW, I'm giving up on not putting two spaces after periods.  This is,
after all, a fixed with text file.  I have been fighting it for days
and trying to remember not to do it but every time I stop thinking
about it my hands go back to their old habbits of adding two spaces.
So I'm not going to bother anymore.

Also, I just realizked the :.,!fmt I like to use can be repeated with the
. fuction from different locations.  So once I do it, I can do it again
from the next higher paragraph to see where it's not needed anymore.
Fun learning short cuts you didn't know where there.

So, back to the learning problem.  I can understand why, due to the *a
policy that P1 gerts sucked into the same path with P2 and P3, and that
since P2 and P3 are learning to improve rewards by NOT going to out1,
P1 would get falsely rewarded and distroted into following along. But
in short order, P1 and P2 have learned all they can learn, and then P1
should not have a bias to follow alone with P2 and P3. All paths should be
at minimu, equally good as the P2 path.  So P1 should go back to random
sorting.  And once it goes back to random, A better path that takes it
to out 1 should emerge as better than following P2.  How is this working?
How is this happening?  It's like it's being punished for going to out 1,
because p2 is being punished for doing that?

Oh, wait.  I see what might be at work here.  When P1 forges a luck path
to the correct output, it digs a path for p2 and P3 pulses to follow!
And the following of p2 and p3 is what punishes p1 for creating that
path! P1 is being punished for leading p2 and p3 astray!  So not only
does P1 have to find a path, it has to find a path issolated enough from
the flow of p2 and p3 pulses, that it doesn't suck the other pulses into
the game!  And since p2 and p3 outnumber p1 in volume, anything p1 does,
can get double punished by p2 and p3.

And I'm using a half life of 2 seconds, which means in 2 seconds,
10 pulses from all three inputs flow!  Trying to teach it a behavior,
that only lasts 1/5 of a second!  So for p1 to learn this, it has to
get very lucky and get multiple versions of itself to the right output,
without sucking p2 and p3 along for the ride!

No wonder the large net took so long to lock in (15 hours).

Ok, so let me try very short half life and see how it works.

----

And with the net flooded with twice as many p2 and p3 pulses, all the
nodes are learning the same thing -- it's better to NOT send to out
1. So flooding the net with p2 and p3 data is not in fact good for
this problem becuase it prevents nodes from learning to anything other
than send all data NOT to out 1.  So laving all p2 and p3 find the same
path to one output, is better than allowing them to all flood the net,
because it frees up the rest of the net for other work.  So even though
sending to any output is equally good, it's better even in that case,
to not take advantage of that and send them all the only one output!
So what I thought of as bad behavior, might in fact be required behavior?

And sure enough, P1 is being trained to mix in to the "good" p2 stream
at a 2 to 1 ratio.

So having pulses pile up like that, and then SLOWLY drift to a new path
in time might be good and not bad.

tried cutting halfLife from 2 to 1. No big difference.  At least
for the first few minutes.  And I've got expore set very high at
50% to force the net to be flooded -- but that only makes it that
much harder for p1 to find a path to the correct output without
pulling lots of p2 and p3 pulses along for the ride! So let me cut
that back down.

----

[7:01 PM]

Big storm can though for an hour or so.  Power flickering.  Lots of rain.
tapering off now.

Been looking at the big net and qMean valuess.  When I set half life
to very small values, like .1 or .01, qMeans goes crazy.  Shooting up
to values like 20, and bouncing all over the place.

qMean uses the same discount as the nodes for the halfLife and a short
haflLife means the qMean value has very little ability to see into
the past -- it's value is mostly a function of the last pulse sorted.
And when hl is small, the inc value gets large.  P1 pulses almost
always produce a reward of -1 and the others produce one of +1 most
the time beucase the net has learned to sort the others but not to sort
P1 correctly.  So the qMean is mostly showing what last happaned.

I don't know if the use of qMean as a future estimated reward indicator
work in this case.  Or if q values in general make much sense relative
to each other because there is no relative measure to match with with
a node.  Or, the q value should be it's own relative measure?  I feel
like there's something important I'm failing to grasp here.

This all started when I was surprised by how different the RPS numbers
were from the qMean numbers.  The RPS numbers are like 1.8, which is a
resonable longer term averge when the net has not yet learned to sort
P1 pulses, but the qMean can shoot up to odd numbers like 20 with a
very short half life.  But the RPS is using the same half life. The
problem I guess is that the node q values are using different and odd
other nodes to back prop estimations.  And becuaes the squence of nodes
used is very mechanical due the frequency of my three input pulses, it
can probably get locked in a pattern of which node is using which other
nodes for feedback and causing odd results. I guess since it's not a long
term average.  But the qMean, once the pulse comes out of the network,
and at the same time stamp, creates a reward, should have been an average
o ------ wait....

----

[7:38 PM]

Wow.  Again I find out my code is not doing what I thought it was doing.

The ActivityAvg() does not actually factor in halfLife as I though
it was.  I thought with a half of .1 half the average would be based
on the numbers from the last .1 second.  It doesn't work like that at
all becuase of the way ActivityTrace() adjusts it's value to measure
in something close to rewards (or pulses) per second no matter what the
halfLife is set to.  So the ActivityAvg() is using this adjusted decay
and ending up measuring in decay seconds or something -- not even sure
how it works out to be honest.

But the bottom line is that the average has almost nothing to do with
the halfLife value. Half lifes of 1000 to .01 produce almost the same
effect with ActivityAvg.  The fact that it's different at all is probably
just a measure of how adjusting inc doesn't really make it measure
true Units per second.

qMean is calcuated using ActivityAvg.  RPS is calcuated using ActivityTrace.
Even though they share the same halfLife, the effect really has nothing
to do with each other.

I don't know if this means I need to fix ActivityAvg to actual base it's
value on the decay. or what?  I'm not even sure what ActivityAvg actuall
is calculating!  But no wonder qMean was so different than RPS -- they
really hvae very little to do with each other.

Maybe I just need to fix ActAvg to actual use the decay correctly?  I
need to think about this and figure it out.  Well, that gives me something
to dig into I know I can fix!

Ok, tested some.  It makes no difference how long it's been since the
last update of the Activity Avg. So the old value doesn't decay with time.
The amount of the avg assigned to the new value vs the the old, is not a
function of time. For a real long time period, the old value should have
decayed to a tiny fraction, and the new value should be most the avg.

But if you give it a second number at the same time, then what happens?
No decay, and the two numbers are just averaged and divided by two?
But if you input 10 numbes at th same time, they should all have equal
weight.  I think what I needed to do, but wasn't doing, is using two
ActivityTraces. One using values of 1. And one using the input values.
Then divide on by the the other to find the average.  I think I
was just doing it all wrong.  Let me try that to see what it does.

----

[8:15 PM]

Ok, tried making activityAvg just use two AT and divide the one by the
other. It's not doing what I thought it would.

If the activity trace stats off at zero.   Has a half life of 100.
Then at time 100 I avg a value of 100, then half the avg should be from
the 100 and the other half from the rest -- so the result should be
50. But the result is 66.6666.  I have no clue.

I'll have to dig into this and figure out why it does't do what I expect
it to.  And is ActivityTrace even doing what I thought it was doing?

Also, Activity Trace was set up to start with a non-zero value. It was
coded to start with a value of inc.  Thast's odd, but I assume I did
that for divide by zero reasons.  I'm tired.  Still sick.  Need to rest
before I dig in this this probably tomorrow.  Unless I have insight and
have to run back and code somethign.

----

[10:32 PM]

While watching TV I figured out what was going on.  The divide the two AT
to get the average should have worked, but I was confused by the fact it
didn't correctly start at zero.  Adjusting for that by stating at zero,
and dealing with the divide by zero problem that would happen if it were
called before the first item was given, it worked as expected!  Now
ActivityAvg does what I thought it was always doing.  Not sure it will
make much difference in how I was using it but I'm much happier with
it now.

Since it was used for qMean which was a crtical part of the system it
might in fact has been kinda important to get it right.  The old way
was calculating an average, but not the correct one.  Not sure what
it was calculating.  But to be sure, I really should check the old
against the new and verify the two are not doing the same thing.  Shit
maybe the old on was working correctly and I just thought it wasn't!

-----

[11:05 PM]

Well Shit.  Turns out the old ActivityAvg was working correctly after all.
Because the ActivityTrace it used started as if it had an update at time
zero, the Avg acted as if it starts off with one value of 0 already
factored in at time zero.  So the first values it produces could be
confusing if you didn't expect it.  I changed it to start at zero now.
Not like that would make any real difference anyhow.

What exactly did I see to make me think it wasn't working??

Now I'm really confused.  Good think I tested it to find the truth about
my confusion.


-----

[11:31 PM]

Setting hl to 0.01 and fixing the system to apply rewards the second it
comes out of the net instead of caching and holding, the system should
be applying the rewards directly to the nodes it just passed through.
The qMean, should be the mean of the nodes it passed though, and the
target should be discounted back to that time. BUT IT"S NOT. It's
discounting it all back to the time of the last reward -- which was
so far back in time, it's beyond the memory of the system.  Which means
the target is not cacluated correctly --- it's calcuated as fucking 0.

Ok, too late now to cope with this. I have to think this through and
figure out what's right tomorrow.  But with this micro half life, all
rewards should be applied directly to the pulse route that caused it.
And it's not,  So something is wrong with my logic!

Also, before, becuase the code was caching and applying later, none of
this was working as expected anway for the short half life.  And when
it was updated, it had already passed though the first node, so it
was lgoically acting as if all the update was applied to the first node
in the NEXT PULSE -- all wrong for this micro short half life.  Ok
sounds like this is the right trail, just need to figure out what's right.

And stop caching rewards.  If the sender wants to save CPU, HE should
cache it and appliy it as one reward summed!  If I do cash it, the second
the clock ticks to the next point, the cached rewards should be
applied for the last clock!


================================================================================

2016-02-25 9:56 AM Thursday

Yesterday

1) Made link q learning use qLearningSpeed, and node q learning use
sqrt(qLearningSpeed)

2) Started digging into how a 11x10 net learns and doesn't learn. Tried
using very small halfLife 0.01, and found qMean and node q values were
doing very odd things -- shooting up to values like 20 when the avg
rewards were only 2.

3) follow me issue.  On the large net, P1 was being given a higher
reward for following the P2 pulses. Which seemed odd at first.  But then
I realised the problem.  If P1 tries to take a different path and makes
it to the right output, it gets rewarded.  But, it also cuts a path that
causes other P2 and P3 pulses to follow which then creates negative
rewards!  So P1 is being punished for leading P2 and P3 down the evil
path!  Which opens a whole can of worms about how hard learning might
be in a follow-me network like this.  But it also relates to the fact
that I'm training the network to learn simple instant gratifications,
but using a longer half life.  So for this problem, ti make it easy to
learn, I should be using a very short halfLife.  And I tried it, and it
didn't work, becuase it revaled problems in the code below that show up
in a big way when I tried to use a very short half life!  And if I
fix those other issues, it will be intersting to see what sort of
rewards P1 really ends up with on the longer halfLife!

4) Found for one, when I applied rewards, I had alrady updated the
ActivityTrace of the first node, so the reward from the last pass though
the net was being applied to the first node of the net.  Ugh.  Fixed that.

5) Found ActivityAvg was not doing what I thought it was.  Rewrote it.
The rewrite fixed it. Then I tested and found the rewrite was doing the
same as the old one, so I changed nothing.  Now I'm testing again this
morning and I'm still confused.  When the avreage is 0.  And the clock
advances 100, with a half of 100, and I avarge in 10, the average becomes
6.6666 istead of 5. Why is this?  Why isn't the old value dicounted
by 50%?  I'll have to dig deeper until I undestand my confusion.

6) Found that how applyReward() was working, it was all wrong.
This became obvious with the very short halfLife. If I used a halfLfe of
0.01 and the reward is applied at the same clock tick as the pulse went
though the net, it should be all applied to the nodes the pulse just went
though, and all other nodes, should be so far in the past, relative to
the halfLife, as to have no learning effect.  But becuase I'm using a
discount in the the applyREward() back to the time of the last reward,
that discounts this reward, to zero!  That's all wrong.  But I'm not
sure what is right. So that's the project for today.  To start with,
I realizsed, that the ActivityTrace IS the discount that should be used
for the reward!  Not one made up back to the time of the last reward!
But then using rerwad+qMean without other discounting seems wrong!
I'll have to think though what's right and make very short halfLifes do
what they should do -- apply all rewards to the path just taken.

7) qMean has never quite agreeded with the RPS numbes and this has always
seemed wrong.  And the q values never seemed to match the RPS numbers.
If long term RPS is 1.87 why does't qMean match that exactly?  All the
issues above about ActivityAvg not doing what I think it should, and
qMean not doing what I think it should is died up in all this confusion.

Today

Job one, figure out ActivityAvg and why my expecations don't match
reality.

Job two, figure out how to apply rewards correctly for very short halfLife
or for longer ones!

Job three, figure out why my expectations don't match reality for q
values and qMean and RPS

I've already gone to Starbucks and did Ingress and I've already started
to play with ActivityAvg before I even stopped to write this beginning
of day stuff.

Oh, and John Dittmere sent a message yesterday about 20x21.5x1/8 plate
for free pick up.  I wrote a letter to Phil and Keith to let them know
about that this morning as well.

Now, I feel the need to make a cup of coffee, and back digging into
ActivityAvg!

----

[10:46 PM]

Ok, My confusion over ActivityAvg is figured out.

When I average in 0, at time 0, and 10 at time 100 for a half life 100,
then the old value of 0 is discounted by 50%, but the new value doesn't
make up the rest -- it is factored in at full value.  so 100% of 10,
plus 50% of old value.  That's becuase that's how ActivityTrace works!

So no matter how long or short the halfLife is, the new value is added
in in full strength, and the old value is discounted, a little, or a lot.

So even with a very long halfLife, the new value makes up more than 50%
of the new average every time.

Which is odd.  And not a very useful average.  And this explains
why chained UpdateAvg all seem to update at almost the same time.
it's becuase it's a very fast average, even with an ininitely long
halfLife it only approaches a 50% update and why halfLife has little
effect on the speed of the update.

Ok, so now I undestand why I thought it was broken!  Beucase it wasn't
doing what I expected it to to, but it was doing what I conceived of it
to do!

So now I better understand what UpdateAvg really is.  And I have to ask
whether it's what I want, for something like qMean.  Is it right for that?

Ok, lets go on to problem two.  How should applyReward() work!

Lets do some thinking to understand what's happening.

q values are discounted sum of all future expected rewards.  The Activity
trace defines the discount.

With a single back prop, the formula should be

target is discount * (r + current_q) 
or
target is r + discount * current_q 

either should work

But, for our time based system, r should be discounted back to the time
of the last state.  But q Value don't decay.  ActivityTrace does.

ActivtyTrace * r counts r as if it had happened an infinite number of
times instead of only once. Or, happed as many times as the node
has been used since the start.  Which is not what we need.

But, we do want to apply the error update that many times in effect.

So:

q += (r - q) * activityTrace * speed

So to make every q a good predictor of expected future discounted rewards, we just
back prop r instead of d*(r+mean) as we are now doing.

But that doesn't address secondary reiforment.

let me try just using r and see how the net behaves and if that works as expected.

----

[12:17 PM]

OMFNG.  ActivityAvg() does work as I thought before I started to confuse
myself.  A long half life does turn it into a very long running average!

What caused all my confusion was setting halfLife with an INT!
That didn't work!  So setting halfLife=100 was not the same thing as
setting halfLife=100.0.  So it was my sloppy testing of using an int for a
halfLife that was the real problem in all my confusion!  And no where
in the xnet code had I made that error of tryign to set halfLife with
an INT so that never was part of any problem!

So using ActivityAvg() for qMean can make sense.  At least for longer
half lives.  But it gets odd when using short halfLifes.

So I've added float() to prevent THAT from causing confusion in the
future.

----

[12:26 PM]

Added reverse video to showNode() to highligh max q

Fixed ActivityTrace to float() halfLife to preven that confusion again.

Changed ActivityTrace to start at zero -- didn't cause any divide by
zero issues.

----

Now, on to rewards again.  Switching to using the reward as the target
intead of d*(r*inc + qMean) did work just fine.  It made q values good
predictors of future rewards.  So in my short half life large network
the P1 input went to something like -0.8 and the P2 and P3 input nodes
were like +0.8 range.  But how well netowrks can learn with this I don't
know yet.

and qMean has very little value at this point, because it's such a
short half life it mostly only includes the mean of the last nodes a
pulse was sorted though.  But it does match the ActivityTrace so it is
a good parallel to that.  It tells us what the network "thinks" is a
likely future predictor. Sort of.

If the first pulse is p1, then the qMean will hit the first node and take
on most the value of the first node, which is like -.8. But as it moves
through the net, most the other pulses have values like .2 or something,
so the "good" -.8 value quickly gets very deluted with nodes that don't
know any better.  And the last node, has a qValue based on the fact that
all three pulses are traveling through it, so it's not much better.

So I'm not so sure that using a mean of node values is the best way to
turn qValues into a prediction.  I've talked about this issue in the past.

I could train the network to make the mean a good predictor.  But then
node qValues would not be good predictors on their own.  They would become
the values needed to make the mean a good predictor.  That's the problem
I had months ago when I was using mean like that.  We can't train the
net to make q values good predictors, AND train it to make the mean a
good predictor.

And we need link q values to be good predictors on their own to make
the wiesest sorting decisions posible.

Q values, that are near the mean, are weak predictors -- basically, when
a qValue is the mean for the network, it's making no prediction at all.
It's just reporting wha tthe Mean of the network is becuase it's use
has no predictive powers.

So to do secondary reinforcement, we need to look at qValues that vary
from the mean.  But what is the correct way to calculate the real
network mean?  The forwardlooking ability of the qValues is defined
by the ActivityTrace decay.  And in theory, they already ARE the best
predictors possile.

So, what would secondary reinforcement even do?  How would it make them
better predictors than they already are?

Ah, so ok.  Secondary reinforcement is moving predictions further back
in time.  It's extending the range of the system beyond the reach of
the halfLife in the ActivityTrace.

And this is all about the systems perception power! So why not just
chagne the activityTrace half life to be 10 years?

Ah, because behavior, needs to be based on STATE of the environment.
And different states of the environment, could be good or bad states.
And since state is broken into micro actions (sorts), the same micro
action might be used in a wide range of different states to roduce a
wide range of different external behaviors? Maybe?

So running is sometimes very good, and sometimes very bad. So it's
long term q value might be average.  But the links that connect to
the differentstates can have good values to triggering it or for not
triggering it?  But it learns now to run when it's bad so then running is
only used when it's good to run, so it's qValue reflects how good it is?

Still lost.  Lets these this new network that only back props rewards
and doesn't use qMean, and see how it works on the tests from the past
few days on the small nets. In theory, it should work as well or
better than what it was doing before.

----

Ah, with a tiny halfLife the net can't learn even a very simple
one layer problem!  Beucase the *a policy overpowers both b and w
learning!  Ah, and maybe becuase I started the AT at zero now as
well!  So, we need to fix the *a problem.  The foceExplore is
not even fixing it!

----

[2:34 PM]

Well shit again.  I'm going in circles.  Two steps forward, one step back!

If I back prop the reward as the target, then I'm not calcuating REWARDS
PER SECOND, I'm only maximising REWARD VALUE. WRONG WRONG WRONG. I knew
that before.  That's why we have to back prop a reward relative to a
discounted q target of predicted future rewards!

So instead of seeking a value like 4 for 4 RPS, my qValues are seeking
tagerts like .99 for an aerage future reward value of 1.0.  That makes
the system seek the lager reward, but not to the larger sum of all
future rewards.  That's fine for the simple test I'm doing by rewarding
pulse sorting, but not what it is needed in the long run.

But I can't just make target = reward + qMean.  Without discounting? Can I?
Won't that make the qValues blow up to infinity?  Let me try that real
quick just to everyify. Becuase I can't grasp what the correct
discount sould be!

----

Yeah, so using reward*inc + qMean without a discount blows the system
up. But very slowly.  I stopped watching when q values hit 40.  Because
forceExploration was causing -1 rewards, the system might have
stablized at some number instead of hitting infinity, due to the negative
rewards.  But it's not calculating rewards per second or anything
close to it.  So there needs to be discounting. But how?  What discount
do I use.  What I was using was time to last reward, and that doesn't
work because it causes the target to be basically zero 

Wait, the trace is the discount.  But maybe I'm just using it wrong?

if we do at * (reward+qMean), that is the discount.  Then compare that
to the q value, to create the error?  That can'e be right, because
a near zero at will cause the target to be zero.  It shouldn't 
do that. a near zero at measns "very little effect on me", not, 
very low furture value.  Right?

----

[3:37 PM]

Ok, found something that works.  Trying to talk myself into beliving
it's correct.

target = reward * inc + discount * qvalue

So the current reward is not discounted and is applied "full strength"
per activity trace ratios to all nodes.  But the qValue has to be
discounted back to the last time the qValue was updated.   And inc is
needed to convert units to make the sum of future rewards come out to
something in the ball park of rewards per second.

But, the RPS number is still got a long term average of 4.68 ish value.
And the q value and qMean is hovering around 3.7 now.  Why can't the
two converge on the same answer?  Maybe it will in time?  Maybe qMean
is just a bad predcitor and as such, will always be low. And if it's
low, the q values will allways be low?

Whoa, wait.  What if I use the nodes OWN q value as the estimator for
it's own update instead of qMean!  Shit, that might work.  Let me
try that and see what happens.

----

Yes, this is looking better! So the best predictor for a node is itself!
so we discount the qValue of the node, back in time to the last time it
was updated, and apply the full reward + the dsciounted qValue to itself
to make the qValue the best predictor possible!

It's only making a small difference in wValues, but I think a valid
difference.  I have to wait for the two runs to stablize on a longer
term to see how it works out. But the good node, that can't sort any bad
pulses, has a q value of 4.02 ish now, where it was 3.89 before.  And the
p1 node has a q of 3.50 ish, where in the other way it was 3.69. So the
nodes look as if they are more acculately calcuting their true future
and not getting plamed for the mistakes of other nodes as much!

Ah, but by doing this, we have no secondary reinforcement. And with using
qMean, we do. I think. So maybe this is valid for calculating the true
short term q value of nodes, but not for spreading secondary reinforment
back futher in time?

Ok, but I want to let the test run and see where the q values end up.
I'm still unsure if this use of qMean this way is the best or correct
way to do secondary reinforcement.

----

[4:38 PM]

After some time, the new one where nodes use their own qValues as future
estimators is showing slightly higher q values for everthing, but also
higher RPS (4.69 vs 4.43) which is odd I tink the older run was showing
more like 4.69 in the recent past so I think this may just be normal
random swings up and down due to how many pulses get sent bad paths.
None of the nodes have any issue understanding what is right. b vlues
are all hard nailed at 100 0 0 0 0 for the the right answers.

Oh, shit,  Checking the graph, I see the old one had a drop off colapse
like event.  What on earth caused that?  The drop is only from 4.7
range to the 4.43 range, but still why?

Oh, I have a guess. The can-do-no-wrong P2 pulse probably shifted to a
different output. And in doing that, it probaly was able to suck some
P1 pulses down to the wrong place.  Due this this *a having too much
power over b. Or maybe, it was helping P3 get more to the right place,
and is now shifted to a node out of rach of P3 so it can't help P3 aymore
with follow-me activity, so P3 started to send some pulses down to the
bad output at a higher rate?  Yeah, that's what it has to be. In this
one layer etwork there is no other options for how that could happen.

Can the system learn to use the pulse that helps p3 and p2 get to the
right place becuase it produces more rewards?

Yeah, so the older bad net is sorting p2 and  p3 to different places and
getting less rewards.  But new net is sorting them to the same place at
the moment, and getting more rewards.  So, need to let these run and
see if the older one tha tdipped can recover, and if the newer one
has the same problem.

I have hard wired the *a to have a 10% cap on it's lower power but this is
a bigger issue I need to look at!  That is not a fix, it's just a kludge.
But I bet the extra noise raised the odds of sorting the wrong way just
a little, to cause the drop!

Ok, feel like pizza break time. Haven't been there in a few days now
and I'm not feeling as sick as I was.

----

Oh, and another thoughts.  the qValues now discount the old value,
and add in the new reward * inc, which is exactly parallel to what the
ActivityTrace is doing to calculate RPS.  Using the exact same discount.
So why aren't the values closer?  The q are just using this new value as
it's traget to move towards.  The RPS is using that AS the new value.
As different rewards are given, the values bounce up and down.  and my
long term average of RPS, is running off the values right after they are
updated.  But q values are showing an average of the tragets relative to
the average.  Ah, so maybe this is just an issue that the RPS is bouncing
up and down, from 5 to 4, but the long term average is samping it at the
peaks of the waveform.  So it's a peak measure, and the q values end up
seeking an average value.  If I update the long term RPS both before, and
after, the update, maybe they will be close to the same as the q values?
I need to try that.

Nope, that's not it.  Using the RPS value even at the bottom was still
like 4.4 where as q values are 4.0.

I think I see the light.  When we look at the RPS values, the bad rewards
like -1 are far in the past most the time and the good +1 values are
more close to the future most the time.  So the bad values don't get
as much weight on average in number as the good values.  Or, they get
a strong value when they happen, but fade quickly.  So if they happen
1 out of 100 times, they get a 1% weight in the qValue running average,
but they get far less weight in the RPS on average.

That might be bull shit.  Hard to tell.  But certainly, the two numbers
are being calucated with similar, but most certainly different technqiues.

So it's not wrong that they are different.  Just wish I had a better
understanding of exactly why.

Maybe I should use the qValue technique to calcuate RPS and see what
happens?  But don't bias it by activity like I do with the real qValues.
And see if the seeks a value close to the current RPS or close to the
current qValues!

Ok, pizza first.

----

[6:57 PM]  Back from Pizza

Ok, exploring ultra short halfLife has exposed a bunch of problems.

Led me to find int halfLife bug in AT and better understand ActivityAvg.

Led me to fix when rewards are applied (at the time they are given,
not after updating first node of next pulse).

Exposed the fact that b learning isn't powerful enough to fully overpower
*a policy.  Which is an issue I have not fixed yet.  Or which I don't
even fully understand yet. I think that is the basic casue of this single
layer network losing rewards when the outputs shift to what should be
a second harmless output.

Led me to understand a qValue can be used as it's own target to get the
best estimates!  gee, now that I think of it, I don't think I'm doing
link updates correctly with this!

----

[8:18 PM]

Added more debug into showNode() so I could really see what was happening.

Turns out it was adding in forceExploration to policy that was was the
big problem.  With policy being limited to 1% it didn't have the power
to overpower the *a term.  So I had to take forceExploration out of
policy as it was used in the policy and add it back in before it was
used for reward balancing.

----

[9:08 PM]

Fixed forcedExploration.

Fixed display of halfLife on graph.  .01 was showing up as zero. :)

The single layer test is dead sold now.  As far as I can tell.

Went back to a 11x10 grid which is what I first tried using a short
halfLife on.  BAM!  Fucking solved it in 5 minutes!  With a halfeLife
of .01.  Waiting now to see if it stays stable for an extended time.

need to see how q values stablzie and if the algorithm has colapse
problems or the like.

The other runs I had running for like 24 hours on the large grid got no
where!  Clearly this is working much much better now for a short halfLife.

A big grid with a hl of 2 solved it as well and it's still running now
from 2 days ago. But the reward line if very very noisy.  Probably becaues
of that polcy mistake I made.  I'll have to run it again with the hl
of 2 with the new code and see how it works.  Maybe I've made it work
great with a short hlife but broke it for longer halfLives. That would
be typical. :)

Time to go watch some TV after I get this other test started.

----

[11:12 PM]

Watched the NOVA Rise of the Robots show about the DARPA Robotics Challenge.

Got 6 or so tests running. All working very well so far.  The large net higher
halfLife has not converged yet.  It did with the old softwrae, but took 12
hours. Maybe this will be faster.  Keeping fingers crossed.

All runs are showing dead flat convergence lines.  No wavering.  No colapsing.
Yet.

Just have to wait now.  Think I'll fill in some of the summary of today's work
in the "Yesterday" entry for tomorrow!


================================================================================

2016-02-26 10:38 AM Friday

Yesterday

1) Resolved all my confusion over ActivityAvg.  It always did work mostly
as I expected it did.  The big problem in my testing that confused me was
that I entered halfLife as an int and that broke the code.  I've added
the needed float() now to prevent that confusion.  And I understand
ActivityAvg a little better as well from all the testing.

2) Dug into the difference between RPS and qMean. Still no final answers,
but better understanding.

3) Explored and fixed issues with why short halfLife wasn't working
at all.

4) Changed reward updates to use reward as target.  Then later realized
that was all wrong becuase it cuases the qValue to seek the reward VALUE,
but not rewards per second!  So it would make the system seek the highest
reward value, but not the MOST rewards over time. All wrong.

5) Went back to using qMean as target, but then realized the problem was
that I needed to discount the target, but not the reward.  That made
it apply rewards correctly to the current nodes the pulse just sorted
through when using a very short halfLife.

6) Realized that the best predictor of the futrue was the same qValue I
was updating. So switched the code to use the qValue being updated, as the
future predictor.  That made everthing slightly better and more stable.
But we lost secondary reinforment by doing that.  Will have to decide
if and how to add that back in.

7) Found that my fix to the link.policy to add in forceExplortion to it's
value actually broke the ability of b values to overide a values. And
that added noise into the system.  Fixed that.  Added an rpolicy that
includes forceExploration for the reward processing to use for activity
equalizing, but left policy as a variable for debug output in showNode().

8) With all these fixes, the net is working fantastic!  It solved the
11x10 net in 5 minutes (with tiny hl)! It's solved every problem I've
given it so far -- but some are still running.  Nothing has Colapsed yet.
All the convergence lines on the graphs are dead smooth and flat compared
to what it has been.  I'm running test with e-6 w learning instead of
e-8 to see what that can solve and how fast.  Some were taking e-8 to
not dolapse before.

9) Filled out this "yesterday" report for tomorrow becuase I'm now
just waiting on tests to run to see what problems devlope.

10) Go to go to pizza today and play ingress a little which I've not
had time to in weeks becuase of my addiction to this work.

Today

[10:41 AM]  Saved graphs as images and stopped most the tests I had
running.  They all converged and were stable, nothing seemed to be
changing.  The two large 11x10 nets with with haflLife of 1 and 2
are running since last night.  Nether have converged on
the p1 solution and are still flatline at 1.8 range.  One of these
runs converged yesterday after something like 15 hours so these
may take a while still.  And I think the q learning at e-6 is slower
on these.

Wondering which way to go next.  No big outstanding issue to fix
at moment.  Have to go digging and experimenting.

----

[12:30 PM]

Picked OneNodeTest at random to play with.

Moved Plot code into PlotRPS class to make it easy to add to OneNodeTest

Started to look at PRS and calcuate it the way q values are calcuated
to see how that compares with what the node q values seek to.

Decided to clean up reward().  Removed all the delayed reward support
since we are not doing that anymore and merage apllyReward() back in
reward(). Cleaned up other split out code as well since this use of node
and link q value rocks and I can't see going back at this point.

I had code to ignore duplicate rewards sent at the same clock time.
Didn't want rewards to be lost, so thought I might have to write complex
code to track the last last clockTime to be able apply dup rewards to the
same old time period.  But ebfore writing that, I tested what happens
when rewards are applied at the same clock, so dt=0 and discount=1.
It works just fine!  In fact, I think if I had written the code to apply
the old rewards again using the old discount it would have broken it!
So calling reward() multiple times at the same clock tick is fine. Just
a waste of CPU.

Deleted unused variabls and updated comments thoughout the reward() system.

----

[2:33 PM]

Wrote RPSQ code to calcuate an RPS value using the same update formula that
q values are using to see how that compares to the ActivityAvg RPS, and to
qMean, and to the other qValues.  And the answer is -- it's yet another different
number.  On a long running 8x5 test, RPS2 is around 4.484, RPSQ is 4.6 to 4.7
bouncing around. Slow qMean is 4.883.  Input node qValues are like 4.7 to 4.9

Ok, so they are all slightly different but yet all in the same ballpark.  It
doesn't seem to make any difference which is used to understand the general
reward level of the network and suttle difference in how they are calculated
adds up to the differences in values that are in my mind, too complex to
understand.  So I think I'll just throw out the RPSQ code (easy to add back
some other day)

----

[6:01 PM]

Went to Pizza, just got back.

Experimented with a few different tests.  Mostly working great.

Found TwoNodeTest doing something really weird I now have to track down
and fix.  It got stuck sending pulses out the two "none" paths, and not
out the +1 path!  And whenver it started to try and do the right thing,
it would take off for a short period, and then quickly shift bcak to
the back condiguration where it was stuck!

One part of the problem is that that "bad" path, has very few rewards.
So the traning becomes very slow once it gets stuck there.  A snales pace.
Plus the half life is set to 1, and in the bad configruation, it may
be many seconds before the next reward shows up.  So now only are the
rewards far and between they are often way outside the halfLife window
making it even slower and harder.

But, I have no clue why it's not quickly learning from the 1% explore
pulses sent to the +1% route!  Or what drives the net back into the bad
state all the time.

Ok, well, I take some of that back.  The reason it's not learning from
the +1 path is that it IS learning from the +1 path. But that set the
q value for that link to .45 ish.  And the q value of the "nothing"
links were higher.  So the problem is that the higher q values take
FOREVER to sink back down becuase of a lack of rewards in the system.
And through the graph of the system was FUNNY (saved it), it did converge
on the right answer in time.

But this two node single layer with no overlaps test should have been
TRIVALLY EASY to solve, but yet something about it drove this version
of the code insane!  So time to get to the bottom of this!

----

Spotted the big difference!  The test was set up to start the q values
at 0.9!  Which mean they were all high, and had to sink down to get to
the correct values! So every time it started to get a flood of rewards
for doing the right thing, q values started to sink fast to the correct
values, BUT, the good q values were sinking faster for some reason,
turning over control the bad -- and by the time the control was turned
over, the good q values had enough lead on the downward hunt to mean the
bad q values (with no rewards to correct them) took forever to catch up.

So, the net can't seek down well!  It does very well with q values need
to increase, but when they need to drop, it looks like there is a real
issue here.  Or maybe, really, it's just an issue with the fact that with
this test the bad behaviors were not being punished with low rewards,
so there was nothing to drive them down.  The good behavior driove itself
down with good rewards, but and the more it was randoly selected over the
bad, the faster it went down!  But when the bad behaviors were randomly
selected more than policy nothing changed - there were no bad rewards
to drive them down!s So in short order, the good behavior had dirven
itself down out of reach, turning over control to the bad behaviors and
getting stuck "wihout power" so to say.

Ok, so now I'm feeling like we have just run into the secondary
reinforcement problem!  When the net is stuck in it's "bad" configuretion,
nothing is rewarding it. But yet t q values are all wrong -- there is
just nothing forcing the q values to update each other and make then
consistent.

Ok, so a zero reward, should have no effect on the total rewards per
second or sum of discounted rewards.  Adding in zero discounting doesn't
chagne the value.  So we should be able to send in zero value rewards
to the network to create secondary reinforcement.

Yes, and that was the issue here. The RPS as near zero, both becuase the
+ and - blanced out, but also becuase there were just very few rewards
at all.  Adding in zero value rewards should help fix this.

All need to do some testing with different reweard sets. Instead of =1
-1 as the rewards, I chould use something like 10 and 11, to see how
it seeks. and -10 and -11, to see how that works.  And then add in zero
rewards at a slow rate to see how that helps it get unstuck!

----

[7:14 PM]

OMG!  I really need to learn what I'm doing!  I just realized I failed
to grasp the KEY issue about rewards not being relative!  I was thinking
about that -1 +1 would act the same as say 10 11!  But it doesn't!

That's because the net is now maximizing all future rewards, which means
the SUM of FUTURE rewards!  So adding in -1 is worse than doing nothing!
But adding in 1 is always better than doing nothing!  So negative numbers
actually are punishments -- they make the sum worse!  And postive numbers
are rewards!  They make the sum higher! So doing nothing (that is getting
no reward), is the same as getting a reward of zero now.  And sending
in rewards of zero, don't help or hurt the net's sum of future rewards!

A net that could take any relative values and act the same way would not
be maximising total rewards, it would only be maximizing reward size!
And that's not useful because 1000 small rewards should always be more
valuable than 1 one double size reward!  Ok, so shit, I've learned
something I should have had a good understanding of 30 years ago but
all this time I've been obvious to this finer detail!  Well not totally,
but I failed to fully grasp the full truth of it all.  I failed ot grasp
that maxiing total rewards had to implied that rewards were signed and +
rewards were always good and - rewards are always bad.  So yes the net
will pick -2 over -4 given a choice of only those two, it will always
prefer the higer reward, but it's the sum of rewards that it really
cares about.

This also means that the idea of normazing rewards around their average
is wrong!  Doesn't it?  Yes, beuase if the rewards are +10 and +20,
the net will alway try to get those vs not getting them. But if you
normalize them to -5 and +5, thjen the net will act differently. it will
activaly avoid the -5 rewareds if it can.  So if you have a maze with
food, and some food is +10 food and some is +20 food, the agent will
collect both as much as possible.  But if one is -5 food and one is +5
food, the agent will always avoid the -5 food! So no, you can not just
normalize the rewards without having side effects.  Darn, I was doing
that years agao, and had written it into the code for a bit in the past
few months here but didn't grasp you can't do that if the net is in fact
maximising total rewards (over time)!

Well, that's why I'm working onm this full time now -- to figure out
what I don't know and to fix all my false ideas and designs until they
aren't any false ideas and bad designs left!

[7:27 PM]

So, fixing the Two Node Net turned out to be far simpler than I was
expecting. Basically, there is nothing wrong with the network design or
code this time! It's just me learning what happens when a network can
accidently think a no-reward path is the best (while at the same time
being faced with falling q values)

**** So, when designing reward functions, it's clear that it must
contantly inject 0 rewards if it's not injecting other rewards! ****

And, it's always better to have the network q value rise over time.

So mostly postive rewards with negative rewards only used for things
the agent should avoid.

Ah, and with this new insight on rewards, gee, evething is different
now. I've been falsely thinking for a long time that there was no such
think as a punishment and that all there really was, were relative
rewards.  That a drop in expected rewards was always no different than
a punishment.  But carrots and sticks really are different beasts!

And it all has to do with the agents ability to choose to avoid the
carrot or the stick.  All carrots, are always good, unless the same time
could be better used to get a better carrot.  Bending down to pick up a
penny may not be worth the time it takes to bend down and pick it up, or
the trouble to carry around dirty penneys, but But the penny is still a
carrot and good.  Where as having a rat bite you is always a stick you
will avoid.  Life never gets so bad that a mouse bite would look like
a carrot -- It's still a stick and only if it would allow you to avoid
a worse stick, would it be selected.

And, traning behavor with sticks is in fact very different than training
behavior with carrots.  It's avoidence behavior, vs seeking behavior.
Wow, now I need to think deeper about everthing I thought I understood
but didn't!

And this whole conservative vs liberal theory takes on new deeper meaning
as well.  If you see the world as a place with more sticks than carrots,
the would will of course look like a very different place, then when
the world is filled with more carrots than sticks.  One place you love
and seek out, the other you hate, and avoid.

Ok, but expectations can be and are relative (qMean).  So if the world
is full of sticks, we do learn to avoid them the best we can, but when
we get less sticks than expected, it acts as a reward in that it makes
us do more of what helped us to avoid the sticks.

So there is very much a relative nature to the problem, but it doesn't
mean we can transform rewards and lose their "true" sign, and not expect
behavior to be totally different.  Subtle but important points!

[7:46 PM]

Next issue -- I gained a little insight into qMean vs RPS

For a short halfLife, the distance between a pulse event and the reward
becomes crtical.  Instant rewards count very highly, where as rewards a
little bit out in the future are heavilly discounted.  If a node always
produces a reward, it's q value will be very high becuase it has close
temporal connection to the reward.  But if another node in the network is
not assocated with the reward, and the temporal distance to the rewarfds
produced by the other node is at different random times, then the average
discount for that can be very different.

So, one node, pulses once a second, and gets an instant reward every time.
So it's future looking pulse stream is one reward right now, and the
next one 1 second in the future, repeating forever.  But the other node,
that fires at the same rate, has an average pulse stream that is random
spaced in the future.

Both see the same 1 pulse per second stream, but if the half life is 1
second, the future fades very fast, making how close on average you are
to the first pulse highly critical to the nodes q value.  And highly
critcal to how inc is calculated!

So how I do it now, inc makes the fading hit 1 half way though the fade.
So the second node, that sees the pulses at random times, will have a q
of around 1. But the one that sees it right when it happens every time,
will have a value much higher than 1. And the RPS number will jump to
that high value then fade to below one, and if that is averged without
biase at different times, the average will be a bit less than one because
more of the curve is below one.

But the qMean, will be averging the q value that might be 2, with the
q value that is 1, and end up with a 1.5 qMean.

So the big bias here is created by what the temporal corelation is between
the use of nodes, and the time the rewards show up.  And the shorter
the half life, the more this effect is amplified. So some tests with a
halfLife of 1, where rewards show up only 1 a second or less, there can
be a big variation in node q values and q Mean can be much higher or lower
than the average rewards per second depending on what that correlation is.

But when halfLife is long compared to the reward activity, and pulse
activity is fast, compared to halfLife, then all the q values and qMean
and RPS start to get closer together.

This effect probably the largest single effect of why these values can
be so different at times.

[]

Ok, so there's a remaning issue with the TwoNodeTest I'm seeing

When two paths are equal, it is at the moment, not flipping back and
forth between the two paths. It's stuck on one.  Maybe in time it will
flip, I haven't been watching closely. But it feels stuck. It feels like
there is a bias at work here.

Again, this might be the secondary reinforement problem.  That is, with
all nodes using only themslves as the the future predictor and not cross
training each other, they might tend to get stuck like this with very
close, but yet different values.

In this case, on link q vale is 0.0089 at the moment, the other is 0.0075.
They bounce up and down a bit, the first has been higher for as long as
I've been watching it.  The third, which has been getting a very minor
random negative reward (-0.01 + random) is 0.0038.  So it's ok that it
is lower.

Ok, so damn.  They just flipped as I was writting this.

The first is now .0082 and the second .0087.

Now, 80 and 89.  Hum.  I guess my only issue then is how fast it
takes to flip, and how long the system tends to stay on one side
vs the other.  This is not good for percpetion learning.

(many minutes later 56 and 69 ?????)

Ah, but I see a point.  When all three are good values, the w learning
is working to keep the activity balanced becuase it's punishing the most
active, and rewarding the weakest. But if q learning has picked a loser,
(as in this case) the loser is getting a lot of the help from w learning
trying to push it higher (in vain), and failing to encourge the second
best guy.  But actually, now that all the traffic is on one of the two
winners, w learning is balancing the other two, becuase the activity is
detemrined by random forceExploration, and that allows w to blance between
the two.  But once the second most is allowed to become a little active,
w learning doesn't help, beucse it does all the boosting to the guy that
can't win allowing *a policy and b to push all activity to the one side.

I feel maybe I need more work on how RL and precpeiton share the work here.

But, when all three are equal, perception seems to kick in pretty well
with the help of w helpint to fight any short term b attempts to unblance.
And though it may be chaotic, it flips back and forth on a regular rythm.

But when b learning has picked a looser, the system tends to throw all
actity to one output a lot.  Which as I was looking at the large netowrk
might not be a bad think because it clears large parts of the network for
new paths to form. However, it seems better to me, if the network floods,
and that the volume of different pulses just incrases and decrease slowly
across the landscape vs having them dig gullies they get trapped in.

Also, I've not tested in cases where nodes NEED to be sorted to mutiple
places to sovle the problem. Or where a singl needs to regulate, multiple
other signals.  And this tendency to over gully seems counter productive
to those types of problems. It should dig a gully, when the reward
structure makes it dig a gully (like p1 to output 1 is reward and all
other outputs are punished), but otherwise, it should flow and not gully.

So, I need to fix how b and w and q all interact to make it not gully
when q values are basically equal.

----

Ok, so there is still this never ending stuggle on how to get the net
to balance and or switch between percpetion and RL. If RL could define a
ratio, then percpetion could operate to that ratio. But RL can't define a
ratio because q values never stay close enough together to make it clear
if a ratio is needed, or if the largest is just "right" and the rest are
just "wrong". In practice in harder real world problems, I keep seeing
that q values are highly dynamic things that flap in the wind, but yet
not flap fast enougn to take advange of the b learning style ratios.

Maybe there's a way to balance this I'll still dream up.  But maybe
the option is to not try?  And give up in this dream of having the net
automatcially balance between RL and perception?

To get them to blance I came up with the cool log distance technqiue but
ultmatly, that was a struggle becaue there's no way to know how close
is close enough.

The use of b learning to just measure the retio, is darn cool and
simple. And works really well for making RL work. But it steals control
too fast and holds it for too long when there is no real winner and q
values are just drifting.

Maybe there's a way to limit q value drift that would help here?
Like the overfitting trick of minimizing w^2 values?

Becaues we are calculating total discountd rewards, q values in this self
defining role amplify small amounts of random noise and can dfirt up and
down by large amounts due to the noise amplification effects.  I think.

So, maybe this is why what I did with q values using their own values as
the reference is a bad thing? It might produce better overall q values,
but allow even more drift?  So manybe retuning to using qMean as the
target will have the w^2 type effect of keeping q values more stable
but yet stll give them good relative values?

Ok, I could try to test that some more with the idea that it's a
constraint on qVlaues for the purpose of adding more stablity to the
system -- so it's "ok" that it's not a high quality reward predictor
but instead, just a mean reward measure used as a rock to stablize the
net with.

And maybe using a longer average mean than the halfLife of the system
is good and even important to add that stablity?

After all, I don't think real q values are important.  Only relative
q values are important.  Wait, didn't I just have a talk with myself
about how real values are important?  Ok, here's the rub.  I need to
stop confusing rewards with q values! Repeat after me! q values are not
REWARDS! They measure estimated future discunted reward SUMS!  Q values
ARE REALTIVE beasts, but rewards are ABSOLUTE beasts!  Postive and
negative rewards are very different beasts, becaues one makes q values
BETTER, and the other makes q values WORSE.  But q values are very much
relative things -- where we only need to know which number is larger,
and only INSIDE a node fanOut!

For wecondary reinforment we need to compare and contrast different q
values, and it's important they compare correctly, but the don't need
to be accurlate, they can all be low, or high, as long as they are all
relative to each other in how much they are off or how compressed or
expanded their values are!

Ok, so enough lecturing myself on that.  For now.

So when average rewards drift up and own over time, it's ok that q
values don't follow the change instantly, as long as we do what we can
to keep them all moving together as a herd and maintaining their relative
values. Ok, so watch for that I must. :)

Using some type of qMean as the future predictor for all of them sure
sounds like it is what is needed to help them all stay in sync.  Just like
we use node q vlaues as a qMean for the links!  Hey, maybe there's more
simitry here than I grapsed.  Or maybe node q values should be relative
to q mean value to create a three layer split on q values? Wow, that
sounds intersting.  So we could do a qMean that is a common q value for
the entire network.  Then every node is relative to that, and every link
relative to it's node.

Oh, but wait. The common node cold have it's own relative activity measure
that would be updated for every pulse (logically as if ever pulse passed
though it.  Oh, gee, cold the next nodes -- the first nodes of the netw,
be relatlve to that, then the next nodes down the streem relative to
the previous? gee this is getting complex by intersting.  And heading
back sort of twoards what we were doig before with the delayed rewards
and using the first nodes as the future reward estimates.

But this is getting complex in regards to using future nodes, and how
to correctl discont them and the probnlems we already ran into and then
threw all that code awa (just today).

Ok, one step at a time. Lets just do a qMean reference and be careful
to watch what it does to the networks.

----

TODO: try rest of test programs on current code to see if any issues
show up.

TODO: Think of ways to test percpetion.  I guess going back to the decode
test would be one attack at that. Wonder how the currnet net that works
so much better on these other tasks might work?

TODO: more contemplation on this percpeiton vs RL merging problem.

----

Hey, just looked up at the TwoNodeTest what wasn't balancing very well
between the two qood options.  At the moment, it looks to be doing
pretty good.  It's jumping back and forth fairly quickly in less than
a minute.  Maybe this is was more of just a conversion issue of needing
to give it more time (hours) to stabalize?  And maybe the use of a common
qMean will help it reach a stable point like this sooner?

----

TODO: test qMean as the target, but also test RPS or RPS2 as update
future predictors!

-----

The two large net tests running since yesterday are still running.
The hl 1 test converged on the solution after a day but is slowly creeping
higher and higher.  Odd I've not seen that before. It's looking noise
instead of smooth as if it's reached the limit of it's resolution maybe?
Slower q learning might be needed?

The slower hl2 one has been running for 24 hours now and still hasn't
converged past the 2.0 test.  Will let it keep running to see if it
finds the solution.  Maybe tomorrow?

[9:51 PM] I think I need some rest and TV and snacks.

TODO: remember what I was going to write.

================================================================================

2016-02-27 1:14 PM Saturday

Late start today!

Yesterday

1) Cleaned up reward code.  Merged applyReward() back into reward to
remove the ability to apply rewards at a later time since I no longer
have interest or need to use the next node to receive a pulse as the
future predictor.  Cleaned up and delated other old code and updated
comments as well.

2) Took out code to prevent multiple rewards sent at same clock tick.
Tested and found it was harmless to do that.  A zero dt and 1.0 discount
did not create any issues for the long term.  This surprised me. Though
I was going to have to write  omplex code to save information to speical
case the handling of rewards sent at th same time. Turns out nothing
special is needed at all.  But it's far more effecent to not call reward()
twice with the same clock -- add the rewards together and call it once.
But I'll leave that as a problem for the caller to solve.

3) Wrote RPSQ code to calcuate RPS value using the same back prop bellman
formula that Q values use.  Trying to better undrestand why RPS and qMean
and q values are different. RPSQ was just yet aother different value from
RPS using ActivityTrace and qMean and the network q values.  They are
are all just slightly different depnding on timming and specifices.
So, throw out the RPSQ code.  Mostly.

4) Identified another difference in q vs qMean vs RPS.  Q values are
senstive to reward timing, or pahse shift.  So if a reward always follows
a pulse at time zero distance, it will be given a far higher value in
the discounted sum, than if the same reward train, starts .5 seconds
further in the future.  So the q value is sentive to the phase shift
of the pulse train it's measureing the expected value of (as it should
be). But this makes for very different q values when we use very small
half lifes (and when they use the adjusted inc conversion factor).  So q
values with near correlations to postige rewards and strong correaitons
to only the postive rewards, Can spike very highly. Wher as other q
values of nodes that have no speical corelations, will be close to RPS.
qMean will be the averfage of the nodes, where some could be average,
and some inflated, but maybe none are deflated.  So the Qmean could be
above average, or below averge, depending on what the nodes of the net
correation to.  Where as RPS will be the average of all rewards. So this
opens the door for qMean and RPS to be very different -- espcially for
short half life cases.

5) Got confused again about rewards vs q values and optimizing for
highest rewards vs total rewards.  Realized this ongoing confuion in m
mind made me think rewards could be relative but now realize that was all
wrong.  When we correctly optimize for total rewards (bellman discounted
future rewaards) we are optimziing for rewards per second, not rewards.
And that means, postive rewards are always good and negative rewards are
always bad.  And the system will always try to seek out postive rewards,
and ignore negative rewards.  q values are relative, becuase the sytem
always seeks the larger over smaller, but q values are rewardws per
second, not rewards -- different units!  Maybe I'll finally keep this
straight and not keep conflating the two.

6) Started testing other old tests with new learning code.

7) Moved Plot code into new PlotRPS() class to make it easy to add to
other tests.

8) Found two node test getting stuck and having a terible time solving
a trivally simple problem where two nodes that didn't overlap, with a
fan out of 3 each, where the top node was punished for sending to the
center, and no rewards for the outside, and the bottom node was rewarded
for the center, and no rewards for the outside.  It solved the top node,
but got the bottom node backwards!  Sending all data to the outside, and
took hours to get itself unstuck! Turned out the first problem was that
qValues were started at .9 (me tryign to speed up testing in the past),
and had to work their wan down to valid numbers near zero. But this
was combined with the problem of the outside paths giving no rewards
at all. So when the system tried to learn to send to the center, it
cuased training to pick up, which drove q values down to their correct
lower values.  But when by random chance, the center got ahead of the
outside on the race to the bottom, That causes the outside to "win" the
q value game, but also TURN OFF, training.  So it kept ketting stuck in
the mode of traning being being turneed off.  Only the forced explore
was traning, but it was very slow.  So the net kept shift to off mode,
where it was doing the wrong thing, and taking many minutes to turn back
on for a bref time, before it fell into off mode again for many minutes.
Hours later, it finally got all the q values fixed, and all was good.

8b) So, lesson learned.  Rewards are important.  The system must be
constantly rewarded even at a slow rate to preven it getting stuck.
And rewards of 0 are useful beucase the don't effect RPS values.  And,
secondary reiforment -- we don't have it now.  Reward(0) is not the
same as secondary reinforment. since we are using q values as their own
future predictors.  Constant secondary reinforment would likely have
fixed this as well.

9) Spent time thinking about secondaryReinforment. No answers.
But thinking I should consdier going back to using qMean as a furture
target which both adds a form of secondary reinforment (I think), and
might keep the network more stable in the long run.

10) Spent time thinking about precpeiton vs RL and the still unresolved
issue of how nodes should change from percpetion mode to Rl mode and
whether how the system works now is valid.  Point being, to sovle this
node sort training problems I have been testing with, it's best for the
the net to cut a deep path for nets to take, so the rest of the net is
freed up for other paths.  But when a node has two equally qood paths,
and one bad path, current code seems to perfer locking onto one of
the good paths, and ignoring the others -- and maybe swtching but only
ver slowely over very long terms, which fits with this cut a path type
problem. But with other problems, odds are good this is a bad options,
and the system should spread activity over equaly good paths and just
ignore the bad paths the same as it does when all paths are truely equal.

It needs to learn to bias flow more carefully vs turn flow on and off
quickly -- which is what it seems to mostly do now.  Either a flow has
been restrucited to one path, or it's sent to all equally -- there is no
slow learning effect to blance flow at different rations in the net. I
have to find a different solution I think.

11) Played a little with HallTest. Got to see it's very odd RPS
graph.  It's strugglign.  Something to look at later.

Today

Something to test -- stotastic rewards.  How good is the system at
seeking lots of little rewards vs few random large rewards.

So, I've had this one large 11x10 net running for two days now with hl
= 2. Still hasn't solved it.  The hl=1 test was solved in about a day.
hl=2 not working.  I started a second hl=2 test using slower q learning
yesterday to see if that is needed to solve this.  That has not solved
it yet either.  This might be very slow, or it might be out of reach of
what the net can do.  Either way, it's not doing it very fast. And this
is still a simple problem on a small net compared to what we want this
system to do.  That's an issue.  We are getting much better over time,
but we still have a long way to go to make this approach work with huge
networks will bilions of nodes.

Got to poke around a bit and figure out what direction to work on today...

----

[5:28 PM]

Added doMean back to reward() to use qMean as the future prediction in
the bellman equation controlled by the command line on off argument.
Been testing TwoNode while writting on Facebook about Basic Incomes.

The use of qMean makes the q values convege to close values in the
network -- less spread from node to node.  But it also seems to address
the blancing issue I was thinking of.  The two equal ouptuts do end up
being used a lot more at the same time with the use of common qMean
as the future target.  But, looking again at the two runs I see both
are sharing outputs at the moment.  Now I don't know if it was just
a problem of having to wait long enough for the nets to converge or
something more fundamental?  I feel I need to do more careful long
term testing.  Ugh, everthing is becoming a long term test now.

But, also, the q values are much closer together, which seems to be
clould limit the perception of the system to identify differences.
Maybe again, the close is jsut due to the indement non qMean network
being free to drift farther apart, wne we might see in tie, the values
converge on the same answers.

Such as for the three way nodes, we have:

.007 vs -.068 spread

betweeen good and bad in the independent net, but only

.00003 vs -.0021 in the qMean network.

The independent version has an order of magnatude larger gap than the
qMean network at the moment.

Other thoughts.  Use qMean for nodes, but not for links? Or the other
way aorund, qMean for links, but not nodes?  Probably wouldn't work well
using it for links but not nodes.

Another thoughts, use qMean2 -- slower than the half life, making it
spread secondary reinforcement effects futher back in time?  Or wait
does it?  No, that doesn't seem right. I think a slower qMean wold spread
secondary reifoment effect fuither INTO THE FUTURE, which is not right.

So, the net does something bad, it opens a door and realeases a dangerous
monster.  Then qMean drops when we it sees the monster. But that doens't
help us spread the effect back in time, becuase that's limited by the
activity trace of the nodes that get updated.  But it spread the effect
further into the future, punishing, what we did AFTER making the mistake.
That's not helpful.

So, all is fine, something bad happens, and we fight this bad monster
for some time, and then kill it, making life good again.  All those
chagnes from normal, to bad, and back to good, must punish and reward
what came BEFORE, not what comes after.  Even the use of qMean is
spreading reward predictions into the future.  That doesn't sound right
either!

----

Good NEWS!  The 11x10 hf 2 net that has been running for two days, finally
found the solution!  That was the last hold out that hadn't converged!

Took 4000 simulation hours to do it!

The hl 1 net converged in 1200 simulation hours!

I'll started a second hl2 net with slower q learning yesterday. Will have
to wait to see how long that takes.

----

What if I use the links downstream node as it's prediction target instead
of using itself?  What if I use the links upstream node as the prediction
target so all the links in a node are then using the same common target?
That sounds intersting.

----

[12:11 AM]

Using downstream link created large differences in link q values since
it was using different downstream q values for the referece. That did
not look worthwhile.

Using the upstream common node however looked good.  All link q values
in the same node for referece, but yet each node is indepdendent and
is able to create a more acculate value for each.  feels sort of odd
having links use their their own node but I guess that's really not much
different than links using themslves.

But like with using qMean, this made the links seek values that were
closer together which might slow learning?  But it seems hopeful for
getting links to act more as if in percpetion mode with q values really
should be the same.

So seems like the best comprimise currently of the option.  But this
all remains messy and questionable for percpetion learning.

The q values are just alternating back and forth, adn  values swing
wildly trying to counteract q values and b values.  There's no long term
learning at play when the must contanly battle against the RL sytem trying
to flip back and forth.  I need to take a look at a node's output when
the q values are truely all the same and see how it acts.  I don't get
the impression that the percpetion learning systems reallky every get to
do their job as intended here becuase of all the problems RL is causing.

Doesn't feel like this is the right solution yet even though it has some
potential and some rounting problems are being solved by it.

Tomorrow I'll dig more.

Still waiting on the one 11x10 net wth hl2 to converge that has slower
learning.  It should coverge since it's the same as the other one that
converged in two days except the other had faster qLearning. At worse,
this just might take 20 days since learning is 10 times slower? :)  I
won't wait that long however.  Maybe I'll wait enough day before giving
up if it doesn't converge.

Time for bed now.

================================================================================

2016-02-28 2:02 PM Sunday

Yesterday

1) Experimented with using qMean and upstream node and downstreem common
feed node of links as the future predictor instead of using the links
own q value in hopes it would make them more stable and allow links that
are actually equal in value to switch into percpetion mode.  Found that
using the downstream node made it worse.  Using the upsream feed node
made it act about the same as using qMean.  But using the upstream node
allows the links to be more stable inside a node, but frees the nodes
to be independent and reflect more true values.

2) Spent time thinking about back propigation for secondary reinforcement
alternatives some more.  Decided qMean in fact is not the right approach
beuase it causes reward lessons to spread to future actions that can't
be the cause of the rewards.  But still not sure how to make secondary
reinforment work correctly.

3) The long running 11x10 hl2 net finally converged. But letting it
and the hl1 version keep running becuse they are slowly inching higher
and higher in RPS.  Want to see where they top out.

4) the newer started 11x10 hl2 with slower q learning is still running
and still hasn't converged on the answer.

Today

Ingress Sojouruer badge last day today!  360 days of hacking a portal every
day!  This afternoon some time it should arrive.

Slowing down on work here. Testing is getting long and slow.  Progress
forward seems to be slowing down.  I just have to keep pushing forward.

----

[4:00 PM]

Playing with stocastic rewards.  Wow.  Learned something new.

10% at 4, vs 20% at 2, vs 40% at 1 is not all the same as measured by
RPS or by q value!  The higher frequency has values close to the present
in the discount curve and ends up being quite a bit stronger.

10% at 4 is around .452 RPS
20% at 2 is around .523 RPS (or .517 ish)
40% at 1 is around .659 RPS

All with half life of 10.0 and a pulse rate of 1 hz

So the system has a strong bias towards higher frequency rewards even
though the total long term rewards are the same!

Let me double the half life and see how the numbers change.

Half Life of 20: (seems the same as HF 10 -- hum)
10% at 4 is around .451 RPS
20% at 2 is around .521 RPS
40% at 1 is around .659 RPS

Half Life of 1:
10% at 4 is around .449 RPS (q values around .92 for nodes)
20% at 2 is around .517 RPS (q values around 1.5 for nodes)
40% at 1 is around .645 RPS (q values around 2.9 for nodes)

Ok, so half life has very little effect on RPS numbers!  Odd, and
intersting.

But it has large effect on q values and the speed at which the graphs
converge on the answers.  That might be nothing more than the half life
effect of the speed of the average calcuation and not an effect of making
the net take longer to learn.

================================================================================

2016-02-29 10:05 AM Monday

Yesterday

1) Not much.  Tested stocastic rewards and learned the system using
discounts will give more value to high frequency rewards of the same
size than low frequency rewards.  So it's not actually measuring true
total rewards!  I think I need to get a deeper understanding of this.

Today

Time to head out to Starbucks.  And my 360 days of playing Ingres Badge
is done so I don't have to play ingress!  But actually, I want to go
past a full year so I need to play for a few more days before I allow
myself to miss a day.

----

[10:29 AM]

Back from Starbucks run.  I want to verify that the ActivityTrace does
value high frequency over low frequency when the total sum of events is
the same otherwise....

----

Ok, so testing ActivityTrace, I'm finding just the opposit of what I was
seeing in the network.  High frequency inputs have lower values than low
frequency with stronger factors. So rewards of 1, at 10 hz have lower
ActivityTrace values than rewards of 10 at 1 hz.

But, low frequenncy have far wider swings. So where as 1 at 1 hz for
a hl of 1, will swing from 1.3 to 0.6 around 1, rewards of 10 once per
10 seconds swiing up to 6.6 and down to 0.  Frequencies longer than the
half life fade to zero and shoot up way farther.

So high frequencies are more constant, they are lower in q value.

What a low frequency value will look like all depends on when it's
sampled becuase it's both higher and lower than all the others.

Ok, so again, this use of a discount factor is not the same as maximising
the sum of all future rewards.  It puts a bias on more recenet one over
further away ones of the same value.  A small value 1 second in the
future, is better than a file 10 times the size, 10 seconds in the future.

----

OMG, it's worse and more complex than I could imagine.

For short periods less than the half life, the larger value at the slower
frequence is ranked more valueable.

But for long periods of time, the small value is more valuable.

OMG,it's totally out of control.

What's simple to understand, is that the same value reward, is always more
valuable when it happens sooner.  But if the rewards are of different
values, then the difference in time needed to make them equal in value
to the system is a fucking disaster to understand.

But, given two rewards of different sizes, the smaller reward, always
must be closer to us in time, than the larger reward, in order to be
ranked the same value.

The confusion is that the time difference, is not relative to the
reward difference, and becuase of that, the rewards per second, is
never constant.

So, interseting quesiton.  Is there a way to implment ActivityTrace
so that it is rewards per second?  And so that 10 rewards of size 1 is
always the same as one reward of size 10 in the same time frame?  And,
to also, not sum to infinity?

I guess that's the problem.  Any future pulse, can be discounted back
to a point in time using 1/t.  But then a current pulse would have to
be treated as an infinite value.  So we would have to always have a
time period of > 0 to work with and if .0001 was the minimum time then
a value of one would have a "discounted" value of 10000.

Ah, I see the bigger problem.  Activity trace can be used to discount the
sum of discounted pulses.  Many pulses can be combined into one number
and they all discount at the same rate.  But if we tried to sum together
many 1/x values, they couldn't be combined into one number I suspect.
Yes, how much each number fades in a second, is different depending on
the time.

So a number that starts out as 1 at time 1, becomes 1/2, then 1/3, then
1/4 then 1/5 for each second that passes.  But a number that starts out
as 1 at time 2 follows the same decay by a second later.  1 to 1/2 is
a .5 decay.  1/2 to 1/3 is a .6 decay.  I can't decay one number at .5
rate and another at .6 when they are summed together as one value!

To calculate a running average, I would need to sum all the rewards
and keep dividing by the total time since the start so the sum would
suffer the problem of overflow in time.

Plus, it would put equal weight, on a reward of 10,000 that happened once
to rewards of 1 that happened 10,000 times.  But we have very little
evidence for the one large rewward, so we don't know what the true odds
of that reward are, where as we have very strong evidence of the odds
of the small reward.

Ok, so our use of ActivitTrace is first an issue of practicality of
being able to calucate it, and to combine together many avents in one
discounted weight.  And second, it allows us to limit our knowelge to
some measure of recent activity.

Ok, so here's another related thought. The limit of our memory, is also
the limit of our slow speed/long term, actions.  It's a low frequency
limit of our behavior. It's the other side of our dynamic range limits.
There is a cap to how fast we can act, and a cap to how slow we act,
and that is limited by how much memory we have, and how we divide it up.
The high frequency limit is set by the pulse frequeices of data being
injected into the bnetwork.  It can't act any faster than the flow of
data that is the cause of beahviors.  If the only  poulses show up once
per second, it can't produce behavior changes at a rate of 100 per second.

And the slow speed learning of the system, is set by the half life which
decides how the bits of our short term memory is allocated to recording
the past.  To correctly record true rewards per second, we would have
to not decay the memory, and use all our short term memory equally for
recent events as for distant past events.

Ok, so the system's idea of "equal rewards" is not as simple as I
would like.  that's the bottom line.

But the basics hold up.  When the reward size is equal, the one closer in
time is always better.   And when rewards are unqual, the larger one can
always be further in time when it's equal to the smaller one.  But how
far in time they must be spaced to be equal is complex.  And related
to how I resize the value to normalize for 1 reward a second being a
value of 1 (ish), this means that for times less than 1 second, things
distort oddly over 1 and under one in value, and over the half life and
under the half life.

So, time to move on away from the confusion of ActivityTrace once again.

----

[12:40 PM]

Ok, so the last long runing test converged yesterday!  The 11x10 net
with half life of 2 with a slower q learning, converged.  Both the hl
2 large nets converged at around 4000 hours of simulation time.

The hl of 1 converged in 1200 hrs.

Actually the slower q learning (factor of 100) took 4400 hours vs 4000.

So the slower q learning slowed it down a little but not a lot.
Mostly it seemed the half life was the big effect on how long the net
took to converge. But with only three tests, this is not enough smaples
to know if this was just random luck.

But the good thing is that they all did converge and nothing colapsed!

This new two step q learning is clealry far beter than what I was doing
before for these simple problems of traning the net to sort pulses to
a given output.

I need to save those graph images, kill the runs, and then the computer
wants to reboot for updates....  Couldn't do that with the tests running.

----

[1:56 PM]  System rebooted.  Distracted by Facebook and haskel.

Unrelsoved issue is q values calucation using qMean vs feed node.
Using the feed node make the q values inside a node more stable relative
to each other I belive but it might also mess up their values?  Maybe it
doesn't mess up their relative value?

The issue here is the the system's chagne from percpetion mode to RL mode.
What I sense is at work is that I've made RL work so well that there is
no percpetion mode left.  RL is always domanating the system, and even
if two links are equal in theory one link always has a larger q value
and dominates.

I feel there needs to be a much slower ands mooth transaision from
percpetion to RL where the amount of transision is related to how strong
RL is.

Trying to do this in the past I ran in to the issue that we never
know how "strong" a q value difference is becuase q values are always
relative. That is. 1.0001 vs 1.0002 might be as important as 100 vs 200.

The system would identify a bad path as bad q value and good path as good
q value, but then as policy changed and it learned not to use the bad
path, the q value of both paths then converge on nearly identical values
and if that falls below the noise level of the system, it suddenly they
look the same, and the system starts to sort to both, and the downward
race cuses random bhavior until the system restabalizes as the new very
bad behavior, and climbs again. All very bad stuff.

But now with the split values the colapse shouldn't be as bad.

However, I need to do more than just increase the resolution of the
system I need to make it work even when it does fall below noise level.
It must just stop improving at that point not colapse.

So it should sort more and more to the good path, until it can't tell teh
good path is any better than the bad, and stop sorting more to the good --
stick at the ratio where it can tell.  So maybe at at 80 20 sort ratio,
it can no longer tell the good is better so it should just stick at 80
20. But what I do no, is swtich quickly to 100, 0, then when it can no
longer tell bad from good it switches quickly back to 50 50 and cycles
back and forth slowly.

But all my changes might have chagned that.  So let me test this colapse
problem again and see how the system really works.

But what I need to make it do, is is only shift the sort away from 50
50 as long as the evidence is there, and when the evidence is gone,
don't switch back to 50 50, just shift back towards 50 50 slowly until
the evidence returns.  Could this be a problem of having b learning way
too fast? Maybe b learnig just needs to be way slower so as the sort
slowly shift, the system has plenty of time to accumulate new evidence?

Need to set up this test again and test.
 
----

[2:33 PM]

Wow.  What a mess. The OneNodeTest aka colapse test does't work well at
all.  But this is also about that no reward problem.  It jumps and falls.
It picks the none option and -1 over the +1.  Whenever it starts to do
the +1 it quickly turns away thinking some other option is better. It
clearly is not working correctly.

There's something fudamentally wrong about how this works. I'm using a
hl of 1000 to stress it and sure enough, it's pointing out some serous
issues.

Switched from -1 None +1 to -1 0 +1 to see what would happen.
Just sucks. It thinks the -1 is better than the +1. It seems unable to
tell that the +1 is better.  When it starts to use the +1, the q values
start to increase, but the q value for the -1 path increases faster,
make it look better again, causing the system to switch back to the -1,
and colapse. It's not correctly assigning credit where credit is due.

----

Yeah, with shorter half life like 100 and below, the system is able to
lock on the right answer ver quickly.  But for larger half life it will
get confused.  Running 200 half life and e0-8 -3 -4 learning and it's
showing colapse and problems.  But at 100 it was not.

So I need to fix this. If the problem is beyond the resolution of the
system to learn, it should half-learn it and kee the system at a 80/20
sort or something, instead of 100.0.

Also, qMean on or off vs using feed node all make it act very diferently
when it gets confused.  So these things are not important when the q
vlaues are clealr different for short half lfie, but getting this right
for the hard prblem is going to be critical

I feel like these different systems of q and b and w learning are all
in a fight against each other isntead of working toether.  I feel like
that's a fundamental issue with the current design.

I feel I need to find a design where they don't fight. Where maybe q/b
dominates, but where it refuses to be "too pushy" if the q values don't
warrant it.  But how do we tell that????

----

I have to wonder as well if my use of /rpolicy is the problem here.
Not the use of it but the fact that due ot how forceExplortion is used,
the rplolicy might not be a perfect measure of what the system is
actually trying to do at any moment and that small difference betwen
what's really happening and what rpolicy implies is happening could be
why rewards are being falsly applied too much to the wrong links?

I feel like we need something simpler here as how we assign credit to
links relative to use.

----

Slowed down b learning from -4 to -6 and it's looking like it's more
behaved at the moment. But struggling to learn.  But at least not fighting
the truth.  The slow b is allowing the system to accumulate more truth
in the q values.

But b is 0 0 99, w is 51 48 0, and p is like 32 33 33 -- so w is
dominating b learning.  But the flow is like 2 2 94 because of the *a
effect of sucking all the data to the dominate side once it start to
"win".

This is all a problem in my eyes. Once RL says 0 0 99, the w and *a
stuff should not have the power to overide it like this.  w really
shouldn't be allowed to fight RL. It whould do what RL tells it to do.
Right, but this gets back to the problem that W is attempting to blance
output nodes blance and RL is controlling link flow balance. But w is
controlling link for for the purpose of trying to blance outputs.

I guess what might be needed is that the more RL trys to shift flow,
the less power w should have to override it?  The system must shift more
gracefully from percpetion to RL instead of the two systems fighting
each other so much.  And RL should be slow and careful about using it's
power to shift.

This test with slow b learning, nosilly wondered from .1 RPS range to
.7 or so, then it locked in to the right answer and had and smooth ride
up to 1.0.  I assume that's the point RL dominated over w?

The q values are like .0001 for the two bad paths and .0004 for the
good path it's using.  That's very large compared to what I was seeing
at other times.  I need to let this run and converge and see what the
q values end up as.  I've turned qLearning off but the links are using
the feed node for reference now.

Slow b learning just gives q learning more time to accumuate true data.
That's no better than just using slower q learning I don't think.
The problem here is that the system doesn't operate in a resonable way
when the signal falls below the noise level or when the signal is near
to the nose level.  Rl should not take control unless there's is clear
evidence that the signal indicates so and not be confused by false
signals when the true signal is hidden in the noise.

The whole problem here is I don't know how to make the system tell the
difference between a false signal that looks real, and a true signal.
Or, I fear that my system is generating false signals due to not being
precisely adjusted between how policy adjustment works and sorting works
or something like that?

Basically, if there's strong tendency emerging in the q values, the
system should use it.  And the only strong tendency that should be able
to emerge, is the truth. In theory.

Ok, I just have to keep studying what this is actually doing.

But it desperatly needs to be simpler.  What if I turn w and *a behavior
off, and see how easy hard it is to get rl alone to work cleanly in
these high noise low signal conditions?

If Rl along is not working, and showing colapse behavior etc then that
lets me know this is not a problem of the two systems interacting but
something wrong with RL itself?

I think I need a pizza break....

Another thoughts -- on learning speed and links, if I made small
q values rise faster, and sink slower, and high q values rise
slowly and sink faster, it would cause the system to try and
keep the q values the same, and only strong evidence against that
would push them apart.

This idea would be the same sort of thing as adding w^2 to an
error term I guess.

Save a snapshot of q values so when I get back I can tell how
they have drifted.

-------------------------------------------------------------------------------------------------
XNET Mon Feb 29 15:32:08 2016  RunTime: 0:32:09

OneNodeTest rewards: [-1 0.0 1] 3 x 2  time: 900:13:17.9513  PPS  1137  RPS 0.969 1.005 
Explore: 1.00%  Q: 1e-06  B: 1e-05  W: 1e-04  halfLife: 200.0  fanOut: 3
  .         |  .o|      4 =
 10o009.990.|  .o|      5 =
  .         | 10o|    602 ================

Node [1][0] 0.97299 0.99844
 at:   0.00614151        0.00816105        0.98413318     
  q:   0.00001849        0.00002614        0.00012422      
  b:   0.00000000   0    0.00000000   0    1.00000000  99 
  w:   1.00000000  49    1.00000000  49    0.00001000   0 
  p:   0.00000000   0    0.00000000   0    0.00000323  99 
 rp:   0.00970874   0    0.00970874   0    0.98058252  98 
  a:   0.01570144   1    0.01767205   1    0.96662651  96 
 pa:   0.00000000   0    0.00000000   0    1.00000000  99 
--------------
avgRewards      1.0001
avgRPS          1.0347   1.0052
qMean:          0.9729   0.9654   Happy!

reward:         1.0004071427
-------------------------------------------------------------------------------------------------
[5:34 PM] back from pizza
-------------------------------------------------------------------------------------------------
XNET Mon Feb 29 15:32:08 2016  RunTime: 2:01:45

OneNodeTest rewards: [-1 0.0 1] 3 x 2  time: 2068:17:07.9022  PPS  643  RPS 0.968 1.006 
Explore: 1.00%  Q: 1e-06  B: 1e-05  W: 1e-04  halfLife: 200.0  fanOut: 3
  .         |  .o|      4 =
 10o009.990.|  0o|      5 =
  .         | 10o|    304 ===============

Node [1][0] 0.97321 0.99861
 at:   0.00872770        0.01372354        0.97615424     
  q:  -0.00002276       -0.00001090        0.00000233      
  b:   0.00000000   0    0.00000000   0    1.00000000 100 
  w:   1.00000000  49    1.00000000  49    0.00001000   0 
  p:   0.00000000   0    0.00000000   0    0.00000323 100 
 rp:   0.00970874   0    0.00970874   0    0.98058252  98 
  a:   0.01822352   1    0.02309749   2    0.95867898  95 
 pa:   0.00000000   0    0.00000000   0    1.00000000 100 
--------------
avgRewards      0.9960
avgRPS          1.0305   1.0055
qMean:          0.9724   0.9666   Happy!

reward:         1.0000316065
-------------------------------------------------------------------------------------------------

[7:03 PM]

Ok, turned off w, distanceBias and *a and I'm justing using b along
with forcedExplore as the policy to control sort ratio.  AKA, RL onlye.
And this way I know for sure that the policy is exactly what is used for
sorting, and the q learning link update of a/policy to adjust speed is
perfectly matched to what the system is doing.

Still, even with this, the system is able to get stuck on bad choices
and show a few colapse type problem.

If b learning is too fast, it really has problems getting stuck on
bad choices.

I don't feel it's safe to assume slow b learning will fix this. I fear
that it's dependent on the nature of the problem and that slow b learning
might fix it for one, not not be slow enough for another and the net
ends up being highly unstable.  It feels like slowing down b learning is
just cheating to help the system deal with q learning that's too fast
and it's not solving the underlying problem of the system failing to
spread activity across links when q values are close together.  That is
failing. to blance activity, when q values are are just random noise.

I like the idea of modifying q learning so as to push low q values up
and high q values down so that they will tend to stay together unless
there is clear bias pushing them apart.  Let me try to figure out how I
might do that.  Then the system should stay stable even when q learning
and b learning are too fast.  It should just stay in the scatter mode
sending lots of pulses out all paths in that case, and only converge on
an unblanced sort if the rewareds really show a clear statistical trend
strong enough to offset the "hold-together" bias.

Ah, because of the use of relative link values, the solution should
be to push links back to zero!  That's straight forward and clean.

----

[12:46 AM]

Played with this version.  Works fairly nicely.  Made hl 1000 work.

But, it still suffers from not being able to blance across nodes that
should be equal. The code pulling q  values to zero helps keep them
close, but someone it feels as if the one being used, manages to keep
it's value larger than the others.  Though it's hard to know for sure
what is happening.  I need to set up a grah to grap the 3 q values over
time so I can better see what they are doing, and if hey are changing
or if one gets stuck in the lead and stays in the lead.

And if the link with the most activity is somehow being bised to the
high position, even after the net equalizes, I need to figure out how
the hell that is happening and why we are not seeing random walking on
all three link q values.

I also tried adding a simple random adjustment to all q values after
each aupdate.  That doesn't seem to do it either.  Maybe it needs to
be larger????

I clearly need to figure out what's at work here.

Tomorrow...

================================================================================

2016-03-01 12:35 PM Super Tuesday

Yesterday

1) Studied the complexity of ActivityTrace yet again.  Two rewards of
different sizes have very odd time difference need to make them equal
for the Activity Trace.  Mind blowing to try and comprehend.  It's not
anything like rewards per second since the time ratio can be greater or
less than a true measure of reward/time depending on frequence and half
life etc.

2) The last long running convergence test converged for 11x10 net with
halfLife of 2.

3) Looking at problems of net getting stuck on bad qValues and pushing
pulses the wrong way for extended periods. Try slower b learning to
address the issue -- can help but isn't really a fix.

4) looking at related issue of net not blaning output between links
that should be equal in value -- it likes to pick one and stick to it
for extended times.

5) Turned perception larning systems off to see how RL alone functions
in reguard to 3) and 4).  Turned off w, *a and distanceBias.

6) Added slow seek towards zero for link qValues to see how that helps
keep them together and address 3) and 4).

7) Added random noise to q values to see what that does.

8) Created PlotLinkQ class for plotting 3 q values over time so I can
better understand how q values change.

9) Found out about AGI conference in New York in July.  I should try to
go to that.

Today

[12:44 PM]

Super Tuesday!

Voted in the primaries for Bernie this morning!  Already went out and did
Starbucks and a bit of Ingress as well.  Julie will be coming home later
today for Spring break.  Comming home a day early and surprising Pig.

This balance problem is perplexing.  I'm not comming up with any insights
yet that really solve the problem.  The trouble is trying to mechanically
identify when differences in q values are valid vs when they are just
random noise.  So far, my attempts to make q values stay together are not
resolving the issue.  Even when they are closer, they don't tend to flap
around very quikcly form the random noise, so a false postive can stay
in the lead for such an extended time that slow b learning doesn't fix it.

For example, in this two node tests, one node has a real problem to
solve and the other is not rewarded and should be random.  The real
problem is a +1 None None reward and the other node has no rewards.

But the real node has link q values of .00008 vs -.00037

And the node with no problem has .000028 vs -.000017.  How can we tell
one is a good sold q value and the other is noise?

Wait, q is .42 and RPS is .450.  I don't understand why it's not year one?
I think the frequency is 1?

Oh, right. It's a 40% chance of a +1 reward.

----

Ok, Let me plot graphs for both the real q values and the alse.

----

Well, that was useful.  Looking at the graphs the "good" q value stands
out like a sore thumb.

But how to correctly identify the a good q value from fakes?

Time is one clue. The good q value has remained in the lead for the entire
run and never switched places.  The bad options are all swtiching places,
even if the time is slow.  The bad might hold a lead for 1000 simulation
hours in this test, before it swtiches.  But the good has held the lead
for 6000 simluation hours and will continue to hold the lead forever in
this example.

So, one option would be to use a very slow b learning rate that woudl
require it to hold the lead for 1000's of simlation hours.  But setting
that value correctly would be a chore.

Another option would be having a window of seperation were the good
value has to be a certain distance away from the mean, or the worse
before it should be considereed worthy of higher sort rates?

But again, this becomes a real problem to know how to correctly pick
that value.

In this test, the good q value is .0003 ahead of the bad.  And the equal
values are about .00005 apart from each other.  So it's a distance of 30
vs 5.  Makes for an obvious picture on the graph but not such an obvious
number to pick out of thin air.

Noise might be the answer.  I've thought of using variance in the past
as the key for identifying the quality of a signal relative to others.
It was how I was trying to solve this problem before by plotting all
the q values on a log scale.

This sounds like the right approach.  But how to best measure signal
noise?  I could track varriance for each q value, but that's a lot
of extra memory.  I coult track varriance for the node's q value, but
since the nodes q value is being updated with a larger learning speed
it's going to have a totally different noise level then the link
q values.

So, let me add a variable to the node, to track the link q value update
noise - for all links at the same time and see what sort of number
that produces.

----

[3:16 PM]

Added linkQMaxAvg to nodes which tracks the avg q value of the max link.

Added linkQMaxVar to nodes to track the variance away from the mean of the
max q value.  Both are usin glong time averages 1e5 halfLife currently.

So I'm only tacking the max q value.

This gives me a standard deviation noise measure of the max q value.

I can then mesure how far each q value is away from the max, in terms
of number of standard deviations.

The good q value is like 15 SD away from the others.

The equal q values on the other node are like .1 and .2 away from each
other at the momemnt but were as high as 2.5 sd and the good q value
was as high as 70 sd away from the other two. But that's when I had the
randome noise injuector turned on.  I've deleted that now.  No point
to it.

I also have the seek to zero feature active as well.

As the net stablizes on an answer, the noise level will fall and sd
measures will likely rise.

I'm thinking now the correct way to cacluate flow volume ratios is
straight from the current sd measures.  Drop the current idea of b
learning.  The node in the lead will have a volume of 1, and the others
will have lesser values relative to how many SD they are below the max.
Then normlize the numbers to get the actual precent ratios.

Hey, and if this is an expoential sort of function mapping SD measures
to flow volume, it would never need to reach zero, guranteeing some
small level of flow to all nodes all the time.  And eliminate the need
for the b learning speed variable!  But it's been replaced with a half
life for the average. But that should be larger than the system's halfe
like by some large measure?

Oh, wow, now the good node is now 200 SD above the other two!  And the
three equal nodes, are bouncing around, but in the .5 to 2.5 sd range.

But as the net stabalizes, and Sd falls, I'm seeing the equal q values
go as high as 13 SD!  But the good one is 200+ SD!

And now the equal ones are back to the .3 and 3 SD range.  We might
need a SD to ratio as high as 10 SD for 50% difference?  Or maybe
it would be good to let the equal weights flap around a bit more
as a form of exploration?

----

[6:04 PM]

Had to run to Town of Vienna town hall to drop off business licenses.
Had Pizza after that.

But got SD based sort ratio coded and working before leaving.

Looking nice!  Used 5.0 SD per 50% drop in flow volume.  That seems like
not enough, so I started a second one with 10%.  Turneed off the seek to
zero feature, and I'm testing using the link as it's own future referece
to see how a raw q values without those helper features will deal with
the new system.

With an 1e-5 (I think) sort of half life on the varriance, the system
ossolcated a bit becuase it started to learn very quickly, but that caused
the q values to shift ver quickly, and that quick shift, then caused SD
to slowly rise, and then push the system back to less strong sorting
(hey, things I changing too fast, better slow down sort of effect).
Reduced halfLife to 1e-3 to test. That seems better and more stable.
But still testing.  Darn system locks on to valid answer very quickly
now but this is of course is a simple problem.

This is exciting suddenly again. My heart is pounding. But I fear there's
something important I'm failing to grasp at work here that will bite
me on the harder tests.

I'm thinking the push towards zero feature is actually good and important
to create more stablity.  But more testing is needed.  The equal value
q nodes are drifting too far apart and becoming unblanaced.  They are
at 72 SD spacing now for no good reason!

The good q value is at a crazy 6000 SD spacing.

----

[11:50 PM]

new Sd based RL sorting is working very well.  But it has issues to work out.

On simple test, it couldn't seem to lock onto sort all to 4 solution.  Then
I realized I had explore set to 5% with 10 outputs which was messing it up
from solving the problem.  Reducing back to 1% and it worked fine.

OneNode test was havin gissues s well.  Then I realized on it I had halfLife
set to 1000!  Slowing down q learning to e-8 fixed that.

Also adjusted the half life of linkMaxQVar to 100* half life of the net. It
should be slower than the rest of the net I believe.   Maybe 10x is all that
is needed.  I have to explore.

One OneNode with halfLife at 100 it was working and very slowly inching it's
way up to 1.0 RPS, then as it approached, the q values did their turn and
dive back to zero -- but the dive was too fast, SD soared, causing sorting
to go back to 50/50/50 causing a colapse way down to negative q values.  Now
it's claimbing back again.  I think there's a good chance it might sovle it
but it might have to bounce back and forth?  I'll have to let it run.

Don't know for sur ehow to solve this.  Slower q learning will probabky help.
But I'd really like it to learn faster without colapsing like that.

All in all however, these problems are minor compred to what it was doing
before.

Most important, all the equal inputs stay nicely balanced in use!
This is looking very nice.  But it's going to ake more testing to
undrestand the colapse beahvior and see what it takes to prevent it.

The q values rise as the node q values seeks a new high, but as the
node appraoches it, then the link q values take a fall back to zero.
That turn is measured as an increase in the noise (which it is not),
which causes the net to start blancing more, which causes rewards to
crash, which causes the q values to crash even harder, which is a postive
feedback loop that leads to colapse.

Tried switching to use node q + link q to remove that fake noise from
the system.

================================================================================

2016-03-02 8:34 AM Wednesday

Yesterday

1) Changed b learning to use noise based logic!  Nodes track the
average and varriance of the max q value in the fan-out.  Flow volume
is calcuated by the number of Standard Deviations between q values.
Max Q gets a number of 1.0, and for each 10 SD, the volume is cut 50%
in an exponential fashion.  So .5 for 10 Sd, .25 for 20 Sd, .125 for 30
SD etc.  So b values are calculated directly from q values and the SD
of the system.

2) Set halfLife of ActivityAvg used to track linkMaxQVar to 100* halfLife
of the network.  Maybe 10x will work?

3) Lots of testing with the new SD based b learning code.  Looking very
nice.  THere are some iissues and beahviors of colapse or failure to
converge to be fine tuned and understood, but mostly so far, those
problems look like just an issue of getting paramaters set correctly.

4) The new SD based b code solves the need for the system to distribute
flow evenly on inputs that are equal in value by defining how close
they need to be together to be close to equal in flow (less than 10 SD).
Looking like it has a good transistion from prevpetion to RL.

But, all this was done with perception turned off.  I'll have to figure
out the best way to make percpetion work with RL again.

5) At the end of the night, switched to measuring the noise in the
node+link becuase the combined set there doesn't get more noisy on the
link turn and drive, it gets less.  But this has far more noise due to
the far larger learning noise in the links, ugh that's not right.

Today

Thinking last night. I realized the problem.  I should be looking at how
the link q values are changing reltiave to each other, and measure the
noise in that!  When they take that dive, they are all moving in unison
so they don't risk crosing each other.

I'm thinking the way to to this, is use the difference between the highest
q value, and the average of the others.  Was thinking maybe doing it
between the two two, and that might work, but the top two might be equal
answers that keep crossing each other, and a bottom one might be highly
noisy from being a very dangerous option or something and we need to
know the noise relative to that as well.

Also, my tests last nigt attempting to fix the colapse prolem all failed.

So lets try coding this new idea.

----

[10:22 PM]

Coded it. Played a bit.  Started some tests. Went to Starbucks run and
played a bit of ingress and now I'm back.

The hard 1000 half life test is struggling.  But q learning is e-6.
Probably needs to be smaller to make it work?

Smaller half life is working real well.

But, had to set the SD ratio to 50 SD for 50% drop.  With this new way
of measuring noise between links, the noise levels are very small so it
tends to allow large changes in routing in response to small changes in
distance between q values.  Creates odd and bad oscillations on hard
problemls where it starts to climb, theb backs off and dips a bit,
then recovers and starts to climb again, etc.  Would like it to act
a little better for hard problems.  But I'll have to test more and see
what happens.

On easy problems, this thing is amazing!  See the +1 None +1 problem
below where it not only solved it instantly, it does a killer job
balancing between the two +1 options!

-------------------------------------------------------------------------------------------------
XNET Wed Mar  2 10:18:50 2016  RunTime: 0:03:04

OneNodeTest rewards: [1 None 1] 3 x 2  time: 129:09:46.8607  PPS  2195  RPS 0.975 1.025 
Explore: 1.00%  Q: 1e-05  B: 1e-05  W: 1e-04  halfLife: 10.0  fanOut: 3
  .         |  3o|    531 ============
 10o505.090.|  .o|     14 =
  .         |  6o|    562 =============

Node [1][0] 1.02356 0.97472   LinkSD 0.0000004283
 at:   0.32545833        0.00035861        0.64890567     
  q:   0.00001496  2.8  -0.00142027 3354.0   0.00001614      
  b:   0.49046011  49    0.00000000   0    0.50953989  50 
  w:   0.00001310   0    1.00000000  99    0.00001314   0 
  p:   0.48582838  48    0.01000000   1    0.50417162  50 
 rp:   0.48582838  48    0.01000000   1    0.50417162  50 
  a:   0.28746601  28    0.01009157   1    0.70244242  70 
 pa:   0.28276142  28    0.00020432   0    0.71703426  71 
--------------
avgRewards      1.0005
avgRPS          1.0348   1.0251
qMean:          1.0242   1.0210   Happy!

reward:         1.0007350066
-------------------------------------------------------------------------------------------------

And notice it has no problem undrestanding that "None" is not as good
as as +1 rewards.

I was confused in the past thinking the None option gave it no rewardds
to pick from.

But since it's option is to pick rewards, vs no rewards, it knows the
rewards are better than not getting rewards!  Sorting to the "BNone" option
reduces the frequency of the rewards from the good option over time and
that's seen by the system as an obvious bad thing!

This new approach is a killer solution compared to what I was doing in the
past!

So far, the only issue is getting it's behavior a little better when it
reaches the noise limit of the system where it can't tell if more sorting
is better or not due to there being too much noise. But maybe, the behavior
doesn't need to be beter when there's too much noise????

More testing needed...

----

[11:43 AM]

Testing going well.

Tested [-2 None -1] to see negative learning ability.  Generally works
well but this test pointed out a problem.  Because None was the best
answer, the more it selected None, the less rewards it was getting to
be able to accurately measure the reuslts of it's choice.

So None doesn't need to be rewarded when it's a bad option, becuase
the good option gets rewarded and measured.  But when doing nothing is
the best option the system stops getting rewrds, so q values don't get
calucated without rewards.

[-2 0 1] is easy -- it picks 0.

So, I added a 1% random use of reward(0) and that was an easy fix.
This way the system is always getting a low level of rewards so that
doinging "nothing" is still be rewarded and q values updated.

Without the help of the random reward(0) it seemed to actually be learning
fairly well. It learned that -2 was far worse than -1, so it had shut
down -2 to 0.  And it know none was better, but it could only shut down
the -1 to about 10% flow and it was bouncing around a lot without the
help to measure the value of doing None.  It was headed in the right
direction however and might have solved it just fine but it would have
been very very slow beucase the close it got, the less rewards it got
to verify the choice.

did tests like [-1 -2 -1] and it worked very well to balance the outputs
across the two -1 option.  Negate learning seems to hvae no issues and
works just the same as postive learning -- so having q values sink in
value to find their correct value doesn't seem to be causing any issues.

----

The only issue remaning is how the system acts when there's too much
noise for it to be able to solve the problem.  Such as when q larning
is too fast.  So when trying to solve hl1000 problem, q learning has to
be very very small for it to not tripping over itself.

But also shows up with hl 100 if q learning speed is e-5 for the onenode
[-1 None 1] problem.  In that case it gets to about a 0 25 75 range sort,
and can't do any better.  That's with the 1% random 0 rewards as well.

The rewards are up around the .75 range but it's doing this scallop
pattern of dropping, then rising, then the q values collapse and RPS
starts another scallop.

It would be nice if it were more stable and didn't oscillate as it hits
the limit of the noise.  But I guess, this sort of pattern also makes for
a good indication q elarning is too fast for the problem? But it seems
important for learnign in general that the system doesn't itself become a
noise generator that makes it harder for the rest of the system to learn?

----

[4:04 PM]

Back from Pizza.

Letting tests run to see how they converge.

The long running cases get slow after a time.  Must be too much data in
the graphing array.  I'm going to look at auto-thining the data.

----

[6:56 PM]

Playing with plot code.  Added some code to prune data points.  But it
needs improving.  Just deletes very other data point when it hits 10,000
(cutting back to 5000).  But this means the beginning data is thined
and deleted in time losing all record of the inital shape of the graph.

Added code to allow window to be closed without killing application.
Saves the overhead of updating once you close it.

----

[11:23 PM]

Went out for movie tickets and Ingress.

Found I had gap set wrong for plot on OneNode test causing it to collect
way too much data and slow down processing from 20000 pps to 300!
Wow it works so much better when I fixed that!

Rewrote the prune data code to prune time evenly and create a map of what
data points were deleted so it keeps the time spaced evenenly acorss
the map.  This should fix the problem of the old system throwing away
all the starting data and making the graph a joke.

----

The halfLife 1000 test turned out not to be stuck!  Once It wasn't taking
12 hours to run, but 30 minutes, I saw it converged just fine all the
way to 1.0 but it looks like it had a few colapse events after that as
q valus were adjusting but due to my prune data making a joke of the
graph I'm not sure what really happaned.  Running it again now with the
few fancy prune code.

----

[12:18 AM]

Wow, just started a Big net for the first time with this new code.  It was
an odd 20x8 (vs 11x10) hl 2, e-8 q learning.  This new code solved it in
30 minutes!  I don't think that net had been solved by anyone before.
It's not spreading activity evenly for P2 and P3 yeat.  Will need to
give it mor etime and let q values settle and see what happens. Will it
colapse?  Get confused?  Waiting to see!

Now I've started a 11x10 to compare to what others have done in the past.

BTW -- the other runs are going nicely with the new data pruning code
so far.

And all this is with RL only -- no percpetion learning turned on at the
time.  RL is doing a great job of spreading activity though the whole net.

================================================================================

2016-03-03 9:37 AM Thursday

Yesterday

1) Still playing with RL only SD noise system for automatic explore
exploit control.

2) Swtiched to measuring the difference between the largest q value of a
link and the mean of the others as the signal source for noise detection.
Had to increase SD to 50 with this measure of noise.

3) System works very well on all the easy problems.  Very fast, and
effecent.  Converges very quickly.

4) Looking more closely at hard problems like hl=100 or when q is
too fast and creates too much noise for the system to see an answer.
Behavior is odd.  Rising scalop start and stop behavior.  But the
bad problem is that hours later, as link q values try to get back to
zero after a run up durring learning, the lead gets too close to the
second becuase it seems to be dropping to zero faster, and that causes
colapse, the system may have to colapse and realine a few times but in
the end solves the problem. But it's odd and wrong that it solves it,
and then during a period of "readjustment" the thing ends up colapsing
and forgetting all that it knew.

5) Improved graphing.  Write pruning code that didn't work well,  Replaced
with cool fancy time sample pruning that is working nice for preventing
long runing graphs from slowing to a crawl due to exessive data build up!

6) Made it possible to close graphs without killing app.  Removes the
ovehead of the graph once closed.

7) Found that a few tests were running slow not just because of too much
data, but because the gap was too small and it was calling the graph way
too offten.  OneNode that runs at 20,000 pps was slowing down to 500 pps
with large data sets and too many calls.  Much much faster after fix that!

Today

OneNode hl 1000 test wth e-8 learning looked like it wasn't going to
solve it, but it turns out that given time it did.  But once solved and q
values head back to zero, it trips over itself and colapse mutiple times
due to the lead q falling faster than the unused q value and getting
too close to them.

I'm looking at alternatives to make that trip behavior go away.
Trying removing the pull to zero logic.  Maybe it's not really needed
or all that good?  Part of the goodness howveris to help heard "equal"
q values together.  So if I adjust to make unqual stay apart on this end,
it could break the hearding of equal q values together on the other.

But worth expermenting with.

----

[11:59 AM]

Went out for Starbucks. Brought Bagle BEC back for Julie and myself.

Got 5 experiments running.  The OneNode 1000 hf with learning of e-7 in
two tests. One with pull to zero off, the other with pull to zerro at
1/10 of q learning.

Both solved the problem but are suffering constant short term colapses
as the q values seek back to zero.  Looks like they both will solve it
and I'm not seeing much difference in the colapse behavior between
them with different pull to zero effects.

Also have a e-8 running with .1 pull to zero.  Curve looking the same
as the others before for e-8.  No colapse yet but scaloped effect on
RPS rise to top.  Reached top, now q values are heading back down and
soon this is when we expect the colapse.

Another is an old run from yesterday of the same e-8 hl 1000 run.
It solved it, tripped a few times on the way down, than had a massive
colapse near what looked like the end as the values got too close
together, but on the recoery from negative q values the spacing became
large and now it's converging on zero but the spacing between q values
is very very very small.  It's not copalsing beause one is converting
up from negative and the other two are converging down from postive
making the long running average highly stable in the middle allowing SD
to become very very small.

But what I'm seeing here is that the fundamental problem is that for this
hl 1000 problem, the q values need to be amazingly close together and as
they drift together from far apart, it creates too high a SD because they
are closing towards each other too fast, even though in truth, they are
likely on a convergence path not a crossing path.  The SD code that looks
at how the distance between them is changing can't tell the difference.

----

[1:29 PM]

Wasting time on facebook waiting for these danm tests to progress...

The long running OneNode [-1 None 1] hl 1000 test with learning of e-8
is now converging on whaty looks like the final answer.  But the q values
are amazingly close toether, and still getting closer!

The spacing is now about 1e-7 between the best and the second best.
No wonder it even has problems at 1e-8 learning speed!

The 1e-7 speed tests seem to be headed to the a steady and correct ansewr
but it's produces a lot colapses in the process trying to get there.

I need to try a 1e-9 learning rate and see if it can sovle it without
colapsing.  But boy is that gong to have to run for a long time.

I getting the feeling that there might not be a solution here.  If the
only learning problem there is to solve, is below the noise limit the
system will struggle and act oddly trying to solve it.

But, if the system is given mixed learning problems to solve, it should
be able to solve the easy ones,  and stay stable due to the fact that
the easy problems where solved.

I should test this, with maybe a [-1 +1 +1.01] sort of problem and see
if it can learn to tell the two +1 values apart?  And if not, does it
act strangly, or does it just blance the behavior between the two.

Leaning not to use the -1, is the "easy" problem, and picking between
the two close together is the "hard" problem. But I bet in this case
there is no odd signs of stablity at work and nothing to wory about.

In real worl problems, we ware liekly going to give the system easy
problems to solve, as well as harder problems -- so as long as there
is an easy problem dominating behavior, the hard ones should not create
this chotic almost solved, then cloapses then almost solve sort of cycle.

Sonds like a plan. But I need to wait for current 5 jobs to finish before
starting more.

----

[2:23 PM]

Killed the two e-7 hl100 tests.  They were oscillating and colapsing
and I don't think ththere were every going to sotp because there was
just too much learning noise with e-7 for that problem.  They had
basically sovled the problem, but keept falling apart as the q values
got too close together.

----

[3:14 PM]

Sent email to Mount Vernon earlier today.  Saying I don't have time
to work.

Just got back from Pizza.

The e-8 hl 1000 test I killed.  It has .1 slower pull to zero and seems
to be doing worse in terms of how much it's colapsing as it decends zero.

The e-7 tests of the same I gave up on. they were tripping constantly
I'm I'm sure they would just keep that up based on other tests even
though they would basicall solve it.

the older e-8 test which had solved it I killed.  The distance between the
good +1 q and bad "None" had dropped down to about 1e-7 and seemd like
it might be stable there.  Very small spacing but becues it's was dead
flat relative to the other q values the sd was even smaller so it was
all dead stable.  But it took a lot of triping and reseting to get there.

Still have the e-9 test running to see how that does.

Still have the 11x10 hl 2 test running.  it has not yet converged but
others of that size took longer so I'm stlil waiting.

Started a 11x10 hl 0.1 and it solved it in under an hour.  So that
conferms this new Sd based RL code can solve that problem for the simple
micro small HL.  It's also doing a half decent job of sending pulses to
all the other outputs that should be equal as well.  Good sign.

Want to test the idea from above of a -1 1 1.01 sort of test to see how
the network acts now.  WTF movie at 5:15 I'll have to leave for soon.

----

[4:40 PM]

Well, [-1, 0, 1] hl 10, solves quickly and fine.

[-1, 1, 1] solves correctly.

[-1, 1, 1.1] solves correctly quickly.

[1, 1, 1.001] is solving correctly.  Had to wait for hump to go back to
zero and for system to stabalize.

[1, 1.001, 1.001] works fine.

[+1.0, +1.00001, +1.000000001] trying this.  Leaving for movie now.

----

[7:49 PM]

Back from movie.

The [+1.0, +1.00001, +1.000000001] sovled corretly by picking the
middle one.  Stupid me I was thinking the last was the correct answer!
It looks like maybe the .00001 is nearing the limit of what it can solve
with the e-6 learning rate and hl 10.

The bad choices are only 200 to 600 Sd down from the good, and it's
jumping around a lot on each display (+- 200 change on each display).
That's why I think it might be near the limit of detection.

Let me try 1, 1, and 1.000001.

Tried 1, 1, and 1.00001. (e-5) first.  took 10 minutes to converge and
setal enough to pick up the difference.  The higher q was like 2e-7 away
from the next.  Very tight spacing for e-6 learning speed.  Trying the
e-6 now.

The net is prefectl finely behaved.  All it does is use both good options
when they are too close to tell apart instead of one.

The colapse problem happens when the to "good options" are far apart,
like 0 and 1, but yet due to too long a hl, they can't be told apart
by q value without supper slow learning and the system jumps randomly
between 0 and 1.

I real life problems, it's not likely this would every be an issue
because the sort path of any single node is not going to make such a
significant difference.  I'm thinking.

I'm thinking I can ignore all of this -- ane concentrate on making the
other end of the probnlem work -- that q vlaues that are equal do a good
job of sharing volume.

I think I'm almost ready to wrap this exploring of RL only up and move
on to adding percpetion back into the system again. Though there are
a few more tests I've not turned on and tested yet.

The 1.000001 test sort of seems to have sort of converged on the answer.
But the q values are within 1e-8 of each other.

Learned a new trick!  With two graphs, you can pop up the Save dialog
box and freeze the app, but the other window responds to events and I
can take my time to zoom in and look at the graph in detail!  That makes
it real easy to check how it's converged!

On the above e-6 problem the red line was on the top all the time, but the
two others were bouncing up and down in it's "wake" so to say just below
it and trying to cross it at times.  So it could be seen from the graph
that the red was better, but enough to make it a clear winner.  I forgot
to check the SD numbers but the sort was not 98 1 1, it was more like
70 10 20 sort of range. But the winning q value was getting most the sort.

I had pull to center at q*0.1 while testing this.  Putting it back to
just q and running the same test again to see how it converges and if
it makes it any better or worse.  I'd rather leave it at just q vs q*0.1.

----

[10:27 PM]

Ok, I have pull to center set at qLearningSpeed again. Seems fine.

And SD cranked up to 100 now (was 50) so each 100 SD seperation
creates 50% drop in sorting ratio.

Learning 1, 1, 1.000001  at e-6 doesn't work but keeps fairly well
balanced with the above paramaters.  That's an e-6 problem with 3-6
learing.  Doesn't work!

But 1, 1, 1.00001 which is e-5 problem is learned, but the sort is sort
of in the 10 10 80 range but bounces a good bit.  So the net can "see"
the difference but it not lock it in at 100%.

1, 1, 1.00002 is better, but maybe 5 5 90 range of sorting. It's about
another 100 sd further apart so it's 50% less sort on the down side.

Let me try 1, 1, 1.0001 which is e-4 problem with e-6 learning speed.
That should lock in pretty well.

And also, a 1 1 1 probelm to see how well equals balance!

----

Ok, so 1 1 1 is staying within about 100 SD of each other for equals
meaning the sort difference is 50% or better on average.

The 1.0001 e-4 is around 1000 Sd spread between good and bad links making
the target about 10 powers of 2 or about 1/1024 so better than 1 1 98
most the time -- so a mostly hard lock when exploration is 1%.

So, with these parameters, q learning needs to be about 2 powers of 10
below the size of the learning problem to get a had lock on the sort.
So an e-4 learning problem needs e-6 q learning for a hard lock, or e-5
for a soft lock, and it won't really see the solution for e-4 learning.

This all looks ok to me. I think I'll keep it.

Oh, wait, and all that is with halfe life of 10. With halfe life of 1
it should do better.  I wonder how much ebtter?  10 times better?

Damn now I have to run more tests.

----

Tested. Yes, switching from hl 10 to hl 1 improved ablity to solve
problems by a factor of about 10.  So the e-5 problem that was soft
solved with e-6 learning at hl 10, was hard solved at hl 1.  SD spread
was sort of 1500 is range with a lot of bounceing.

The e-4 problem was double hard solved :) with e-6 learning at hl 1 SD
spread was in the 10,000 range from good to bad.

Now trying solving e-6 with e-6 learning.  Should get a soft ish solve
for it.

Well, a little worse than "soft".  The correct link is getting more
pulses in the long run but other links sometimes take the lead so the
correct link is staying higher, but is mixed up in the other q values
from time to time -- so not really a solution,b ut the long term averge
of pulses might be 50% higher for the correct link than the others.

So, yes, cutting half life from 10 to 1 improved the resoution of the
system by a little better than 10x.

Ok, this seems enough for the OneNode test.  Tomrrow I'll move one
to more testing with this configuation and see if I really like it
enough or not.

================================================================================

2016-03-04 11:03 AM Friday

Yesterday

1) Still experimenting with SD basded RL which used noise levels of
q spacing to set sorting ratios in the explore exploit problem.

2) The hl 1000 tests all have problems with colapse but the generally
seek towards the answer anyway.

3) Tried removing seek-to-zero and making it slower.  Neither had any
real effect on the colapse problems with the hl 1000 tests.

4) Decided that this colapse behavior for the hard hl 1000 test might
not be an issue to worry about?  The problem is that at a large hl like
this the differences in q values are incredibly close together and would
require amazingly slow levels of learning.  If the net was solving a mix
of easy and hard problems, the falure to solve the hard problems would
be hidden in the fact it was solving the easy one.

5) Tested how good the system was at solving mixed hard and easy prolems
with tests like [1, 1, 1.0001].  It was able to solve it correctl down
to about e-4 for q elarning at e-6 and hl 10.  Making it easier by
changing to hl 1 allowed it to solve it down to e-5 level 1.00001 vs 1
that is. Belwo that it basciallyh looked the same.

6) Decided to keep seek to zero learning at q learning speed and but
changed SD range to 100 standard deviations for 50% drop in pulse routing
which seems to make a decent level of blance for a 1 1 1 problem of equal
options.  I'm going to ignore the hl 1000 hard problem for now and assume
it's a problem that needs to be solved.  Or, if there's a solution, it
lies in incredihbly slow q learning.  Hum maybe I can simlate the same
problem by using low hl but much higher frequency with fast learning
and get the same problem to show up much faster so I don't have to run
tests for 12 hours?

8) Found a new trick on the graphics windows. Poping up the save dialog
freezes the applicaiton, BUT, the other window works, and I can zoom in
and examine the graph without it resetting on me!  Great trick to freeze
the app so I can look at the data.  It resets after a few seconds if I
don't freeze it.

7) Told Mount Vernon guys I don't have time to help them for the next
few months due to this project.  Makes this all that more important to
get done.

Today

Started a second big net 11x10 hl 2 test with slower q learning of e-8
vs the e-6 that has been runing for over 30 hours.  The e-8 solved the
problem at 770 simulation hours!  Probably about 6 hours real time.

With the old code this was solved, but it was around 4000 simulation
hours instead of 1000!  Solved with e-6 learning in the old code.
Interesting that this took e-8 learning but solved it faster.

Also have a long running hl 1000 test at e-9 learning. Like watching a
snail reace.  It finally had a colapse hickup just like what happens at
faster learning.  But it's recover looks good so I'm inclines to keep
it running for a another day to see how it evolves.

Same problem of the top q value falling to zero faster, and running into
the q below it.

Need to save and document the 11x10 nets.

Need to test the idea of faster pulse rate with slower hl to see if
the hl 1000 problem shows up.  More puposes per hl should create
the same problem I think but I might be able to reproduce it much
faster that way than the long hl and very slow learning.

Time for starbucks run...  [11:36 AM]

----

[1:06 PM]  Back from Starbucks run

Added sd: line to showNode() to fix formating issues.

Testing 100 pulses per second instead of one.  Wow, struggling.  Trying 10
per second first to see what happens.  Yes, scalops and colapsing at e-6
learning.  Good idea to test this.  So it's not just a long hl problem
it's a too many pulses withing the halflife that creates issues.  Cool,
something to study.

----

[1:42 PM]

Using 10 pulses per second, set learning to e-7 and it skipped 10 times
on the way down to zero but converged just fine in the end.  Setting to
e-8, looks like it will solve the 10 pps hl 10 problem with no skilling
or colapsing.

One option here is to not only set ratios automaticaly but to set q
learning speed automatically. It would be nice if that could be done so
that every node could be using it's own optimal q learning speed.

Another new thought -- don't change b values so quickly.  Averge in the
new values with the old to slow how fast the system shifts.  That might
fix the problem of colapsing and just instead force it to slow down how
fast it changes? I'll experiment with this.

----

[2:55 PM]

Added the b learning back to slowly change b values and use the SD as
the target.  To stop the colapse problem with hl 10 and 10 pps.

Works great.  Requires ver slow b learning to work however.  LIke e-6
level of learning.

q e7 with b e-6 solves the hl 10 at 10 pps smoothly and perfectly.
But it takes the system a little longer to converge than when we just
let it colapse.  Maybe 10% longer ish?

Speeding q learning to e6 allows it to find a solution as well.
Very smooth, BUT, it maxs out at a 88 9 2 blance and a RPS 8.34 instead
of 9.7 for the 99 0 0 solution with e-7 q learning!  But it might still
be creeping up to a better solution.  There's clearly a lot mor enoise
in the q values due to the e-6 learning speed.

So this is very cool.  The system is solving it the best it can given
the noise level, but no longer jumping around all spaz like when the
signal drops into the noise floor!  It just does the best it can and
stabalizes at a point where the signal is as close to the noise floor
as possible instead of causing the system to colapse due to the signal
falling into the noise floor.

----

[4:09 PM]

Added current b values to showNode() based on q values so I can see the
target separate from the b values.

Wow. Now with b leanring, I've reduced the SD spacing from 100, down
to 1!  And speed up q learing and b larning to e-5 for this 10 ppps 10
hl problem that was struggling before and even with all the noise this
creates, it seems to be solving  it without issues. I was using the
large SD spacing to counter act for not having the slow b learning to
protect the system from fast colapse.  But with b learning at or below
the speed of q learning, then the q values can keep up with the change
in q value that shifts in policy (b leanning) is creating.  And becuse b
is averaged over many q samples it's ok that the q values get real close
together now in the noise.  At least that seems to be what's happening.
So I can get signal from data with much faster q learning.  Which means
the net can solve problems much faster this way!

----

And now I've moved up to 100 pps, with the very fast e-5 q and b learning
and the thing is solving it, but it's stuck at about 18 32 50 sort level
and about 30 RPS out of what should be 100 posisble.  The q value graph
is total noise -- I can't even see a pattern beuse they are all overlapped!

But yet the system is able to pick out the value becuase the good link
is higher than the others a lot.

Oh, fuck, wait.  Have I just gone in a fucking circle and ended up right
back where I started?  With a system that only cares which q link is
the largest?  Wait, how exacdtly is this different from what I had before?

Ok, it is very similar, but better!  Because as it is now, the one
q value can stay in the lead for a long time, but they don't need to
actually cross in order for the system to detect they are "too close".
Where as with the old sytem, the only way it could measure closenes
was when they crossed.  But that created too drastic and quick of a
transistion from one being in the lead, to being random.  Now the system
can detect it's getting "worse" and more dangerous, as the q values come
to gether, but not have to wait for them to cross.  So it's creating
a very smooth approach to the best sorting it can do and not hitting a
sudden colapse problem allowing it to stablaze on "the best it can do"
given the noise level.  So it is much like before, but with a far better
measure of "closeness" using SD vs "who is in the lead".

So with e-5 learning, the 100 pps 10 hl problem is being solved
down to 31 RPS (out of 100).

With e-6 learning, it's gotten up to 81 RPS.

Trying e-7 learning now.  Looks to be headed to 100.  But it's very slow.

Also testing -1 1 1 learning problem with old easy 1 PPS and 10 hl to
see how the system is able to balance activity with this very low
SD 1 measure!  Becuase of the b learning, the system is far slower to solve
these easy problems than it was before!  I could speed up b learning
to make it solve the problems quickly but ... hey wait

Maybe it can learn to detect the easy problems from the hard, and make
the easy problems converge faster!  The SD measure tells it how good
a solution is.  This has a 5000 Sd spread, making it push the bad link
to 0.  But the speed of the push is not taking advantage of the fact
the system knows it's very very bad (5000 SD).  Maybe I should do the
b learning on the SD values instead of on the sort ratios!  That way
a 5000 Sd value will push much harder than a 10 SD value.

Then map the slow moving SD values directly to b values!

----

[5:24 PM]

And, ok, this system with an SD 1 ratio, is not doing a good job of keeping
the outputs balanced as well. The really bad link took a long time to
go down to near zero sort becuae of q learning, but the good values are
even sloewer swining back and forth allowing one to get way ahead of the
other for extended periods.  If they were low SD values, they would swap
very slowly and I think stay closer to 50 50.  I think.

Ok, recode AGAIN for the second time today.

----

[6:20 PM]

Coded the slow sd learning.  Seems to be be working.  Switched SD spread
from 1 back to 10 again to try and get better equal balance.  Testing
again with 100 pps hard problem.  The b learning speed may beed to
be somewhat different with this (which is really sdLearning now).

And now I forget what the other tests I have running were testing for.

But that's all the old code now.

Need to go to BGOP meeting soon.

----

[6:35 PM]

Yes, the new slow sd learning with a 10 SD spread is balancing
the -1 1 1 problem at about 55 to 45 at worse.  Which is looking
good.  

An old slow b learning I started with e-7 for 100 pps on the -1 1 1
problem solved it correctly and is balancing it very well at
like 48 52 ratio.  Wonder how the new slow sd learning will do
for that.

----

[11:57 PM]

Back from BGOP, and taco Bell, and Grim (99th episode -- big show
for 100th!)

So excited about this code.  Darn this stuff really makes by heart race
when it starts working so much better.  The tests I rand before leaving
couldn't solve all the way to 100 RPS. The e-7, 100 PPS, -1 None 1 test
topped out at 3 70 78 b value.

The -1 1 1 did something similar.

I went back to a 1SD spread instead of 10 SD becuase I saw evidence
that in fact it was balancing fine if just given enough time for all
to equalize.

The -1 1 1 100 PPS, hl 10, e-7q e-7b did just fine.  Toping out at
0 51 48 balance.  Sd was 42 for the 0. aka (2^-42).  The balance is
in the 50 49 51 48 sort of range for the two equal. Looks excelent.
And it converged very quickly considering the e-7 learning.  Maybe 30
miknutes ish?  It's been running for 2 hours now.

Started the same but with -1 None 1 test a well. It's convering nicely
as well.  Takes a long time to get the last little bit done.

Neither of these was able to fully converge with the sd 10 setting but
are converging with sd1.

----

Testing fast -1 1 1, with hl 10 but back to the easy 1 PPS test.
Using e-5 for q and b learning.  Converges in fucking 30 seconds.
But with the SD 1, it was not balacning the two equals.  It was off as
much as 5 90 or something like that.  But decided the solution would
be to slow down b learning.  So moved it back to e-7, and that got the
balance back to 45 54 ish range.

Now it's 47 51.  The SD might still be converging?

Going to try again with e-7 learning for this fast problem Mostly got
the right rewards in less than a minute. But the q values are moving
much slower and will take some time to converge to theri final near
zero values.

The bad one went to 0 very fast.  But the two good ones are still very
unbalanced at 97 2 sort as the q values work to get back to zero.

Yeah, RPS is at 1.014 which seems max for rewards reading to that many
decimal places within 3 minutes.  But it might take 15 or 30 minutes
for the q values to converge.

The balance is 24 76 between the two good paths so they are getting
better, but it may take some time for SD and q values to converge.
11 88 now. Getting worse. :)  But I bet it works out fine in the end
once it converges with these lower learning values.

----

Wow.  Just set up the slow hl 1000 test that I had given up on without
having the system sputter and colapse to try and solve.  The e-8 test I
have had running with the old code for 35 hours and the q values have not
converged yet. And it's had two colapses so far on the way to finishing.
And probably more to come ebfore it's done.

The new code, same test, running for 6 minutes, I used e-6 and it's got
the rewards solved in a few minutes, and the q values are quickly headed
back to zero and so far no sort of skip or even a budge on the rewards
line off of 1.04.

----

[1:07 AM]

Wow.  This code is amazing.  What I was going to give up on this morning
this code now eats for breakfast.  hl 1000 was solved in 30 minutes with
e6 learning.  The q values came amazigly close together for the solution
--- like e-7 close together.  But no issues. The SD is still 32 which
is like e-10 version of zero in 0 0 100 sort solution.

Got to test big net now.

Oh, wait, that -1 1 1 test at 1 pulse per minuite at e-7 larning is
doing a shit job of balancing at the moment. it's got 0 100 0 instead
of 0 50 50!  But it still hasn't finished converging.

The two good options are still inching their way down to zero.  I bet
when it gets down to zero it will do much better.

It looks like the one not being used is finding it's way to zero faster
than the one that is, reinforcing the spread and imbalance.  Odd.
Just have to wait this out and see what happens.

----

[2:27 AM]

Just finished figuring out how to add set_position() on the axes to adjust
size and location so the titles are now visible!

----

-1 1 1 hl 10 pps 1 test with e-7 learning.  Never has balanced.  Got stuck in 100 1
config for the two good values, and the q values seem stuck apart now.  No progress
towards balance.

I think the e6 learning did better.  The e7 seems to have enabled this separation.

Will start the e6 test again.

================================================================================

2016-03-05 8:18 AM Saturday

Yesterday

1) Added sd: line to showNode() to resolve formating issue

2) Experimenting with higher pulse rates to duplicate the hl 1000 problems
as lower hl settings.

3) Added b learning back to slowly merge in the SD sort ratio to b values
to smooth change and prevent colapse problems.

4) Changed the b learning, to be SD learning.  Slowly learns the SD
values instead of the b values.  Helps it converge much quicker for
large (really bad) SD values.  Sd ranges from 0 to infinity, which maps
into b values of 1, to 0.  So really bad q values with really large SD
values seek to large bad values very quikcly even with slow b learning,
so they go to zero as b values very quickly.

5) Lowered SD spread from 100 to 1.  With the b learning the large SD
spread isn't needed to prevent colapse.  Then raised it to 10, thinking
it was needed to help better blance equal.  But went back to 1, when
I realied this system did a very good job of balancing, but that you
just have to to give it a good bit of time for the q, and SD values to
stablize before it will balance all the way.

6) test test and more testing of new code.

7) Adjust graph sizes so headers are visible!

Today

Noticed linkMaxQAvg is halfLife*10.  See if *10 is needed! Test more.

All the easy tests I left running to check for balance have failed
to balance.

SimpleTest with 10 outputs trained to not sort to out 4, but should
balance across all the others, is not balancing.  Despite the fact that
the lead q value keeps jumping around as I watch it, and the graph looks
like a tanlgled mess of interwied vines, one q value has a SD of 36 where
the others have values like 80 and 114.  Where we only need a difference
of one in these values to create a 50% shift in sort balance.

So as tuned, this net worked really well on the other end to detect
very small differences in q values correctly, when we are faced with
what should be equal q values, the net is too senstive and seems them
as drastically different.

Going to take the *10 off the half life of the moving average and see
what effect if any that has.

This contant battle between trying to make the system very lose so
it sees equals as equals, and very tight so it can see differences as
differences is a killer.

I think I'm just going to have to tune this to work correctly at the
equal end, and if it looses resolution ability at the other I'll just
have to accept that.

----

Idea.  Try using the distance between the max q value and the second to
max as the source of noise measure.  I had this idea when I first coded it
but decided to go with mean instead.  But I think there's an issue with
the mean beuase when the system gets locked in it's invalid state of a
one heavilly used link and others not used, the others seem to have lots
of noise (probably a side effect of the /rpolicy adjustment?) but when
we average all the other links together we get a far smoother reference
measured to the smoother lead Q making it look like the SD is very very
small between the lead q and the average.  But if we measure it between
the lead and the second we will get the full noise of the single q value
and make sd larger -- indicating they are closer together in SD terms.

----

[10:58 AM]

coded the idea above.  Made no real difference in SD values.

Shit.  Going in circles.  When I add features to help identify small
differences between good and bad links, that same code breaks the concept
of equalty at the other end of the scale for equally good links.

To make SimpleTest work, I just had to add 100 SD spread instead of 1 sd.

The problem is that equal nodes really aren't very equal in q value.
Even more so probably based on how much noise is in the reward signals.
In SimpleTest one node is very bad, the rest of 6 are equally good.
On the graph the bad is miles and miles awy from the good.  But the good,
when we zoom in on them, are far apart as well.  Though they perform a
random walk, they still take 100's or 1000's of simlation hours to cross
each other so even the sd average stuff doesn't get them close together.

To make them see somewhat the same to the system I had to widen the SD
test to 100 Standard deviations for a 50% drop in flow volume.  And that
still has them at 4:1 flow rations at the moment.  Which means I need
more like 500 SD to make this suck flatten out the "equals".  Guess I'll
try that just to see if it works.

----

[11:49 PM]

Increased half life of linkMaxQVar to *1000.  Didn't like how much
the SD was bouncing around.  This seems good.  Before this, the SD was
bouncing on every display update from numbers like 20,000, to 150,000.
The average seemed like maybe in the 50K range.  But with the long Var
average, it's hanging around 90K to 100K so there's far less noise in
there to be confusing the SD numbers.

This made the Sd numners for SimpleTest a little better, but still
causing 100 0 0 0 ... sort when SD 1 (I moved it back from 100).

The SD range on the good links range from 45 216 -- a spread of 171.

I had tried 500 and that should make this very flat now but still allow it
to be solved because the the bad links are in the 10K SD to 20K SD range.
I'll restart again with SD 100 spread.

Ok so here's an idea.

To make perception work in the first part of the net, and RL stronger in
the later parts, we could vary this SD spread paramater from columum to
column as the way the system trades off perception and RL.  So make it
very wide in the beginning, to bias the nodes to act like percpetion
nodes and to make RL very weak.  And in later parts of the net, use
smaller and smaller SD spread values so RL dominates.

So don't try to find one value that correctly seperates noise from signal
but increase it from level to level so even at the early leavels, a very
strong signal will be separated out, but perception will domainate and
as it gets deeper, RL takes over.  So the early net is less sprase the
later net more sparse.

Oh, but that's another thought. Auto adjust the RL spread to regulate
overall network sparseness?  Seems like it could be hard to control
depending on the nature of the environment and problem to solve.

----

[12:13 PM]

So at 1 sd spread, the simple test was locked hard onto on of the 5 good
answers most the time, but would switch from time to time.

At 100 sd spread, it was balanced, with the weakest being about 50%
below the strongest of teh "equal" signals.

With 1000 sd, the balance is 19 20 19 19 20 0 ish, for the good and 98 0
0 0 0 0 for the other.  But that's like a .002 zero, not a .0000000 zero.

So it's a strong correct solution, but at the edge of where it would
start to be a weak solution.

So I'm feeling these tools do a good job of solving the RL problem now,
but we have 3 major parameters to play with, which is qLearning speed,
bLearningSpeed, and SDRatio. (SD per 50% routing)

I'm feeling there is no way for the system to every correctly identyify on
it's own what's "equal" and what's not.  I don't think it can correctly
identify all the noise sources and measure the actual noise to know
when the signal has emerged above it.  Part of that is the fact that the
system internally generates noise as well probably with it's stocastic
systems as well as the general complexity of the chaotic feedback that
can happen between parts of the system.

So, what do these do.  qLearning is a trade off between sensitivty to
rewards vs speed of learning.  You can learn fast, with low senstivity,
or fast with high senitivity, but you have to pick what's best for the
probelm the design wants the system to be able to solve.

bLearning is how fast the system is willing to exploite reward knowelge.
It's an explore exploite trade off paramater.  If it expliotes too fast,
it can get confused on false postives and superstious behavior. Slow
exploit gives the system more time to learn.  And I guess, q learning
and b learning are related in that b learning should not be faster than
q learning so the speed at which it is trying new behaviors, is in line
with the speed at which it is able to learn the true value of any behavior
it tries.

SDRatio is the high contrast vs low contrast brain paramater that
controls how much trust it is willing to put into it's perceived values.
It's the ratio of how strongly the system will lock onto a truth and
how strong behaviors will be. But it's oddly inverted so a high SDRatio
is a very wishy-washy system that is slow to commit and make decisions.
It needs strong evidence to act.

SDRatio a measure of how strong the evidence must be for the system
to act.

So, qLearning, speed vs resolution of reward learning.  bLearning,
speed of exploitation.  SDRatio, how much evidence is needed to act.
All of which are RL parameters to tune to the problem at hand.

Maybe in time I can find other solutions and reduce those to maybe
only one?

But with these, we can tune the current system to solve very hard
problems like hl1000 or high pulse rates that need very fine resolution
of extracting signal from noise.  Or very fast learning and acting,
if the problems we need to solve are all easy high reward problems.

----

[3:03 PM]

Had to run Pig to metro.  She's going down to FL for a week.  Julie is
here. Sue is down for a conference so she's going to visit with Sue as
well as visit with John.

I've noticed the links with less use tend to have a lot of noise in
their q values.  Just hink about this, if I divide the activity level
by the expected activity, they should be no more or less noisy than the
links with a lot of activity.  I'm thinking the a/rpolicy isn't working
becuase a is not a linear represenation of volume.  Or becuse AT chagnes
over time, the average expected value of AT is not linear with expected
activity ratios.  If I could adjust for that and keep the noise the same
in all links that would be better.  Seems like more than I want to deal
with right now however.

I'm very tired becuae I didn't get much sleep last night.  Went to sleep
late (3am ish) got up too early, (8 am ish).

----

Testing OneNode with 1 1 1 equality test.  Need an SDR of 100 to make
it balance weakly.

But with SDR of 10, it could solve a 1 1 1.00001 test.  With SDR 100
that works for balance, it won't be able to solve to that resolution.
I don't think.  I'll try.

--- Ok I was wrong. It did solve it.  At least to the 96 3 0 level so
far and it might get better.

So even with SDF 100, this hl 10, 1 pps test, has the resolution power
of 6 digits of resoution in a single node to measure the difference
in rewards.  You know, it seems to me that should go a very long way in
solving real world problems.

Need to stop messing with RL and dig into Precpeiton at some point.
But I feel sort of sad to leave it.  Like I'm not 100% done.

----

[6:09 PM]

Back from Pizza.

Traced down source of matplot warning at state and fixed it.

Wrote crtClear() and crtHomeAndClear() to replace hard coded strings.

----

Just set up hall problem.  Added graphs.  Realized qValue were set to
start at 9 which doesn't work. Stopped that.

New net solves it instantly. It's a simple route foodLeft to top and
foodRight to bottom. Not so hard.

----

The hall test is only rewarding for food, not for movement.  And I added
a random (0) reward to keep it learning with 1% odds of happening.

I swapped top for bottom so the net had to learn to cross wire itself
to wire foodLeft to move left output and visa versa.  Had to make net
one layer deeper so it's 7x5 now to give it a path. It's a fanOut 3 net.
Still it learned the solution in minutes and then got better and better
for minutes more.  This is the little net that can!

Saw a colapse problem however with e-5 learning.  The top q line cross
over the others and it died.  Didn't wait for it to recover.

Again, this is a problem of the q being used the most moving faster than
the two not being used most. The should move at the same speed if the
a/rpolicy was working correctly.  I need to attack that!  Figure out
the right formula for calculting expected a values given ratios and map
the ratio into the corect a value and make all the links more at exactly
the same speed even when they are being used at different levels.

I hope this is just a non-linearity issue and not a phase shift problem
that might be much harder to correct for.

---  With slower e-6 learning for q and b it didn't trip so badly
but it did have some odd paths it followed getting back to zero q
values.

----

[11:15 PM]

The other 11x10 hl2 with faster e6 learing finally converged!  It did
it at 2500 hours vs over 4000 a few days ago with different version of
the net.  So everything running converged!

---- Played with all more earlier.  Widened hall from 10 to 20 then 50.

Had to give it some movement hints becaues it didn't look like the random
behavior of the net was going to move the beasty all the way to the
other side without wating days. But a 1% random move reward was plenty
to get it learn to walk down the hall in a few minutes.  Then get stuck
on the other end, but soon learn to walk back, in just a few cycles,
it would take off and go to town.

But that is why the smell sensor basically telling the system which
way it had to go to get food.  The harder one will be to add feedback
into the network and let it learn for itself to genertae a rigth left
alternating pattern.  Far more than I've ever trie in the past with
any of the previous versions of this type of network.  But I think this
network is getting close to being able to do that.

But in theory, perception will have to work before that will be resonable
to learn.  So I need to dive into that and test if percpetion is adtually
working to create a representation of the state of the environment how
it's coded now (but turned off for all this recent RL only testing).

Going to the forge in the morning tomorrow.  Can dive into this
other stuff tomorrow afternoon.

================================================================================

2016-03-06 2:53 PM Sunday

Yesterday

1) Testing and truning SD based RL code more. Noticed I had halfLife for
the linkMaxQVar to 10x network halfLife.  Took that off.  Made little
to no difference.

2) Looked at the fact equal links were not balancing.  Getting frustrated
because I can't tune the system for hyper senstivity between good options
while also making it work at the other end of the scale for balancing
links that should be equal.  In the end, gave up on this fight.  Made the
sdRatio a paramater you must hand tune to control the point at which it
makes links balance.

3) Fixed matplot warning at startup.

4) Changed halfLife of linkMaxQVar to 1000x halfLife of net while learning
half life of linkMaxQAvg the same as the network.  This makes SD values
far more stable and constant which seems good.

5) got Hall test working -- new code solves it very well.  Though it
is slow to learn.  When I slowed down the speed of the simulation to
watch the activity, I relaized how slow the system really was learning.
But it learns fine which is cool to see the code learning to control
something "real" intead of just spit out numbers.

Today

Went to forge this morning.  Took Phil some gas to try and make a
spring with.  That failed.  Helped him with ideas of how to make it work.

As I was just now recaping yesterday's work just now, I had this thought.
What exactly has the noise based code done?  The point was to auto detect
the switch from equal balance to routing.  And the system as it works
now works very well to smoothly switch from blanced outputs to imblanced
sorting.  But what does having the SD actually buy us?  Why not just use
the raw spacing between q values instead of normalizing by dividing by
SD first?  What if this form of B learning was done without normalizing?
Is that possible?  Do I really even need to calcuate the SD?

Instead of using SDR why not just specify absolute q values to define
the mapping from q values to routing ratios?

Absolute q values are different for different nodes, but how fast they
change tend to be fairly simlar across the net due to the fact that
reward updates are applied to the entire network.  The only way they
would not be, is if we had very differen sized rewards, that applied to
only some nodes.  So some nodes, when active, would see reward levels
like 100 rps, and others, when ative would only see levels like 1 rps?
And these two tended not to active at the same time so they live in a
different reward level environment?  In a larger more complex systems
I guess that is possible.  Ah, and the other thing that is more likely
is that some nodes would be highly activive while others we have far
lower levels of activity.  The high activty nodews would be changing a
lot faster with larger learning updates, and have larger noise levels.
But that could maybe be adjusted for without the overhead of caluating
the SD?

----

TODO: The big improvements to RL was not from measuring the noise, but
from using q spacing to set exponential blance and to do b learning on the
spacing to make it slowly change ratios in response to shifts in q values.
I'll have to try to use hard coded q values for the ratio and throw out
the use of SD.  If that works, it makes it all much simpler not to have
have to calucate noise levels dynamically.

----

TODO: Also, need to test this mapping from ratios to expected AT values
to do the a/rpolicy learning speed adjustments.

Then at some point, move on to testing percpetion to see if that is
working!  But now, pizza and some ingreess.  Just 3 km away from the black
walking badge!  I'll come back and work on these other things later.

----

[4:06 PM]

Didn't go to pizza yet.  Added sLevel paramater to Network that defaults
to None.  If it's None, the Sd systemis used, if it's set, it's used
to define the mapping from relative q values to sorting level.  Setting
it to .00001 made the TwoNode problem trivally simple.  Could make
it all so much simpler than trying to measure noise because when we just
define it as a constant, there's no dynamic feedback questions in
the system to worry about.  Have to test this on harder problems and
see how it works or whether we might actually need SD to be a shfiting
dynamic relted to how fast the q values are changing or something.

It sure makes it all so much simpler that as long as we have to specify
a sort ratio value anyway, just do it as a constant.

But time for pizza now.

----

[7:02 PM]

Back from Pizza, Ingress, and picking up buger for Julie as Wendys.
Got my black trecker badge!

Playing with the new sLevel setting.

----

[8:51 PM]

Testing new sLevel for stablity or issues. Not finding any.  Where as
the complex systsem using Sd could trip and have issues or create
odd looking graphs as Sd shifted, this has not such effect since the
sLevel is a constant so there's no feedback effects.  Looks beautful.
Only issue is knowning how to correct set it.

Also, I'm seeing the problem of the more active nodes learning faster
than the least active so they winning node crosses over the loosers on
the way to the bottom and chagnes places a few times.  It all works out
fine in the end, but this causes the system to shift "answers" a few
times one the way to the bottom.  And in fact on the -1 none 0 problem
we should see an issue if the winner crosses over the others.  Just not
seeing it in the 1 1 1.00001 since it has no big effect on the RPS values.

But this problem is all about not adjusting learning speed correctly
which is on the TODO list to figure out and resolve.

I'm really liking how clean and soild this all is to throw out all the
complex mean var and sd stuff.

----

Also, caught an issue of sd values getting to large, making 2**-sd
underflow to zero, making all b values zero.  Added normalizing of sd
values back to zero for the min value to fix that.

----

Modified showNode() to show the current SD value as qsd, and the b
learning version as bsd.

Just waiting on slow tests to finish now.

----

Added sLevel to plot windows.  Alread did it for net.show()

----

[10:00 PM]

Found examples of the right settings for the new sLevel code to get all
confused and messed up.  But again, the confusion happens due to the
q values crossing due to the one being used the most moving faster.
When configured correctly, the system can solve it anyway, but when
configured incorrectly for a hard problem, like high pulse rate per half
life, with a problem like the -1 None 1 problem where false positives
cause drastically different rewards, which cause q values to shift in
different directions, things get totally confused.

One bad problem is that with tiny sLevel, large q value differences
cuases massive relative values that causes sd to blow up to very
large numbers very fast -- but struggles taking forever to return
the values back to zero.  The point of this was to allow bad values
to do that, but in this case, they were false negatives.

Ah, I think that's it.  Red was the correct answer, got in the lead as
it should, caused rewards to explode with happenss, node q values can't
keep up, so link q values explode upwats, but once the node catchs up,
link q values dive, but then the bug shows up, becuaes red that was in
the lead, dives to zero faster than the bad ones, and soon, falls below
the others. But it keeps faling, because b learning delays the switch.
But the new false postives, aren't very fast at b learning (postives
don't move it fast), but allow red to get very negative, and in time,
the red negative is very very large (bad) SD Value that pushes bsd values
to the point of making the sd very very bad, one of the bad options then
gets control, drives rewards down, and in time red q values rises to
the top, but it take forever for b learnign to turn control back to red,
then it's in that same problem again where red starts to do the right
thing, but then has to dive back to zero, pass the others, and it
ll starts again.

The b learning creates a delay that causes an osscilation that is created
by the failure of the system to move all q values at the same speed.
When the failure causes negative feedback, the system copes.  But in
the conditions of postive feedback, it blows up.

Postive feed back is when the q values are in postive area, but must
run down to zero. The leader beats the others.

Now wait.  I'm don't really uderstand this. I'm too tired.  I need to
just put this aside, get some sleep, and figure out tomorow.

I'm not sure I understand the condiion of when it blows up.

One issue might be that I've set b learning too slow.  So when conditions
change, it takes the system too long to actually switch policy.

I need to look at this again when I'm not so tired and can think
straight.

But, bottom line, with the right hrd problem, and right setting
of paramters, the use of sLevel can get crazy -- and even seem
to get stuck in a way it doesn't look like was able to get
unstuck from.

Tomorrow..

----

Ok, just watched it again.  The problem is that red wins, goes up, then
turns and dives towards zero, but passes the other ones.  Then one of the
bad ones takes over causes rewards to go negative, driving all q values
down to the negative, but then it does the same loop upside down this
time, with the "winner" (the bad negative reward guy) beating everyone
back up to zero.  So with a negative reward, the bug in the system works
in favor of the winner.  So the system rewards a false postive of a link
that produces a negative reward.  All q values then make it up to zero
in time, and in time the true good guy, drifts slowly above the others,
but then the node q values are all set to the negative q value, and must
make huge changes to the postive starting the cycle all over again.

I think speeding up b learning will reduce the magnatued of the problem,
but the core problem at work here is that the q values need to all move
in unison which is what the a/rpolicy is trying to do, but not getting
correct. Will have to look at this more.

----

Ok, I think I spoted another issue.

With high pulse rate relative to half life, if rPolicy is 100, the at value gets
high, but when rpolicy drops to zero (1 actually), at is still still
high.  so a/rpolicy explodes!  Instead of 100/100 or 1/1, it becomes
100/1 and causes the q value to move 100 times faster suddenly!

So, again, this is yet another discount between what is really happening
and what q/rpolicy is implying.  Shit, it might be harder than I was
even thinking to get this right because we don't just have to cope
with what policy is NOW, we have to know what it was over the entire
half life -- like the last 1000 pulses.  We have to track the difference
between policy and truth, somehow and use that.  a/rpolicy was
the right idea but totaly implemented incorrectly!  That's a prolem
for tomorrow!

================================================================================

2016-03-07 7:04 AM Monday

Yesterday

1) Replaced sdRatio with sLevel and tested.  Decided that if we are
going to need a tunable parameter anyway, what's the point of all the
complex mean and varriance code?

2) sLevel looked good at first, then ran into more colapse problems.
But realized all those problems were due to the q values crossing each
other when they shouldn't.  That is, crossing and changing place just
becuase the most used on is updating faster, and moving to a new steady
value faster than the rest when what we need is for them all to move at
the same rate.

3) Fixed a bug when sd levels got too large, cusing underflow on
calculation of v values resulting in 0 0 0, intead of 0 100 0 sort result
and divide by zero blow up.  Fixed this by normalizing sd values back
to 0 for the min after every update.

4) Changed showNode() to show qsd and bsd (current sd vs moving averge
sd).

5) Identified new issue of q values not updating in sync.  Not only
is the a/rpolicy slightly wrong due to discounting effects, it's
very wrong for high halfLife beause each pulse that makes up the history
was routed at a different sort probablity, so dividing them all
by the current sort probablity can be very very wrong at times.  This
only showed up in long halfLife (or high pulse rate to halfLife).

Today

Thinking about how to fix the problem of q values in bed when not able
to sleep.

Decided how to fix it.  Must track expected AT.  So use a new set
of AT for each link, and for each sort, we divide the one pulse up
by the current sort ratio, and do a partial update of the expected
AT for each link.  So if the sort ratio is .8 .2 .2 we update
the expected ATs at factor=.8, .2, and .2 no matter which way the
pulse was sorted.  Then the actual AT is updated with 1 or 0 which
is what we are doing now. Then, instead of dividing the actual AT
by the currentsort ratio like .2, we divide by the expect AT value
instead.  That should get all the discounting perfect.

But that is likely to still have this noise prolem where the most
used links have far lower noise levels and the least use links have
far greater noise leves.  This is beause when we take on pulse away
from 80, it reduces it to 79, but when we add that one pulse to a 20,
it increases to a 21.  The first was only a 1.25% reduction, where as
the second was a 5% increase.  So the sorting of pulses is creating 5%
noise in one and 1.25% noise in the other.

I need to equalize that. [EDIT LATER -- it turns out there was no noise
problem]

If the learning speed is then actual/expected, the magnatude of that
number around 1.0 needs to be adjusted as well.

I can't help but feel all this growing complexity should be reduced down
to something far simpler.  It's as if we are doing it all wrong to start
with, and then bending over backwards to fix what we did wrong to start
with, instead of just doinging it right to start.

But I don't yet see how to do it right, so lets keep pushing forward on
fixing what is wrong, and see where it leads us.

But I'm not sure how to fix the noise problem.  I think the best bet
is not fix it.  Fix the other by tracing expected first, and then lets
see if there even still is a noise problem before we fix something that
isn't even there!

Ah, but a side thought, a way to fix the noise problem might be to
not track actual as 0 or 1, but to track actual as an adjusted number?
But if we adjust the expected in the same way, then the ratio ends up
the same and reomving our adjustment.  That's not right.

Maybe the noise problem is something that is actually impossible to fix?

Ok, time to code expecte AT...

----

[7:40 AM]  Done.

Shit.  That was trivally simple, and fucking beautiful.  Now the test
that was just blowing up, is working prefectly.  All three q values are
moving in perfect sync where the difference between them is just the
accumulated true statistical differences.

And it just solved the hard problem instantly because the good q value
moved ahead of the bad very quickly and is just staying ahead of the
bad ones.  So very small sLevel values should be possible now allowing the
net to solve problems much faster without fear of false q value crossings.

I'm not seeing what looks like noise level problems in the weak links
either.  Maybe the noise was all yet another side effect of not doing the
updates correctly?  More testing is certainly needed.  I'll need to let
this one high speed pulse test run longer and equalize and see if noise
issues might show up as the values get to their final locations near zero.

It's amazing at times how just 5 lines of code, changes verything from all
messed up and complacated, into beatiful simplicity.  The graphs make the
problem look trivally simple to solve when there's no longer complex lines
crossing each other.  It just made a hard problem look trivally simple.

And I bet I can get away with using much smaller levels of
forceExploration as well.  That would help test and verify if there's
still a noise level issue with the lesser used ones having a noise blow up
issue if they drop all the way down to like .01 % or something like that.

And this makes me wonder if I can go back to a standard deviation
test to set sLevel and make it automatic this time?  Or evem better,
just calucate the correct level of sLevel from the other factors like
qLearning and average reward rates or something?  Making it all automatic?

And hey, q learning speed.  Maybe qLearning speed becomes irrelevant?
Becuase any q learning speed works and the real issue is just how much
data has to be collected before the valid q values emarge above the
noise floor?  If the link q values all move in lock stop with q learning
updates, then it makes no difference how far they are all jumping up
and down together as long as their relative values only represent the
true statistical evidence of their difference?

Damn, this could be better than I realized.

But first, time for my Starbucks run while this test runs.

----

[8:37 AM]

Back from Starbucks run.

Wow.  The 100 pps -1 None 1 problem has converged and I'm seeing no
indication of extra noise in the two q values that are running at 1%
sort vs 98% sort for the lead.

Well, I'm having this thought now.  I've added a ton of complexity to
how RL is implemented in the network, in the way of these split links q
values, using the node as the future reference, to make links more stable.
Pull to zero to make the links more stable. The whole b learning stuff,
now implemented as sd learning.

What if none of that was needed, and that all that was needed to fix
links so they update in sync so their relative values stay true to the
statsitcal evidence of which is better?

What if, this new technique of tracking actual vs expected AT was all
that was ever needed and that I can throw all the rest of it away now?

Or merge it down into the link update formula?  That is, maybe I don't
need node q values after all and and I don't need to split the link q
values between the node and the relative link value?

Ok, but the node q part of it updates relative to the speed of the use
of the node.  And that's crtical because the speed of use of our links
vs other links is crtical to credit assignment.

Ok, but if we merage that back int our the link q values, then we just
need to update the whole thing, relative to the speed of use of this node.
So, nodeAT * LinkAt / linkEAT so the link values adjust for link activity,
but nodeAT is still part of the learning speed.  Then link q values stake
on their full real value?  But what do they use as their future predictor?
They must share a common future predictor so that the links will stay
in sync relative to each other.  Yeah, that's where the link q value
remains useful I think.

Ok, but pull to zero might not be important. Lets try turning that off
and see if anything changes.

----

So I on this "hard" problem I cranked b and q learning speed up to e-4
and it solves this once "hard" problem in all of 30 seconds. Except
the noise in the system would cause it to jump for small periods to
bad answers, and then recover.  Making it "mostly" solved in RPS value.
had to put the b learning to e-5 to filter ot that noise allowing the
hard problem to be solved in like a minute.

But as I look at the link value, I see three numbers jumping up and down
widely, all perfectly in sync, with the only thing we actually care about,
is their value realtive ot each other.

So what's the point of having this random noise signal mixed in the q
values since we just ingore it by looking only at the difference between
signals anyway?

We aren't using the q values for anything. Why calculate it?

Or, to the extent we need it to calucate the learning update, why store
this value that jumps random in the q values?  It seems like we are
calcuating something we don't need?  Why isn't all that random jumpling
only in the node value, and so that the link values become notihg but
the simple and clean difference signal?  how do we do that?


Ok, so if the learning signal in effect is saying add 5 (maybe .00005
really) and we add 5.1 to one, and 4.9 to another, becuase at/eat is
1.1 for one and .9 for another, why add 5 to both of them, is what we
are asking.  So noramlize 1.1 and .9 to .1 and -.1, and use that as
the adjumstment!  Take the 5 out of it the formula!

Or, calculate the link updates for all links, then normalize the
difference amounts to sum to zero?

But are we using the link values as part of the update or not?  Do they
need to be the real q values for the update to work? I need to check
the code and see what I'm actuall doing now.

Wait, but first, lets remove pull to zero and test that.

----

Turned off pull to zero.  Tested same -1 none 1 test with high speed
pulses.  Same good answer but now q values are spaced twice as far apart.
I see no reaon to pull them together anymore.

----

Looking at the update formula, I do ues the link value currently as the
sort vs the target. So the source is node+link to make and the target is
just the node.  So if we can give up on making the link true q values,
lets just do relative updates for the node q value and see what happens.

----

[12:28 PM]

Wrote long letter to Tague when he asked what I was working on.

----

Changed source of update from node+link to just node to remove the current
link value from the update to see what would happen.  Basically it worked,
but without the link it didn't seek out a zero value, it rose at first,
but instead of diving back down, it just want horizontal and then kept
spreading out unbounded.  So the more evidence it accumulated, the futher
and further the q values grew apart from each other.

I think that growth would be uncapped and would eventually lead to
overflow so we can't have that. I could put bavck pull to zero to stop it.
Actually, let me turn pull to zero back on and see what happens.

----

looking at code, I see that having the link vlaue as node+link in the
update formula is almost identical to haveing pull to zero!

With it in the main update it gets this this as an extra term:

  link += -link * qlearningspeed * lat/leat

And the pull to zero just does this:

  link += -link * qlearningspeed

So by having it in the update formula, I'm bising the pull to zero by
the main lat/leat learning factor which is pointless because there is
nothing about pull to zero that needs to be learned there! It's not
contengent on what the nodes have been doing!

And, doing it in both places is effectively nothing more than doing it
twice as strongly, while at the same time, adding a little more noise
due to the lat/leat term that adds no value!

So, ok we should remove it from the update and use pull to zero instead!

----

[2:54 PM]

Long message to Tague.

Tried a test with simple tst without Pull To Zero or without the link
referece.

Turns out the noise problem of unused links is cuased by pull to zero.
When The heavy used link, which gets to locate itself near zero, has very
little fighting with the pull to zero code. But the lightly used links,
that have values far away from zero, are in a constant battle with the
pull to zero code where they try to expand out futhre, and pull to zero
trys to pull them back to zero.  That's where the noise comes from! When
I turned it off, the heavilly used link and the unused links had what
looked like the same level of noise in each -- but the unused links were
marching off to negative infinity.

And, without pull to zero, the links that should have been equal, end
up on a random walk away from each other, which then led to one being
selected for full time use and the others unused.  But the full time
use kept it on a more consistent upward path (false postive rewards),
wghile the others drifted far and wide below it.

Ok, so noise is not cuaused by pulse imblance -- at least not to any
extend as much as it's caused by pull to zero. But the pull to zero
noiseis harmless becaue it's only nosiy when it's so far away from zero
to be clearly bad.  The closer two signals get to each other, the less
noise there is -- or the more the noise is the same in both and not an
issue with determing which is in the lead.

So, we can throw out the link relative updates, and replace with the
better pull to zero approach.  And it doesn't look like there's any need
to address the noise I thought was an issue.

----

[6:08 PM]

Played with a few tests using the new update andit was working very well.
Then I tried halltest.  And it's now learnimng!  The old code was able
to learn but this on just gets stuck on either side.  It slowly learns
to go to the other, than gets stuck on teh other side.

Something about this new appraoch is making it harder to learn this
problem! Ugh.

----

[9:45 PM]

Went to Home Depot to get shelf for Basement at Julie's request.

Found all my old Studio Sound foam has turned to mush.  Bagged it up in
5 big bags and trew it out.  Also fixed Julie's tolet flush handle --
replaced it.

The HallTest I started earlier actually coverged and learned using the
new SLevel system and the at/eat.  Though it took far longer than with
the SDR code.  And the SDR code worked with the new update formula that
no longer uses the link's own value.  But link graphs are sure a mess
under this new system on the HallTest for some reason.

I have no idea what's happening or why it is struggling so.  I'll have
to find some simpler tests to try and uncover what's up.

I susepct that the old code that allwoed q values to cross by accident
actualy helped the net get unstuck and explore different options where as
the new code that never crosses by accident will be far more senstive to
making up it's mind and not changing it. Maybe this is just an issue of
the system be highly senstive to correct setting of sLevel etc becuase
if it converges too soon, it won't likely want to change it's mind.

But I'm at least happy to see the new approach converge.

A key issue is that the SDR code tracked separate noise levels for every
node and maybe whith a larger net like this that becomes important so that
we can't get away with using a single Slevle value for the entire network.
That might be part of what's happening.  Maybe.

----

Also still runing the very long and slow OneNode [-1 None 1] test with
e-8 learning e-9 Sl valuen and hl1000 (1 pulse per second).  Ran for
a day and then as it was converged on the old code, the old code had a
problem with the q values passing each other, and then it colapsed down to
negative and caused a big spike and recovered.  Theting to make sure the
new code doesn't have an issue with this and so far it's looking good.
The q values are staying perfectgly in sync and parallel so there's no
reason to beleive it will have an issue.  SD spacing is 93 and 171 as
well so it's not even close.

================================================================================

2016-03-08 7:36 AM Tuesday

Yesterday

1) Added the at/eat code to fix the fact that a/rpolicy was not the
right way to do it!  Works create now!  I feared it was going to be
highly complex to try and calucate the correct /r factor to do this
correctly but it turned out to be trivally simple.

2) Took out pull to zero.  Didn't feel it was needed becuase the at/eat
code made the system update the links in perfect lock step.

3) Looking at how the link q values updated I saw lots of noise in the
q values as they updated in lock step that seemed to have nothing to do
with relative value of the nodes.  And since relative value is the only
thing important for making sort decisions, I felt I should be able to
change the update formual to fix that.  So I went looking.

4) Took out the use of link q in the update of of link q values and
only used node q.  This means that link q values are no longer real q
value values.

5) Discovered that having link q in the update forumula was nearly
identical to my pull to zero code!  Except it also added a small linkq *
at / eat bias which is pointless.  And without pull to zero, the q values
would diverge farther and farther apart forever.  So I put pull to zero
back, but left linkq out of the update.  bias

6) Lots of testing of above code.

7) Found that everything worked far better with none of the odd colapse
or hickup issue now that q values don't randomly decide to cross each
other just becuase one is updating faster than the others.

8) Except, HallTest -- which for the most part stopped working.  Ugh.

Today

Work up this morning and had an epiphnay about the HallTest problem
while in bed!

Hall test has alternating inputs which activate one part of the net
then the other. The flow volume through the network is what tells the
learning system to update one section of the network, then another.
This is why the AT term is in the formula of course.

But my at/eat TAKES OUT THE AT FACTOR!

This is becuase unlike rpolicy, eat is not normalized back to a
percentage!  So by using at/eat, I removed the very important AT term from
the update!  That didn't effect the learning of link values, but it very
much broke the ability of the network to learn WHICH nodes to update!
And that's what is different about the Hall Test!  I basicallyh took
out the part of the larning rule that makes the network do the most
important learning!  No wonder it was faling to learn, and then even
worse, after learning. forgetting and unleanring hours later!

To fix, I can etither normialize the EAT terms before I use them.
Or, the simpler way, just use the node AT value and add that back in.
So nodeAT * LinkAT / linkEAT.

So, let me code that, and see if it fixes the Hall test!

----

Yes, thank god. It's working again!  The new code doesn't suck for
complex hidden reasons!

Doesn't seem to work any better than before as far as I can tell at
first glance. But it's not broken anymore!

The problem of having to set SL values vs q and b learning speed is sort
of a bitch becuase it's so hard to tell what's wrong when you have them
set wrong.

----

Maybe I need to add more intentional randomness in the the pulse
rounting to help the system learn?  Now that I have the at/eat
code in place random exploration at pulse routing would be easy
for the system to take advatnage of.  Or maybe there's a better
way to do that, like injecting random pulses into the netowrk?

----

Oh, shit, The HallTest that was working so well with a steadilly
increasing reward curve, just got stuck on one side and now the rewards
are tanking!  And a second run with the same parameters is struggling
to get started!  Not as good as I thought it was!

----

[8:33 AM]

Started a third test with different parameters now.  Going to wait
and see which if any can learn and if the broken one recovers. Time
for a trip to starbucks.

================================================================================

2016-03-09 11:37 AM Wednesday

Yesterday

1) Added NodeAT to NodeAT * LinkAT / LinkEAT to fix the fact that the
addition of LinkEAT had factored out all the node to node differences
in volume that is key to rewarding the links of one node when they used
and not another node when they are not being used as much. Thought this
would explain why the hall test was no longer working.

2) Though 1) above was important, it didn't fix the fact that HallTest
was still broken.  The AT/eat change broke something important.  I don't
know what.

3) Wrote long letters to Tague and got pissed over the letters and pissed
that fixing the code with the addition of NodeAT hasn't fixed HallTest
and pissed over the fact I had no real clue why it wasn't work.

4) Went to see Eddie the Eagle movie as an escape. That was inspiring.

5) Watched the first match of AlphaGo playing againsg Go world
champanion Lee Sedol but fell asleep before it was over (it
started at 11PM).

Today

Woke up and watched the end of the AlphaGo game.  AlphaGo won!

Started digging into why the fuck my network seems unable to learn!

To reduce the complexity of the ahll problem, I turned it down to a single
layer network, full full fanOut so each input wires to all outputs. That
should make it trivally easy for the system to learn to wire node 1 to
output 7, and node 5 to output 3, etc.

Still couldn't learn.  Or, it would take a very long time, and would
finally learn, then it would forget and lose it.

This is trivally simple, it should not be hard.

In trying to understand what was happening, the showNode() display has
got me totally baffled.  The show() display is showing the network output
nodes to two of the 7 outputs mostly, and the showNode() dump shows the
same thing in that two putputs are most active, but it's NOT THE RIGHT
TWO OUTPUTS!

I did a showWire() dump to make sure it's wiring the net as expected. It
is.  I looked at the graph output and compared the graph q values to
the showNode() q values -- and they are consistent.  The q value that
shows the highest in the graph, is the same one in the showNode().

I'm totally fucking lost as to what is happening.

Switched from Hall, to OneNode, to verify the network wires, and displies
as expected.  And with OneNode, it seems like it's working as I expect.

But, looking at this test, I have one pulse intput, 5 outputs, and I'm
rewrading it to send pulses to output 3.  And the damn think can't learn
to this this most simple of tasks.

So I'm going to put the mystery of the wiring and display of HallTest
aside for now and just tackle the fact I've broken my network and it
can't learn even the most basic of tasks.

And I think I understand the problem.  The at/eat adjustment was too good.
It was working before with the at/(rpolicy +.01) exactly because it
was broken.

For the net to learn to send pulses out links 3, it has to reward 3
when 3 is used more, and not reward it when others other used more.
But the at/eat is equalizing all  nodes in an odd way.

I think it's this size based adjustment problem I was worrying about
before that has broken everything.

If output 3 has a volume of 10, and output 1 has a volume of 1, and
then one extra pulse is sent to 1, instead of to 3 as part of the
random routing, then output 1 doubles in size, while output 10, goes
down by 10%.  And then we equalize both of those with the /ate, and we
2/1 as the adjusted volume for one, and 9/10 as the adjusted factor for 3.

Ahhhh no I don't think this explains it.  That should cause the low
volume nodes to be more strongly adjusted, and more noisy, but I don't
see how it creates an unfair bias. that would sotp the network from
learning which link is the good one.  And I thought this was working.
Did I break something in the code by mistake without being aware of it???

----

OMG -- when I tried to add nodeAt to the update yesterday, I actually
addeed node.qValue instead!  OMG that broke everything.  I have no clue
how it was working at all. But, a quick change, and something is still
not working...

----

Ok, so going back to the old old update code, of using qLink and
doing /rpolicy that works fine.  But the new code with the / eat isnot
working. I thought that was working beautyfully the before????  What the
fuck happened????

----

Ok, fuck.  OneNode was set to a 1000 halfLife.  I didn't notice that and I
turned learning down from like e-8 to e-6 and of course learning doesn't
work at e-6 with a 1000 halfLifew without giving it far more time to
extract the data...  Stil digging to understand what's going on however...

----

[2:05 PM]

To help see what was happening, I've added code to notomralize the qValues
around zero -- I substract the qValue aVerage from all the qValues
after each update.  Since only relative qValues are important when I
don't use the qValue in the update, this is harmless, and it helps me
see what the qValues are doing by removing all the massive jumping up
and own they were doing.

And what I see is intersting. In this -1 None 1 test, they start off
very with very little noise, and start to fan out, exactly as they should
relative to each other and relative to their true values.  But then once
they get far enough apart to start causing the network to bias the sort,
and once the rewards start to come from that, large levels of noise show
up in the q values and they jump wildly all over the place.

Then after the reward learning is complete, they settle down again.

So, this can create an issue with the fixed SL system.  Q values looks
good, SL starts to kick in, then noise levels pick up, and they jump all
over the place, crossing each other etc, flipping what was a good solution,
to a false positive, and totally messing everything up.

This is cuased becuase the node Q values have to drift to the new reward
level, and it's the difference ebtween the node Q value and the rewards
that is is driving the size of the learning update signals.  When
the nodes are correct for the current policy, the signal is small. But
as policy changes, the signal gets larger until the nodes drift to the
new value.

So I think this just all points out the complexity of the interaction
between q learning, b learning, and SL values.

If SL is too small, the system will kick in with a good solution, and
things look good, but then noise picks up, and the false postives get
created, and the system gets into a mess.

Slower b learning should give node q values more time to catch up with
the current policy, and help reduce this transistion noise, but sl values
have to be large enough that they don't allow false postives to form.

I'm thinking that the key problem with the Hall test is that the correct
setting of these parameters is very critical, and then when you get
them wrong, the system doesn't work. So, how on erath do we correct
set the paramaters???  That's the question.

----

[3:39 PM]

Ok, so even with paramaters carefully adjusted something else odd is
happening.  I turned off pull to zero, to allow q values to spread out as
far as the want.  I assumed they would keep spreading to infinity. But
the system is stuck with a 39/40/20 divide and the q values are no
longer accumuating any additional evidence and spreading futher apart.
They are staying the same and the graphs have gone horizontal.  This is
with a long halfLife of 1000.

If learning was working correctly, this would not be possible.

Something about how this at/eat update is working is causing the system
to give weaker nodes false postive rewards. I'm thinking it must be this
bias problem but I don't udnertand how it's working so I don't understand
how to fix it -- or if that really is the cause or not.

----

I've made sl much smaller, to force the system to keep routing to
all paths, to see if that will make the system allow the q values
to drift that much further apart before it starts to converge.

This is to prove that it's the flow imbalance that is causing the
learning bias that is breaking everything.

----

Damnit.  I was using the update that included the qLink value which makes
it do pull-to-zero.  Shit it wasn't something else.  Going to the most
recent one again that does not use qLink and should keep spreadking
futher apart.

----

[4:30 PM]  Have 4 tests with different parameters running.

With pull to zero off, the q values keep spreading futher and futher
apart as they accumulate more evience.  With pull to zero on, the spread
is capped -- which means this cap, trades off between q learning speed
and learning noise and sl values.  If the sl is too large for the cap,
the sytem can't converge. If it's small enough to fit inside the cap,
then learning noise might be too large.

All this needs to be automatic becuse there's no way t know how to set all
this for a random network you can't see what's happening on the inside.

I've got a .1 pull to zero running as well to see how it works when we
let the spread get futher apart.  I assume it will work better due to
strongr signal to noise separation.

It seems to me that the pull to zero is needed to keep q values from
slowly marching to infinity over a long period, but that it might be
controlled by measures of signal to noise so as to let the the q values
spread as far apart as needed to get a strong signal to noise ratio.

And with the systems ability to let q values expand to improve signal
to noise, we shouldn't need to worry about making the system work well
in high noise by having slow b learning.  IN fact maybe the entire b
learning think that helps deal with noise needs to be thorwn out and
replaced with automatic pull to zero control that lets the signal lift
itself out of the noise.  This sounds like it has great protential.
With such a system, the learning speed is only limited by how fast the
signal is able to raise itself out of the noise.

Ok, so around and around we go some more.

Going to bike to pizza and leave these running...

----

Also, we might be able to throw out b learning.  And just adjust policy
based on signal to noise measure.  The sort policy should be biased to
want to be at 50/50, and be pushed futher and futher away from 50/50 by
higher and higher levels of signal to noise.  If noise returns, policy
starts to fall back to 50/50.

At the same time, the pull to zero should allow the data to keep expanding
when signal to noise falls.

----

[7:04 PM]

Back from Pizza.  Wrote bike in 80 deg weather!  Heat wave.  Back to
60 ish soon and likely will return to the 50's which is more normal for
this time of year.

Playing with SDR with the new code where I can see q values deverge and
where pull to zero is reduced or off.  Trying to figure out how to make
it automatic.  Again.  Or at elast, make it easier to set paramaters for.

----

[8:58 PM]

OMG. Figured out all my confusion about the Halltest from earlier today.
I couldn't figure out what data was being sent where, becuase I thought
it was a 1 layer network, but in fact, I had created a 2 layer network!
So where the first layer was sending pulses had nothing to do with where
the second layer decided to send pulses!  Glad t ohave that fucking thing
resolved.  I was going insane!

================================================================================

2016-03-10 2:49 PM Thursday

Yesterday

1) Found that I had used node.qValue when I tried to "fix" the update
formular yesterday insead of node.at!  No wonder it wasn't working at all!

2) Found that when I was trying to test HallTest, by reducing it down
to one layer, and then I couldn't for the life of me figure out why the
nodes outputing data in the showNode() display was not the right nodes
for the output of the network, was all caused by the fact I had made a
3 layer (two layers of links) instead of a 2 layer (1 layer f links)
network. So the nodes I was looking at were just sending data to the
next layer, not to the final outputs!  So nothing was broken other than
myu ability to type a 2 instead of a 3.

3) And, when I tried to test with OneNode, and was getting really odd
results, but couldn't understand why, it was because the halfLife was set
to 1000 instead of 10!  No wonder it wasn't learning even the same simple
problems I was uign before -- not to mention the big 1) mistake as well.

4) Modified the links to normalize q values to an average of zero to
remove the noise that wasn't imporant. This was mostly just needed to
help make the graphs of q values easier for me to understand since shifts
in relative value were not improtant to how the network acted.

5) Found that something I didn't expect was happening.  The q values
would start off with very low noise levels, (straight lines), then as
it started to learn, the noise would increase and the lines would wonder
all over the place, then after learning was done, thye would go back to a
low noise. This was due to the node qValue neededing to seek a new value
-- staritng at an average reward of zero, then as the poplicy shifts,
rewards increase, the node has to increse and converge on a new  value,
like 1.0 maybe.  So the noise increases as it's learning, and settles
down as it converges on the answer.  This idea that it would start low
and then expand was unexpected to me, but makes total sense now that I
understand it.

6) Also played with turning PTZ (pull to zero) off and letting the q
values keep getting futher and further apart.  Realized the good thing
about allowing that, is that the it improves the signal to noise as
it's happening. The furture apart they are, the more resistant to noise
they become!  Also tried qLarningSpeed*0.1 for the ptz learning speed
as well as 0.01.  Slower learning allowes the q values to spread futher
apart and cope with large levels of noise.

7) Tested a lot of code with the fixed node.at code, and the 0.1 PTZ code,
and found evetyhing is working much better once again with this.  Oh,
and using the SDR code instead of SL constant level code.  The constant
level code doesn't cope well with noise levels that are rising and falling
at differen ttimes in different nodes. Using SDR 100 on these tests.

8) Looked at the idea of once again trying to make the setting of the
parameters automatic.  Couldn't figure out how to do that yet.

9) Watched AlphaGo win game 2!

Today

Slept in, finished watching AlphGo.  Starging to get another cold.
Lack of sleep is triggering this I think.

Wow, so, I made a bunch of stupid mistakes, and took me a day to figure
them out. These emails with Michael I think is highly distracting to me.
Sent another smaller one to him yesteray and got anogher one back,
where he's basically telling me to suck it up and suffer and that it's
all "my mistake".  Shit man, I don't need this negativity in my life.
I need to keep working on this without distractions. Do I need to dump
Micahel as well?

Running some more tests with the 11x10 net today.

The 11x10 hl 1 test converged in 120 simulation hours. The only older
version of the same test I saved in a jpg toook about 1200 hours to
converge!  So that's a great improvmeent.

startged a hl 10 test yesterday.  Has not yet converged.  Started a
hl 2 test, just now.  I've done more of theose in the past to compare
against. It's running.

Think I need to do pizza or something. Don't feel like digging into code
at the moment.

Oh, yeah, and it's Pig's birthday today. She's down in Disney with John.

================================================================================

2016-03-11 11:45 AM Friday

Yesterday

1) Slacking off.  Not doing much.  Got pissed off at Michales eMail
and it but me in a bad mood.  Was feeling sick as well.  Napped
for a few hours then watched TV to late.

2) Running tests on 11x10 networks.  The HL2 version of the 11x10
converged at around 850 hours!  Older tests converged at 1500
hours and at 4000 hours.  But one more recent also converged
at around 800 so this is about the same as the best of the recent
tests.

Today

Good question.  Nothing at the moment is broken so I have nothing to go
hunt down and resolve.  So I have to head in a new direction.

I could dig deeper into the RL code and see if I can do moare automatic
setting of parameters. But I feel I really need to turn back to the
perpcieotn code and dig into that.  Though I'm dragging my feet at making
that happen. Been posting to the Jeff Hawkins list,

I should work on the decoding problem.  That is, with two inputs changing
in rate in a binary repeating pattern, will the network percpetion
features decode that to into a time varying "state" signal?

----

[5:01 PM]

Got distracted on link SD stuff.  Decided to test using the learning update
LinkDT value as the source of noise measurement just by calculationg
the avg of LinkTD ** 2.  Looked at other otions as well but decided I
liked this the best.  It's simple, and clean and it shows greater stability
of value than the other options and what I was doing with the complex 
measure of distance between the max and the average of the others.

I'll probably go with this new approach just becuase it's so simple.  But
it tends to be smaller, so instead of using SDR of 100, I'm using SDR 1000
with this new measure of noise.  Running some longer tests now to see
how it works.

But simply using the learnng rule update error is far simpler than
what I have been doing before so I really like the feel of this.

----

Also wrote some long posts to the HTM list and a post ot Tague.  Got to
stop letting myself get distracted.   Pig came home from her FL trip
today.  Julie picked her up.  Julie is off at work now.

----

[6:00 PM]

back from Pizza.  All 4 of the OneNode test worked fine. The first two
had a strange fluke of behavior where the middle green value drifted up
and crossed the leading red q value which causes a dip in rewards before
it recovered.  But the other two tests did not so it was just a random
fluke that they both did that.

Going to just delete the old code and use the new linkVar idea
beause it's much simpler.  And send the default SDR value from
10 to 1000.

----

[6:51 PM]

Code cleaned up and convered to new new simple SD code using TD error
update varriance.

----

[11:54 PM]

Added PlotCol to Overlap so I could see how activity traces were changing
over time to see if percpetion is working in any sense...

Watching AlphaGo game three now!

================================================================================

2016-03-12 10:25 AM Saturday

Yesterday

1) Rewrote SD based code to just track the varriance of the Link TD Error
Tested on multiple tests and decided to throw out the old code that was
using the spread between qValues as the source of noise information.
The new one is far simpler, produces the same basic results, and isn't
as likey to have odd feedback isssues.  But it tends to be smaller so
I switched fromi using a SD 100 as the default to SD 1000.

2) All tests but Hall seemed to be fine with the new code.  Hall however
has coverged very nicely but took much longer -- like 1000 hours vs 25.
I suspect this is due to the SD 1000 measure.  Maybe it's not bad?

3) Started looking at percpetion by once again digging into the Overlap
test.  Trying to remember what I was last testing!  Turned percpetion
learning features back on.

4) Wrote PlotCOL graph code to plot changing AT values in a column
of the network to try and understand if the system is working as a
percpetion system.

5) Watched the full game 3 of AlphGo beating Lee Sedol. Just had to
feel sorry for Lee.

Today

Need to continue on the perception research. Is it working?  Do the AT
values acutally represent the state of the environment?  So far I'm not
seeing that it is!  But this could be due to having too many degrees of
freedom in the network relative to the information in the input data.

I suspect however it's just not doing what I hopped this design would do.

Need more work on the graph code as well.  I need to extract the data
update code to the common Plot class and make it easier to quikcly plot
different items -- giving me more power to look at issolated data points
in the system.

Oh, on the percpetion problem, I feel the AT values change too quickly
and drastically.  I'm wondering if this is a problem of needing to
use longer halfLife values, and or, higher frequency pulses, to give
the AI values far more freedom of grwoth relative to each other.

Even if this is all fine, what I mght be looking at is in effect, just
low level noise in the system when I have such a low purlse count
relative the the halfLife.

----

[12:57 PM]

Added +1 per graph to offset the graphs in the same window and create
a nice strip chart like effect.

Adusted halfLife and learning rates so i could better see what the system
was doing.  Was creating some nice secondary signals in the next layer,
which were sort of inversions of each other along with one that was
twice the frequencey.

But, the killer is that 3 of the 6 lines are basically flat and contain
almost no information.  That is not what is needed here.  I don't want
signals that contain no informration.

And adding graphs to the next column, I see the data seems to be turning
to mud  -- just equal flat line data with no information.  Just added
a thrid column to see if it gets ever worse!

The punish sorts to high and reward the low signal is just making the
network REMOVE all the information from the signals by avergering them
together.  That's not working.  I need a learning algoirhtm that will
information maximize, not information minimize!

Soring to the lowest signal will have the largest effect on that signal
I think.  Maximising change to the sgianls seems like it might be a good
rule to follow.  So I'm going to go back to the idea of traaining the
system to sort to low by punishing sorts not to low and see how that
chagnes things.

----

[6:15 PM]

Back from Pizza.  Walked to get some exercise.  Too much sitting in
this chair!

Adjusting pulse rates and halfLife creates the sort of effect I was
expecting with the network merging signals together to create new signals.
But with a fanout of 5, on a net of only 7, the signals very quickly
turned to mud -- all averaged together with little to no signal left!
But changing to fanout of 3, make it produce far more intersting results
by limiting how many signals were being merged together.

However, I'm sure, with a few more layers of debth, the result would be
the same -- gray goo -- not good separate signals.

One possiblity is that this is ok!  With real world data we won't have
the 3 or 4 signals like I am playing with.  There would be millions
of signals.  And even with a binary tree like network toplogy to mix
and match the signals, they would remain as unique signals for a large
depth I would suspect.  Turning to grey goo might just be the networks
way of indicating all the possible combanations had been created?

But, I have to wonder if we can do even better?  I'm really attracted to
the deep learning idea of the hidden layer trying to form itself into
the best set of signals so as to maximize the signals predictive power
for the inputs that create it -- aka, trying to encode as much of the
intput data into the hidden layer as possible.

I assume this really only makes sense if there is a reduction operation
at play.  Like encoding 1000 signals down to 100.  Human, maybe the point
here is sparse encoding?  So it's not that we need to reduce 1000 down
to 100, but that we reduce 1000 down to a signal with only 100 bits on
at a time?

I'm looking at my learning rule of traning the network to try and sort
signals to the node with the lowest current activity level.  That works
create for balancing activity, but it also turns the signals to mud --
if you always ssort to the least activfe, that makes all the activity
level the same --- aka MUD.

And I see this in my testing.  Though each layer has interessting and
unique signals, the strength a varrance of each level is getting less
and less with each level.  So maybe it starts off with signals going to
1 to 2, but the next layer is like 1.2 to 1.9, etc.

Didn't actually measure it, but it's doing soething like that.
The magnatude of the signal is less and less with each layer.

So I'm thinking I need to do something to maximize signal strenth in
the learning rule while also doing the balancing.  Whicxh seems oddly
at odds with each other.

But, maybe the solution is connected with the idea of sparse encoding?
We try to minimize the number of nodes active at once while at the same
time trying to balance activity?

The *a term of the policy plays a big role here of attracing pulses to
the currentl most active node.  And I'm thinking now, if the output
nodes don't range in value from 0 to N, but instead, range in value
from 1.8 to 1.9, then we are reducing the power of the *a term to suck
pulses to one path vs anouther.  Maybe we need to suck harder.  Hee hee.
Maybe harder policy *a suck will automatialy create sparse (or sparcer)
encoding?  What if we use a**2 for example???  Or what if we normalize
the *a values so they range from 0 to N somehow to adjust for the fact
that don't make it down to zero?

This sounds exciting! Basicly, increase the amplificaiton effect in
the policy.

Maybe by changing how we use the w term, we can somehow make this work
with w values alone?

This is exciting becuse I sense there's a way to make this work. Even
if I don't know that is yet!

We can't yet A go to zero of we use *a in the policy becuse that would
prevent any pulses from being sorted to that node! 

Another way to look at this is as rducing the correlation.  So if signal X
is collated to output A, we sort X to A to combine then into one stronger
signal and if it's 50% corelates to A and 50% to B, we stort 50% each way
-- or something like that.  And the *a term (with help of w balancing)
is to some extent already doing that.

----

w*a is the suck power.  So increasing w makes it suck more.  But I think
it's wrong to confuse this.  We can't train w to balance pulse flow,
AND use it to make nodes suck MORE.  I don't think.

Yeah, I think the problem we are facing has to be fixed without using w.
W controls how much it sucks ALL the time, we need to fudge how much it
sucks when it's in it's low state, and how much sucks when it's in it's
high state.  Seems we can reduce how much suck power nodes have when they
are low, or increase how much suck power they have in their high state?
We need to fudge the high low suck power balance?

We need to balance the varraince I'm thinking (vs just the volume which
is what w balances).  If we calcuated the varriance of all the nodes AT
values, we could then use a v variable to control that. Make it stronger
for the node with the smallest varriance and weaker for the one with
the largest.  So how do we control varriance with a weight?

Hum.  If we balanced varaince, could it (would it) balance overall
volume at the same time?  Or would volume not be important to balance
if we balance varriance?  Suggesting the idea that maybe all we need to
balance with w is the varrance of each signal?

Might be tricky if the varance was zero? :)  But I guess with an activity
trace it's never zero, as long as you sample it as times other than
when the pulse just went though it?  Like sample it before each pulse
is about to be sent maybe?

If we tracked the average AT value, then we can do (at-avg) and bias
the suck with that.  Somehow.

w * a * a/avg  ???

w * a * ((a-avg)/sd + avg)/avg   (sd adjusted varriance)???

w * a * ((a-avg)**2/var + avg)/avg

Something like this might work. But we will have to track avergae,
and maybne varriance, and add another weight for every node.

I'm sensing there's probably a solution far simpler than this that
produces the same result?  Maybe it's using w, but using it to balance
varriance instead of pulse voume, which means using it somehow differently
in the policy than simply w*a?

a**W?

Or using it to balance pulse volume, but do it by amplifying variance
in the weak nodes and mute it in the strong?

With a**w, w would be 1 by default, making it just *a.

But w wold be higher, pushing it up to w**2 etc.  Or lower, pushing it
down to w**1/2 etc.  So w would be 0 to infinity in power with learning
being towards zero, or away from zero?

I don't think this works.  A higher exponent makes it stronger if a is
greater than zero, but weaker if a is less than zero.  Any a values less
than zero, get weaked by a larger w, not made stronger.

This is the wrong approach.  Unless we normalize a to a value of one
when it's at it's average value.

(a-avg+1.0)**w  nah...

(a/avg) ** w   ???

Ok, this last one seems like we are on the rigth track for using w to
balance varriance.  But can we use this same trick to blanc flow volume
at the same time and end up with blanced variance and blanced flow volume?

ORRR, blancing varriance mightbe all we need and let the RL system
balance flow volume?

I guess can can't tell what this will do.  So testing feels to be needed.
But instead of changing w, I'm thinking I'll another variable.  v

----

Ok, instant issue.  If we are trying to balance the variance of NODE at,
then an (a/avg)**v maybe should be done at the node level and not the
link level -- which is where w takes place?

So each node has it's own v varible, and and a/avg**v happens at the node,
to suck more data from all it's feeding links when we increase v?

That sounds possible, but how and when exactly do we adjust the v
variable?

----

Ok, here's an idea.  what we need to train with this varriance is the
min value or the sparseness of the signals!  So one varible is turning
the varriance to adjust the minumum value of the signal, and the other
is is looking at the max value to adjust the top.  So one is adsjuting
the top and the other is adjusting the bottom?

So, like we were training to sort to low, we can forget that and train
to not sort to high -- so as to balance the high end with the w value.

Then, with v, train to not sort to low?  Or something like that?

Or this use of "not" seems odd.  So train to sort to low using w?

And train to sort to high using v?

So we balance the sorts to low, and we balance the sorts to high, but
we adjust each with these different cotnrols?

If it's working this way, it feels like each link will need it's own w
and v varible.

This feels strange and complex, but also feels like it might work.


================================================================================

2016-03-13 1:26 PM Sunday

Yesterday

1) Digging into perception problem now.  Using two nod overlap net
for testing.  Showing strip-chart like graphs of at values of nodes in
4 layers.

2) Noticing the problem of the nodes turning to mud at deeper layers.
That is, all signals just get averaged together, and all the interesting
variation is lost.

3) Thinking of complex ways to adjust *a policy to control sparseness
and to perserve signals.  Creating sparse represenations to drive RL.
Beucase mud is not a good driver of RL!

4) Tried a**2 and left graphs running last night.

5) Had to adjust halfLife and pulse rates to give more pulses per
halfLife in order to see waveshapes forming.  otherwise, behavior was
too chaotic.  Also adjusting famout 5 to fanout 3 keps the signals anlive
and interesting longer.  didn't turn to mud as fast, but was clearly on
the way to becoming mud none the less.

6) Game 4 of AlphaGo vs Lee Sedol last night!  Lee won!  Yeah!  He gets
to keep his dignity.  The next game will be interesting.

Today

Watched game 4 of AlphaGO last night!  Slept in and finished it in
bed in the morning on the phone.  Julie headded back to school today.
Got a ticket already on 29 heading home! That will help teach her to
slow down!  Time change just happaned so I'm an hour beheind on my day!
But the sun will stay up late which is always so enjoyable for me!

Continue digging into percpetion research and see what is possible.

The a^2 test last night didn't show anything good!  The first test I
did yesterday had w decressing to e-9 (effectively off). At that level,
we have most nodes were just off, and a few were on constant.  so all
data just got mapped down to 2 of 7 nodes.  Not every useful.

The second test only reduced w to e-6 so it would stay in play.  What it
caused was a system that was very sparse with a few nodes staying on
for 100 hours at a time, and the others contantly off.  But then it
would switch.  So at a rate of 100's of hours, it would switch. But I'm
sure that switch as not by polity, but by w learning slowly "correcting"
the imbalance.

Clearly, this is not working. Though it is creating sparseness, it's
not what is needed.  The policy needs to create dynamic sparseness not
w learning.  The system must have the ability to keep working correctly
as a perception system even with w learning turned off.

----

It seems to me that if we increase amplification of suck, then we will get
waveforms that max out -- that suck all the pulses from the input nodes
and turn into square waves in effect.  That seems like overkill.  If we
have a system that tends to produce sin waves that overlap out of phase
it seems to me that would give finer grained control over behavior timing

Wave forms that turned into square waves could in the end reduce down
after many layers to be only a single finaloutput.  All pulses would
get sucked to a single output -- that could flip as input flips but it
seems wrong that we would want sparseness to be amplified to that degree.

It also seems very tricky to balance this.

----

Testing a**1.1 showed very different beahvior than just a**1.  Odd.
But it once again does the same things we see with a**2.  It gets into
situations of zero activity for a bumnch of nodes, with all the activity
of the pulses limited to only a few nodes. Then switches to other nodes
later, very slowly.

It's feeling like *a is a magic number and if we go beyond that at all,
it breaks down, and in effect the system is unable to blance the outputs
with any setting of w values.  The control of the w values is not enough'
to create a balance in a system beyond *a in "suck" power.

lets try a**0.9 and see what that does!

----

I had decided on *a a while back due to a belief it made the system
correctly implement bayesian logic.  So the idea of shifting away from it
could be considered questionable becaues it breakes the natural bayesian
logic of the system. But is that eally correct?  Really important?
Or what?

with the larger than *a like a**1.1, the system is getting into long
term osscilations where too much data is sucked to a node for too long,
then the w learning fights it back until it seems to swtich elsewhere.

It feels like the system is right on the edge of chaos with *a and if you
add anymore, it pushes it over into a land of self feedback osscilations.

The goal of course is for the system to only be a direct response to
the the senory data and not to have it be a chaotic signal generator on
it's own.

----

It seems to me that my learning alogirthm is measuring pulse flow and
blancing pulse flow. This does a lot, but It feels like blancing pulse
flow is ultametly wrong.

I need to blance something more complex like information content.

That would be something like how predictible a pulse was.  Or how much
"energy" was in a signal. A DC signal has no energy, and one with large
amounts of chagne, has a lot of energy. So it seems to me.  How can I
measure and "balance" energy?

---

Signal energy is sum(x^2) dt.  So I find. If I normalize the signal
around the mean, then this is certaonly a real measure of how much
variation there is and that seems to be an intutive match to what I'm
thinking by saying a flat signal is void of information.

Could I measure it relative to zero by just calculating at^2 I wonder?
A flat signal with lot sof pulses would have a value but one with the
same number of pulses, but more vairation, would have a larger value I
belive due to the peaks Being squared.

Let me check.  Say we have 1 3 1 3 vs 2 2 2 2.  The first would be 1 +
9 vs 4 + 4.  so 10 vs 8.  So yes, even if we just measure from from zero
and don't normalize, it would give us a potentially useful measure.

Hum, and if we had 0 2 0 2, which was lower level, but just as much
relative amplutide, that would be a signal streigth of only 4. Far less
than the 9 of the flat signal with twice the pulse rate.

But yet to me, that increased amplitude seems to be more important than
the volume.

This is telling me that (at-atMean)^2 would be the correct energy measure
to try and maximize across all the signals.  So how on earth do we train
w values to maximize that measure?

If we send a pulse to a node, the AT goes up by the inc constant.
We can calcuate the value of this new level-mean^2 to see what energy
value it contibutes to the node.

Sending a pulse to a node in it's highest value, or in it's lowest value
would contibute the most energy to the signal. Sending it to a node when
it was in the middle would contribute maybe no energy at all.  Hum.
that's a different idea. So we reward for sending it to a high or low
node, but not for a middle node?  Or maybe we reward it for how MUCH
it contributes???  Or reward for sorts to the node that would create
the most energy and  punish for sorting to the one that creates the least?

But wait, don't we want to measure how much it changes the energy? So
if the node is high, and we incase by inc, then it incrases by inc^2?
And tha twould have the same effect no matter where we send it?

No, I think what's ipmortant is that the pulse defines a new point on
the AT curve, and we need to measure the energy content of that curve at
that point.  So, (at-avg)^2 after the sort.

So if I calucate the energy of each sort option, I can punish the sort
to the non max, and reward the max.  But make the amount of the reward
relative ot the difference.  So a sort to to the second best would not
punish the second best much or reward the best much. But the reward to
the worse, would have strong learning effects.

This sounds like theory that makes sense. But I don't know if it's even
posible to maximize energy by adjusting the w values like this.

Or, worse yet,  might this force the the system to route all pulses to
one output, becuase that large volume output would always have higher
energy, than if we spead the pulses across all outputs?

Ah, but no, becuase if we map all to one output, it turns to mud and
the energy relative to the mean, drops to near zero!  So if we trained
the system correctly, that wouldn't happen.  In theory.

Unless there was in fact only one signal in all the data, then it would
happen, but it would be correct to do that???

This is sounding interesting!

But, doesn't a large signal, always have more energy than a small signal?
Ah, no, again, if the large signal doesn't have large variance aorund
the mean! So even though we have a sinal of strength 10, if the mean is
10, then sending pulses to it doesn't help maximise energy!

Ok, so lets code this and see what the hell it creates!

[5:53 PM] cats want dinner!

----

[6:35 PM]

Well the energy sort is intersting.  It's creating two signals, out of the
4 it started with.  Where one is the inverse of the other.

This might be "broken", or it might eactly what is right?  The 4 signals
i'm sending it are all syncronized and correlated. Maybe these two
inverted signals is the correct maximum energy solution?

I need to see what the total energy of the system is!  And compare to that
the total energy of each column using my standard data balancing solution!

But, noddles and Co for dinner first!

----

[8:04 PM]  Back from Noddles and Co dinner

There is a problem here.  Though it won't sort all signals together to
create mud, it will select a strong signal over a zero or flatline signal.
Sending a pulse to s flatline signal will never improve the pules as
much as sending to a strong signal, as long as the adding the pulse to
the strong signal doesn't flatline it.

So the 3 fanOut net stanalized nicely with two complementary signals.
But the 5 fanout net is showing great lack of a stability and is
drifting oddly and wildly, ad also has mostly just 2 or 3 signals with
the rest flatline.  Could be intersting to see if it eventulaly stanlizes
on something.

But, I need flow balancing AND energy maximising at the same time
somehow!  So if the sum of some signals is the optimal solution,
that same signal should be sent to N different links if need
be!  How could this be done?????

----

An idea.  Compare energy produced by the pulse sort, to the curren energy
level of the node.  e-Energy.  And then train to sort to the node where
our pulse does the most good to increse energy!  This might work.  Sorting
to a flatline node should do more to increase energy than sorting to
others.

----

[11:28 PM]

Coded that idea.  Been testing it. It's sort of interesting.  Even with
fanout 5 it doesn't turn the signals to mush. But with the fanout 5, by
column 3, we basically had only 2 different signals spread over all the
nodes which repeated at the frequency of the lowest frquency.  The high
frequency compoents of the intputs were mostly lost.  But at least the
signals did represent the intput.

There are certaonly volume balancing issues.  Though this appraoch is
keeping anything from flatlining, some signals turn out to be very weak
and others much stronger.

Testing fanout 3 now to see how that works.

I don't really like the cmplexity of this approach.  I have to calculate
both a mean, and an energy signal for every node.  The at is derived from
the pulses.  The mean is derived from the at signal.  The energy measure
is derived from the mean and the at signal.  Then we adjust w learning based
on at, mean, and energy.  Wow, just so much to mess with.

But I like the fact that we are making some intresting progress down this
path.  I jsut feel there must be a far simpler way to get the same sort
of results.

Also, I've wondered if we can do both energy measure learning, and flow
balance learning at the same time on the w signals?  Might it work?

We can regulate the power of the two by adjusting the speed of each?

Something else to try and experment with I think.

================================================================================

2016-03-14 6:49 AM Monday

Yesterday

1) Digging into percpetion.  Tried a*a and a^1.1 and a^0.9.  This is
producing bad results.  Gave it up.  Feels like it needs to be *a for
bayesian logic to be correct.  Messing with this might have been a
bad idea.  Deleted code.

2) Came up with Energy maximising idea.  Measure envergy is sum(x^2)
of signal.  Added atAvg and atEnergy to nodes to track extra information.

3) Tried training w learning to sort to node that produces the largest
Energy output relative to mean -- so (at - atMean) **2.  This would not
sort to a flatline s it sorted all pulses to two signals that were prefect
opposits of each other an the rest were flatline.  Interesting, but wrong.

4) Next more complex Energy idea.  Compare the the energy result of a
pulose sorting to a node, to it's current Average Energy and train to
sort to the node that increases the energy the most.  Tried fan out 5
and fan out 3 tests.

5) fan out 5 worked, but mostly reduced all signals down to the same
signal or it's invert which was the lowest frequencey of the 4 inputs.

6) fan out 3 worked better, Produced an intersting set of differe signals.
But I'm not sure if it's much different than what the old default w
learning was alread doing.  More careful testing would be required to
see what the difference is.  Also, it suffers from poor total blanace.
Though it produced 8 different output signals, they varry in pulse volume
by a large amount -- 20 to 1 ish on the first colume, but reduced down
to about 20:5 on the 3rd and last column. Didn't test larger nets.  tried.

Today

Work up at 4 am.  Wide awake. I realky should not have eaten a whole
box of thin mints and drank coffee before going to bed!

Lay in bed thinking.  Had some thoughts that made me give up trying to
go back to sleep and decide to get up and start writting notes which is
why I'm up before 7 am.

----

So here's the epiphany from Bed.

Input nodes, sort using policy to output nodes.  So we can directly
predict what the output node signals will be based input node signals and
the policy.  But what we need to learn, is the inverse of that! If we run
the algoirthm backwards, we can predict the input nodes, from the outputs.
So, what we are trying to do is reverse engineer, what the external
enviromeht has done to the signals before we received them!  So if we
had a two layer network, and just picked some random policy values to
create the second layer node signals from the first, what we want to do,
is reverse engineer that as we are solving the "percpetion" problem.

This is what deep learning systems are doing and what we need to do in
order to solve this.

But what we do, is complex.  None the less, if we look at the AT signal
as I'm doing in these graphs, we can directl compute the output signals
for a given set of w values.  But if the envioenment was doing a linear
transform on some signal sources, in order to create the sensory signals
we are recieving, what we need to do, is find the inverse trasform of
the environment, so we can then comppute our hidden layer, by referse
enginering, the transform that exists in the enviornment.

Now, we don't do a simple linear transform with w values.  We not only
use the linear transform of the w values, but the *a of the policy,
not to mention the RL and distnace Bias and forceExplortion.  This
could make finding the inverse  tricky.

But ignoring the tricky, We need to train the w values so as to
make the inverse function (running our algoirthm backwarsd) produce
the intputs.  We must adjust the w values so as to make the
backwards running of he agorithm, as accurate a prediction of the
iotputs, as possible!

So, if we have a thee node network, mapped to three output nodes,
at any point in time, we have three AT values for the intput nodes
and three AT values for the output nodes.  Because of how our network
oeprates, we can directl compute the output nodes AT values, from
the input knowing the w values (plus the other crap).  I think this
is true.  That is due to how Activity TraceWoroks, the three
link AT values, will sum to be the AT value of the intput node.
And the three links that feed an output node, will sum to directly
calcuate the output node value.

And, if wek now the link AT values, we know it all reverses.

But, given the w values, we can't run it backwards.  I don't think.
Got to check this.  If we take the output nodes, and divide the flow by
the w values backwards, we don't get the same answer.

Let me check if this is true with two nodes:

Assume the input nodes have values of 1 and 2.  And the first node has
a .5 .5 fan out.  The second has a .1 .9 fan out.  That would create
outputs of .5, .5, and .2 and 1.8. Which sum to two outputs of .7 and 2.3.

So running it backwards, .7 would fan out to .5 and .1.  That normalizes
to .833 and .16666 so the outputs would be .583333 and .1166666

The 2.3 fans out to .9 and .5 which normalizes to .6428 and .35714 which
makes the outputs 1.478 and .8214

making the reverse inputs:

  1.4047 = .5833+.8214

  1.59466 = .11666+1.478

And those correctly sum to the 3.0 we started with but don't match the
1 and 2 we started with.

And if we didn't mormialzie it itwouldn't produce the right answer either.

So, if we are given a forward mapping as a linear transform, the inverse
mapping would be the inverted matrix of the first.  Obviously the same
numbers don't work both direction.

Hum, so what is it we are trying to find?  Numbers that do work in both
directions? that's what deep leanring is doing. That alsomost seems odd.

If we found numbers that wee the inverse of the universe, then running
our same numbers backwards would not create the answer we want. So why
does this trick of running it backwards and trying to make the backwards
and the forwards be inverts of each other?  What is that doing???

To assume we even can figure out the inverse, we have to have already
made some assumptions about the nature of the signals we are inverting.
Which is what ICA is doing?

So it assume the hidden signals you are trying to discover, have
a normal/Gaussian distribution.  Ah, but the more we mix together
the closeer to Gausssian it becomes.  So ICA works by trying to find
the inverse vectors that produce the lease Gaussian signals it can.
Does this trick of finding weight that are as close to self inverting
as possible somehow do the same thing???  Or something very similar???

Ok, so we can save the last policy we used that is the number passed to
the last random Pick.  And use those are as working w values.  And we
can check wha the inverse from the output nodes back to the intput nodes
look like.

Wait, no, what we need to use is the current eta values!  That fixes
the long term desired mapping from input to output.  Doesn't it?

I need to check how close the input nodes, times the EAT fan out, match
the output values!

But, whatever we use, we need to run it backwards I guess, and then
see how far it's off, and then adjust w values, based on how far and in
which direction it's off?

Something like that.

We need to find a way to test how well the output values, are acting as
predictors of the inputs, and then adjust w in the directions to make
it a better predictor.

So we can use at values, to  check how it's working.

Or maybe we can use it to predict pulses?  That's sort of what I've
looked at in the past by having the output nodes learn to be predicors
of the next pulse.

Another idea, is acting as a predictor of WHEN the next pulse will show
up.  But AT values are all about areadly so using AT to predict when the
pulse will show up is close to what we have sort of been doing already.

I sense burried in these ideas is a cool solution waiting to be found!

[8:55 AM]  Maybe more sleep now?

no..

So making the outputs be the best predictor of the inputs, simply means
that the outputs contain as much data from the inputs as possible.
I guess. So if we are doing a N to N mapping, then the best mapping
is just 1 to 1 with no mixing adn that will gurantee the outputs
predict the inputs.

BUT, we are doing something more complex with the AT values. But again
I think we are faced with the same issue. if we map all pulses from
one input, to one output, that will make the one output the best
predictor of that input.

Ok, we need to do something btter than thtis.  We want the output nodes at
value, to be the best predictor of the current input, when run backwards.
So if the backwards fan in of an otput node is like .8 .2 .2, we want
that to indicat that the probablity of input form those three places
is .8 and .2 and .2.  So when we compine the predictions in reverse,
the current output level of the output node, as reflected as a masure
of stregnth back against the weights, should be the best predictor of
current xpected pulse probablities as possible!  Yes, this is it!

So, when pulses are .8 .1 .1 incoming, we want their fanout to an otput
node to be like .8 .1 .1 so that the output becomes highly active,
so that the inverse is a good predictor.

But wait, maybe that's not right.  Maybe  instad of using the same w
value in both directions, we have to be tricky and use a reverse value
or something.

Ok, this is headed down the right path.  But I need to think more abot
what this means...

Output nodes are pattern recignozies in effect, and when they see their
patern, it means the input should be matching their patterh, so the
output nodes active at any point should be th eonest that best describe
the current inputs.  And we waht the w values to move towards whatever
configureing allows the output nodes to best describe the inputs.
To minimize the error between this reverse prediction and what it
really is.

Ok, this is all good.  Need to go wake up and get dressed.

----

[6:00 PM]

Did go back to bed.  Didn't wake up until after noon!  Then took a lot
of time to get back onto track here.  Distrated with facebook and talks
about the AlphaGo.

Happy Round Pi Day!  3-14-16

I'm having trouble grasping how this great idea really works.  Just
started a run of the old w learning algorithm to compare to the long
rung of energy max algorithms.  Not sure if energy max is doing anything
beter than what the old dead simple algorithm of punishing sorts to max
and rewarding low.

The new test using old algorithm looks like it's almost stable in one
cycle, where as the enegy max stuff took overnight to stabalize on a
solution that seems very similar.  But I need to let it reduce w learning
down to e-6 and see if there's any drift in behavior.

Checking my slaved jpg of the other run, it looks like the new energy
max is creating more complex waves after 3 layers but they varry a lot
more in size and volume. The old simple technique has created a set of
interstin waves almost instantly but they seem to be a bit more similar
with all of them cycling with the same low frequency compoent of this
test. It didn't pull off anything that could be seen as a fourer transform
to extract seprate signals of different frequencies.  I think a better
solution will do a more powerful job of extracting very different sinals
even of different frequencies.

----

So, how to do this new idea.  How do I test the reverse version of the
mapping?  Do I use eat? policy? w values?  I guess a good first step to
check the forward version of the mapping. Can I calucate the output at
values from the input AT values to any accurcay?

Buecause before I can train w to make the reverse mapping correct I need
a way to calcuate the forward mapping.

I guess I could do the policy sort in reverse?  Use the w values and
the rest, including the input node AT value in the *a term, and then
calculate a reverse sort????

So forward sort X, and we get C of ABC.  Then reverse sort C, to aee if
we get X?  And if not, we train?  But what direction do we train in?
Reduce w to reduce the odd of X->C happening in the first place or
incrase X->C to increase the odds of the reverse sort happening?

----

Looking ICA descritin I am reminded it's normally created by gradiant
decent -- aka learning serach instead of some type of direct solution.

That's good becuase that's what we are trying to do with this approach
which is have the w values seek out a maximum solution through search.

Ok, but still lost on what to search for.  Lets dig into the forward
problem!

----

Added total energy by column to compare w algoithms. The energy
maximising seems to work at first glance. Where as the old
algorithm is producing numbers like .1 per column, the energy
maximising is prodicing numbers as high as .6 at times. But
it's bouncing around a lot so I'll really need to let it stablize
before I know who it's working.

----

The energy numbers are highly unstable. Which I find shocking considering
I'm using a halfLife of 1000.  The input signals are highly stable, so I
was expecting the energy numbers to quickly stablize for the first column.
But they aren't.  The drift up and down by what seems to be a wide margin.
I might look into this later. would help to graph them along with the
signals.

First column is drifting from like .12 to .38 range.  If I don't have
a coding bug, I can only guess the half life is not nearly long enough
and that it's drifting for different parts of the signal cycles?  Yeah,
come to think of it, I do think it takes many seconds for this sytem to
cycle though on cycle and this drift is probably in line with that.

I probably need a much longer half life for the mean and energy numbers
relative to what I though I was doing here.

But, ok, that's not the direction I wanted to go anyway right now.

----

Increased halfLife of avg and energy by 100.  New tests show far higher
energy numbers for both energylearning and old w learning.  Old w learning
is like 3.0 range and energy learningis like 14.  Have to wait for w
speed to drop down and systems to stabalze more to see what results.
Maybe energy learning wasn't working as well as it should due to halflife
being too short relative to the signals?  or maybe once it all stabliaizes
it will be about the same?

----

Ok, some quick testing confirms that at values sum.  The link at
values sum to the feed node at value exactly. And the at values of the
links feeding into a node, sum to the at value of that node exactly.
didn't test the eat values, but they would not sum correctly. But they
would indicate what the current policy was TRYING to do.

So, if we want to make the reverse work, we can normalize the at values of
the link values as percentage flows, and back calculate what the feed node
should be if the link values worked as percentages in both directions.

So, we would be trying to make the reverse percentage flows match the
forward percentage flows.  Probably should use the eat values instead
of the actual at values, since the eat values is the percentage we were
trying to use.

Except, since actual at outputs is part of what determines the eat
values and the at values, ah... gee I don't know.  Both eat and and at
will include some effects of actual random flows vs target goals.

So, lets take the second and third columns in this test since they all
have real signals, and do a reverse calcuation of what the inputs would be
using real link at values, as precentaive flows, and see how the reverse
calcuations compare to the actual values of the input nodes!  RBM would
then take the reverse calcuation, and go forward one more time, to see if
it gets back to the same output, and train on the difference there.  Gee,
I hope I don't have to do that.  that would take so much compution work.
But there must be some importnat reason they are doing that!

I'm thinking of cheating and not trying to do all three steps and just
use two! or less. :)

But lets start with doing the calculation and printing it out.

----

Ok, tried to back calcuate at value using links and got the exact
same number I started with.  Didn't work as I execpted at all!

I think I must normalize the links at values as forward percentages
first? Then use those forward percentages, and renormalize backwards?
Ugh.

Yeah, so if two nodes have forward % flows of 10 80 and 80 10,
the back flow of that shared middle node which is 80 80, should
be 50 50. Right, and that won't automtically blance and get
back the same number we started with.

----

Ok, I think I calcuated the back AT bvalues correclty now. But the code
is so fucking complex the odds of making a mistake is created.  But
I have back values that are different than forward, but yet, they
sum to the exact same thing, so that means they might be correct.

So, what I want to do, is train w values so as to minimize how different
the back values are from the forward values! I think.

How do I do that?  If I sort X->A, I could then calcuate a back value for
X from all the forward links for X. Then what?  If back X is too high,
how would I need to change the S->A w value to get it closer together?
I guess if there are no other shraed nods, the back calc will exactly
match the forward calc so nothing would need to be adjusted.  That sounds
good.  Sort of.

But if we hare shared nodes, and our back calc is too high what do we
need to do?  I can't just adjust all w valued down beuase that has no
net effect.  I have to somethign complex like find which forard node,
is contibuting the most to the back flow, and then reduce that one and
increase the others?

Ah, or wait.  I can back calcuate the normalized percentages.  So I
have something like 20% forward, and 80% backwards!  And we want those
to be the same! I'm thinking since forward is 20% in effect, we need to
increase w.  And if forward is 80 and back is 20, we need to decrease w.

I need to draw out some exapmles and see if this is reall what is correct.
Or I could just code it that way, and see what it does.  :)

But I'm feeling the adjust speed should be relative to the differece
as well.  So if it's 20 forward and 80 back, we should adjust alot but
30 forward and 31 back, just adjust a little.

Ok, time for food, and tv, and then GO GAME!  Won't get into coding this
stuff yet.

Should print out forward and backward percentages for the column first
to get a feel for what they look like and how they drift over time.

Need to let the multiple runs I've got going cook some more.

[8:51 PM]

----

[12:14 AM]

AlphaGo vs Lee Sedol Game 5 has begun!

================================================================================

2016-03-15 11:16 AM Tuesday

Yesterday

1) Lots of thinking and experimenting with percpetion idea of trying to
train w values using backwards prediction.

2) Added display of total energy to see if energy learning is increasing
the total energy. It is. 2 without vs 5 with or something like that.
Sometimes a lot more like 1 to 15.  Not really happy with the results
however.  Like the idea of reverse precition better in theory, but having
a reall tough time trying to grasp how to implement it.

3) Wrote some test code for reverse prediction training.  Used link at
values as the sytems current sort percent, and turned the link at int a pf
(percentage forward) vairble.  Then using the verse process calcated pb
(percentage back) from the pf forward for each node. No clue if this is
correct or not.  Need better grasp of how this might work.

4) Watched final game 5 of AlphaGo vs Lee Sedol.  AlphaGo won making
the final score 4-1 for AlphaGo.

Today

Need to dig into idea of reverse prediction and see if I can find a
theory that makese sense to me.

----

[11:49 AM] Back from Starbucks run

So, theory.  the network takes inputs at level 1 and produces outputs.
And my goal is, given the outputs, reverse predict what the network
inputs were.  And the theory, is that the the more accurately I can
predict the inputs, the more information we have saved in the outputs,
about the inputs.

Yesterday, I was thinking of using the link at values as the networks
current policy.  And using those values, trying to reverse calcuate the
inputs. But I quickly discovered my thinking was weak, beuase if knew
the link at values, I could directly calcuated the the iputs, without
even using the outputs.  The link at values basically tell me what the
inputs were (they sum to the input value).

So using the link values is in effect cheating.  That's like looking at
the inputs to predict what the inputs are!

But after the link at values are combined together to form the outputs
information is lost.  And that's the information loss we want to train
w values to minimize.

But, problem.  If there are 8 inputs and 8 outputs, the w values that
are guaranteed to loose no information, is a 1 to 1 mapping.  And that's
not what we want either.  Though it would be useful once we start doing
reduction, and have 10 inputs and 9 outputs.

However, the ActivityTrace is already losing information. It's a lossy
memory of past pulse events.  So can we help the output layer capture as
much infroation about pulses as possible by correc w learning?  I think
that is what we need to do.

So we shouldn't just think of this is maximising our ability to predict
input AT values from output AT values, but rather predict intput PULSES,
from output AT values.

Ok, so we have a network and pulses flowing in.  HOw does the output AT
values predict the next pulse? I alread tried a form of this by saying
the low AT value was predicing the next pulse.  That's cool, but I don't
tyhink that's fully correct. Or as correct as it should be.

The output is a probablity distribution.  And if we reverse the output
though the link policy, we should get a reverse estiamtion of the
probablity of input pulses for the purvious layer at any time X.

Yes, and it's that reverse prediction, we should train the system to
maximie it's power to predict.  So though I was playing with at values
to show the networks current policy, is that right?  Or should we use
the networks last actual sortig policy?

It seems to me, the networks current actual sorting policy, based on the
networks current at outputs, is what is makig the systems current actual
prediction.  Yes, if we use link at we are getting a smeered history of
recent past events. And we don't want to use that.  We want to use the
output ATs as predictors of the intput probablity.  So the AT values
in a given row tell us the hisotry of what has happened.  And we want
are memory of history, to maximise it's power to predict what's abot
to happen NOW.  Becahse NOW is when we have to make a decision about
how to act?  Yes, I think if we are good a predicting what is happenign
NOW, it means we have a good represenation of the current state of the
envionent, and that is what we need.

This sounds like the correct theory.

So, if we wait until we receive a pulse, we will update the policy based
on current output AT values.  And we will then route a pulse.

We an then check to see if this network correct predicted the arriaveal
of this pulse.  That is, we can reward and punish w values, so as to
make the reverse prediction match the truth other time.

So, we are trying to test if the input NODE that received the pulse
was the right prediction!  Not if the link or the output node
that received the pulse was correct. So far, all we have been traning
is whether the right link was selected.  Which I guess is not so
wrong since that's what we are traning.

Ok, but we can't look at the policy and ask, did it predict which
link the node would follow?  Beuase the policy defines which link
the node was going to follow.  So the answer is of ocurse it did.
There is nothing to train that way.

But, we can predict in reverse I think.  If output node C gets the pulse
we can look backwards at it's reverse fan out, and ask whether the links
feeding to that pulse, correctly predicted where the pulse would arrive.

So for a fan out of 3 there were 3 prevous nodes that output node
could have received a pulse from, and we can ask whether we made
that prediction correctly!

Ok, so if we look at the reverse policy, we see a 80 10 10 sorting maybe,
and we can train the reverse policy to match the actual pulse flow!
So we sort forward, but train backwards!

So we receive a pulse from x, we incrase x, and decrease the other inputs.
So as to make the reverse flow sort ratio seek what is actually happening.

What would that do if that is all we used?  I'm thinking it won't work
on it's own.  But It's too complex at first thought for me to know
what will happen.  Will all pulses tend to flow to one pulse instead
of balance?  If we get a pulse then we incrase the links saying we want
more to fllow the same path.  But if we get a pulse on th eother path,
we way we want more form that other path.  The paths will fight each
other Ithink, and the result might be the balance we want.

No, I think as long as each input node has one output node that is the
"winner", it will just push all data to taht one output. If there are
fewer ouptuts than inputs then it's a game of musical charis the nodes
will fight over.  And in that cause, maybe that's exactly what we want
to happen? I did say that the correct solution to preserving maximal
information is to sort all input

But, we could add to this, flow blance leanring as well.  And make the
prediction maximising back learning, fight against the flow balancing
foward learning.

Ok,so this is fucking dead simple and sounding potentially really cool.

So let me code so back probablity learning, and see how the net responds!

More coffee first.  

----

[12:41 PM]

Coded it. testing it.  My heart is starting to pound because suddenly
the theroy feels so right that I'm hopping for a killer victory here!
The training is so slow!  Game here to write comments becuase I'm too
excited to watch it.  Of coruse, normally when I get excited like this,
all my joy is soon brought back crashing down to reality once I see
nothing works as I was expectig.

This traning backwards idea is an old one. I'm sure it's an idea i've
looked at many times in the past on some of my really old designs.
But I think I was always forced to give it up becuase it failed to do
what i was expectig it to.

But I think this time could be different. Bucause now I have the *a
policy and the activity trace for temporal history, as well as descret
pulse events, Instead of just a w value controling flow.  So where it
sounded good, but didn't work with a simple spatial network becaues
without reduction I think with the combiatnion of features I have now,
it could work.

I'm calling this new traing rule information maximising.  I've got it
turned on with the flow blance as well, becuase withotu flow blance, it
will just sort all to a single output.  I think. Need to test to verify.

Then I will need to test reduction, without flow balancing to see what
that does!

As I was coding, I realized I might have been sloppy on my use of
len(links) and have assumed in places that all links are the same
count thoughout the network, so if I switch to a complex network that
does reduction, some of my code may nbeed to be fixed. 

----

Seems to be working!  good or bad? Don't know yet.

Started a second run with info max only to see what happens.

I have 4 inputs, four dead inputs, and 8 wide network.  So I expect most
the signals to pass through. But the network won't let w drop to zero
-- it's capeed at 0.0001 or something so that RL can always win over
flow balance.

Hum, maybe flow balance should be capped in it's power and infomation
max not capped? Do different w values for each?  And infomation
balance does 0 to 1, but flow is capped at .25 to .75 or something?
Intersting possiblities.

Maybe information maximising won't fight with RL as much as flow
balance does?

Ok, need to start a test with larger fan out and see what that does
as well.  Using fanout of 3 in current test.

----

[1:45 PM]

It's working very well as far as I can tell.  This is very cool.

The tests with no balancing are doing as expected. Sort of.  They don't
just pass through the 4 signals, but they form 4 new signal and flatline
the rest.  But the weights are slowly drifting and it will take some time
for them to stabalize.  Or maybe they won't stablize and keep drifiting.
I have to give it pleny of time to run to see how it ends up.

The test with the balacing seems to be drifting more.  I think there's
an inherent fight problem between balacing and info maximising that might
have trouble finding a stable state.  Or it might just take a lot longer
to stabalize?

But this is looking very very cool.  It all seems correct and it's the
trivally simple update rule I was looking for!

So I have to do more testing to see if the signal stablize.  On the
signals I hve for eample, is there one simple solution that's the best
merging possible a deeper net would find and stick with?  Or will each
layer by a slightly different version of the signals?

The signals I'm using are highly correated and not indepdent signal
sources. So we must wonder what the correct infomation maximising solution
might even be for these signals?  So I'll need to generate some other
signals with different characteristics and see what that does in time.

And maybe the flow blance needs to be a different parmaetr from the
informatoin balnce like I suggested above, instead of using w for both?

And mayne one needs to dominate over the other by using faster learning?
So I'll need to experiment with relative learning speed for the two.
And I have 3? different versions of flow blancing to pick from that has
effects.  The only punish sorts to high I'm using at the moment doesn't
train every sort.  It only trains sorts to high. Beucase of that it has
less innereht traning power than the information max training.  As such
the balance might be wrong currently.  Switching to flow balancing that
trains with ever sort might produce more controlled results when we try
to share a single w value for both?  or maybe this is just a good rewason
to put the two traning rules into different weights so they don't fight
each other in their incremental updates?

Or maybe I'm just being silly and doing them separate or togher might
alway produce the same result anyway?

Maybe not however.  Becuase it's a train towards 0 or towars 1, if the
weight is close to one, the train to one has little effect.  So if due
to the other training it was at .5, then the flow training  would have
different effect on the result maybe?  I'll have to look more carefully
at the math and see if two vars is different or the same than one.  Or,
I could quickly code it as two and see what happens.
important effect.

But this path is feeling really good.

So goodin fact I'm going to delete out the crap I coded yesterday using
link at flows as percentages.

----

[7:03 PM]

So, my tests are showing that this is not working as well as I was
thinking it was.

The infomax only learning on a fan out of 3 seemed to do the cool thing
of staring with 4 signals in an 8 wide network, and ending up with only
4 signals at the last layer.  But in fact, all that was happening, was
that infomax was forcing all the signal to one output in the fan out
of 3 for each node. There might have been coolness at work to make ti
alternate every row with signal and flatline but basically, not nearly
as cool as I was thinking.  was shared with three nodes

And the total cross connect version, is turning the signals into flat
line by adding everything together.

I wondered how forceExplortion was effect this so I turned it off and
allowed w values to go to zero or 1.  Now the netwrk has gotten into
feedback ossocations much like the energy learning causing a node to
uch all data to do it for a wimte, then have all data sent elsehwere
for a time.

Uhg.

However, I stll have high hopes this is the right approach.  I think
it just has to be tuned correctly. And I need to test it in a reduction
configration where it should work the best to see what happens.  It should
work but may suffer stablity problems where the solution endlessly drifts
from one anasewr to another.

And in a non reduction use, there must be some balance learning at work
at the same time.  But that's not working on my last tests.  It's got
osscilations growing out of control. So I have to figure out the
best way to balance the flow while also doing the info max learning.
How the balance updates work might be key to playing well with the
infoMax learning.  Might need a pull to zero like effect at work that
limits how far the infomax can push the imbalance.

Going back to using 0.0001 as the bottom limit might be important
in limiting the info max from oscilating out of control?  Seems like
that's not the correct way to limit it however.

================================================================================

2016-03-16 9:13 AM Wednesday

Yesterday

1) Wrote infomax code to adjust backwards fanout w values based on the
idea of the node predicting the odds of where the pulses come from.
Really excited.  This is the simple solution I was looking for!

2) Discovered it wasn't working as well as expected.  Excitement dampened.
But still hopeful.  Some tests just seemed to want to flatline instead
of preserving information.

3) Turned off the force exploration to see if that was interfering with
infomax.  That made infomax worse and caused it to osocilate wildy in
some cases and flatline in others.  Uncovered an issue that if the at
start at zero, and nothing is forceing pulses to route to them the *a
of 0 prevents any routing to the pulses.  Hacked in a non zero starting
value of the at values.  That's probably wrong.  If a zero value causes
issues, there's a problem with the routing logic.

4) More general testing of infomax w learning in different configurations
to get a better undrstanding of it.

Today

Got to get my sleep back on schedule now that the midnight Go games
are over!

Had some thoughts while laying in bed awake again.

This infomax code really is what's needed for reduction. That is,
the problem of taking N input signals, and reducing it down to <
N signals.  When we have to throw data way like that, we want to keep
as much information abot data as possible, and this idea of prediction
is exactly how that is best done I beleive.  I'm quite sure I'm on the
right track here dispite the issues showing up in testing.

But them I rememberd why I headed back into the percpetion reserach side
of the problem.  I wanted to make the system solve the decoing problem,
where a binary coded state input, would fan out to a sparse decoded state
signals to allow me to map 00 input to one output, 01 input to another,
10 to a thrid, and 11 to a forth.

This isn't a data reduction problem.   It's just the opssosit. It's a
data expansion problem.  This info max code only works under the case
of data reduction.  Under data expanstion, it will just try to send all
data to one location.

Infomax Doesn't solve the decoding problem!

So, that is why we need more. Hense, we also need data decoding/expansion
operating.

So, I'm thinking we need algoithms working together to solve the
percpetion problem.  We need infomax to solve the data maximising problem
which is highly important in reduciton.  But we need a data expansion
learning algoithm at work as well that sovles the decoding probnlem.
Or atleast, when combined with informax solves the decoding problem.

I don't think I've seen any signs of the current system solving the
decoding problem.  Which is what I was looking at to start with here.

So I've been using this flow balncing stuff so far. And that's one
way to attack the expansion problem.  So maybe the anwer all lies
in the correct combinatin of the infomax and flow balancing. Which 
is what I was already trying.

But my new insight as to why two algoirthms are needed seems helpful.
This insight that one approach is needed for expansion of signals and
another for reduction.

So thinking fromn this idea, is flow balancing the correct algorithm?
Is it maybe solving the decodin gproblem and I didn't realize it?

Or is it not the right algorithm, and I need something similar, but
different?

That will be the reserach direction for the day. Think about the decoding
pronlem and how to solve it with these tools.

Morning starbucks run time.

----

[11:22 AM]  Back from Starbucks. Ansered a few emails, sadkly let myself
get sucked into facebook a bit.  Gota stop ding that...

So, decoding.  How the fuck will this work in this current framework.
So, we have two signals alternating back and forth in pulse rate,
cycleing through the 4 binary combinations possible. That's what I've
been playing with this this TwoNodeOverlapTest I've been playing with
for testing percpetion.

If we fan out to 4 outputs from 2, I want these outputs to represent what
the state of the evniroment is.  This is the temporal problem where we
need this past history to decode this in a meaninful way.

And it's our internal "hidden layer" activitytrace that gives us this
temporal history to work with.  So in theory, we need something like the
two signals combing together in an AND fashion to create the 4th outputs.
And we need A not B, and B not A, and not A not B for the zero output.

How can my current network even be configured to do this manually? (let
alone learn it on it's own).

A+B is easy.

Yeah so if we do 0A+1B we get one signal.  And we do 1/2A+1/2B we get
the AND signal.  And if we do 1A+0B we get the third signal.

When A=0 and B=1 will the 0A+1B node be able to overpower the 1/2A+1/2B
node?

Wow.  The *a part of the algorithrm might be an issue here.

if A is zero, (ok it should never reach zero but lets ignore that) and
pretend it has. Then 1/2A+1/2B will always sort to the 11 node. It won't
allow the system to switch to the 01 node.

No wait, I'm confused. My notation has confused me.  The 11 node will
get 1/2A 1/2B.  the 01 node will get 1B.  So when A turns off, and the
11 node is fully active, we want the system to switch over to the 01 node.

the 01 node is getting a total vote of 1 where as the 11 node is getting
a vote of 1/2, and that we would hope will lead the system to flip from
the 11 to the 01.  But will it?

And we certainly have a problem. There's no wiring for the 00 node
that will work. We have no ability to create a true negative negative.
I could argue the system is blind to negatives, (it can't route a pulse
that doens't exist), and maybe that's the true answer?  Which is why I
felt the need to add the extra flow blancing input.  Which should mean
the system could create the 4th output.  That might be fine.

So with only two inputs alternating between 3 combinations of 01, 10,
and 11, the system should be able to fan out to 3 outputs correctly.
But asking it to fan out to 4 would produce slightly strange results.

Ok, so if the At were never actually zero.  When it switches to the 01 vs
11 config, the 01 node was getting twice the "vote" for pulses.  So if 01
has a value of 1, and 11 has a value of 10, then the flow volume would
be 1 to 10, for equal "votes".  And the *a term would keep the flow
fixed at that balance. Maybe.

This is tricky.  Since AT is not a true measure of flow volume, this
coulde be a hidden issue in all this I didn't realize existed.

The idea is that this should be implementing the product of two probablity
distributions.  The w values (and the other fudge factors) represent
a current probablity "measurment". The AT values reprsent the systems
estimate of current state.   When then use the current distriution
to "update" the state.  So if the current distribution was 1:10 (aka
normalized bvack down to sum to 1 in theory), and our currentupdate was
50/50, what is the correct next state?  We mlutiple both by .5 and then
renormalize, which should mean, no change at all.

So if the AT output is 1 vs 10, and w values are .5 and .5, then the
system should be sorting pulses to keep the AT the same!  So it should
sort at 1:10.  But we have many problems here.  First I don't think this
works because of how the activity trace works.  A sort ratio of 1 to 10
does not produce an exact 1:10 AT ratio.

So what DOES it do? Tend to sort TOO many to 10 and blow up?

OR tend to sort too few to 10, and decay into an equal sort?

Due to the odd way of how the AT works, which way it goes might be a
function of whether we are operating in the mode of more than 1 pulse
per half life or less than one pulse per half life.

But if this doesn't work correctly, then we might have an innage problem
with the whole idea of how things should work.  So I need to once agaisn
stop what I'm doing and test how the AT works in this way.

So I can create two AT.  Set the values at 1 to 10.  Send it one constant
pulse stream of pulses and randomly pick between the two, based on the
value difference and see how the AT values drift.

But, before I get into this.

Lets assume they implement the probablity multiplication function we
want and think this through.

So, in effect, a pulse input is logically the same as an input vector
of 000010000 where the 1 is for the pulse we receive.

Then the probablity distrutuion output of this, is the product of the
numbers time the w values -- which means we ingore all w values except
for the one fan out we have.  And we could think of this as an input
with values other than 0 and 1 working the same way as relative measurs
of the strength of each input.  Which would then get multipled out by
the fan out w values, to form a total probablity distribution of all
the sum(input*w) values.  Then we multiply, and re-normalize.  Right?
Yes, to continous probablity distriutions are just mlitplied number by
bnumber to form the new one -- then renormalized back to where the area
under the curve is 1 -- ot the sum of the discrete distibution is 1.

But we never need to normlaize to make the math work.

But our AT decay over time.  How does that fit in here?  I though
I had an answer to that once?

No input, is like having no new information.  And that's could maybe be
seen as a 50/50 iput -- or a flat distriution But if you multipy any
thing by a flat distribution, the old distribution remains unchanged.
So that doesn't explain decay.

Abstractly, the older the information gets the lest we can trust
it. So decay makes sense in that way.  We are less sure of it the more
time passes.

But that aside.

If we have one pulse input, we know three new w values, but the rest
remain unknown. So we update what we know (in theory).

Ok, so this all works by stochastic sort of indivdual pulses, and the
number of pulses sorted per time, is the "value" of the output.  So that's
why it decays -- not getting pulses mean the value is decling RELATIVE TO
the ones that are getting pulses.  So it's sort of like the normalizing
process to keep it all summed to zero (but in fact just a trick to keep
it from summing to infinity and a trick in how we slowly forget about
the past).  If we sum without decay we remember everything from the
point we startes summing.  But we don't do that, we forget the past.

Ok, so the forward looking fan out w values, represent the odds of a
given input pulse, being supporting evidence of the truth of one of the
three fan out nodes. So a B input is stong evidence of the world
being in the 01 state, and weaker evidence of the world being in
the 11 state.

Ah, but look. This would assume the input states sum to the same
value!

So the input states would need to be 01 vs 10 vs .5 .5.

So when the world is in that 11 state, B only happens half as often!
And when the world is in the 01 state, B happes twice as often.

So when we receive a B, what does it tell us about the state of
the environment?  It tells us with a strength of 1, that the world
might be in the 01 state, and with a strength of 1/2 that the world
is in the 11 state!  So the w values should reflect this to the
outputs!

But if we didn't flow balance each state like that, when we would
find something different.  Receiving a B pulse, would be equal
evidence of the world being in 01 or 11 states!  So the w values
would need to be .5 .5 to reflect that (but 0 for the 10 output).
So 0 .5 .5. 

Oh, so I sense where this is going!

When we route the pulse from B to the 01 node, we have made the stochastic
DECISION (vote), that the world is in the 01 state!  So just as we are
doing in the reverse fanout approach we could do in the forward fan
out approach.  We can train the w values to match how we voted!  So that
if if 50% of the time the world is in the 01 state when we receive a B
pulse and 50% of the time it's in the 11 state, we should train the w
values to reflect that. So when we route to x, we reward x and punish
the rest. That makes the w value take on the values that match the true
correlation of states to evidence.

But, shit, does that work? It feels like it does just the wrong thing.
It feel like it does the same as the inverse traning in that it pushs
all pulses to the same output.

But yet, I'm hopeful here. I think if we think of both of these udpates
as one problem, the correct answer will raise it's head.

We want the nodes, to be good predictors of pulses, but the pulses to
be good predictors of nodes as well.

So if node 11 is active, it's backward probablity, should be a good
predictor of what pulses are showing up (eqal probablity of A and
B showign up)., and that when pulse A shows up, it's forward fan out
should be a good precictor of what state the world is in.

And of course, this is all about defining the hidden states. And the trick
is all hidden in how we define the hidden states from the intput data.
So what happens when we just randomly pick w values and define the hidden
states randomly like that?

With one pulse input, the output fan out becomes a self fulfilling
prophecy does it not?  We sort 10% one way, 90% the other, and then
every node that shows up is a valid predictor of the outptus!  By self
fulfilling prophecy.

So this all gets resolved, by how the different node fan outs interact
with each other.

Anything we set for the forward values, is correct by self fulfilling
prophyicy. As long as the data from other directions is equal and random
and uncorrelated.

Ok, so yes, if the data to the right, is 1 pps, random, and the data to
the left is 1 poos random, and I set my nodes as 1:2, then nodes will
suck twice as many of the pulses on the right as on the left, and my
blance will remain 1:2 and continue to be a self fulfilling prophecy.

But if the data is not equal, or not random, that won't happen.

sIf the flow on the left is twice as strong, and I set a 1:2 ratio. The
nodes will actually end up with a 1:1 ratio.  No, not actually. Something
more complex than that. Because my sort will be biased by the *a term as
well so i will send more than 1 to the left, and less than 2 to the right.

Shit, so lets try to look at the math and see where the system would
blance in that case.  I'm going to use paper.

So inputs are A B C, two output nodes are X Y, B sorts to X and Y with
a 1:2 ratio (w values).

So the output levels are

X = BX/(X+2Y) + A

Y = B2Y/(X+2Y) + C

Solve for X and Y.  Shit, I think I don't care.

But, the end result, will be that even though w is 1:2 the true sort
ratio will not be 1:2.  It will be something wigher than 1 on the left
and lower than 2 on the right.

Ok, so if we adjust w to be what is actually happening, we will start to
rise the 1 up, and lower the 2 down. Will it equlize at the true value?
I guess it must.  Becuse it will equlize at the value where w matches
what we are actually sorting, and when the average long term values of
the outputs, are balanced!  So we blance the otuputs by adjusting the
w values to match what what we are actually doing?

Shit, that is unexpected if this is true.  Let me check this one case.
We have 2 from A, 1 from B, and 1 from C.  That sums to 4, so to balance
the outputs we need 2 out each.  That will require us to to send all
of output B to Y with a sort ratio of 0:1.  So yeah, if w is 0:1 it
would make the two output values be equal, so wait, no. This is wrong.
The forward w values do not represent what state the system is in!
It's in a 50:50 state, but we are sending output to 0:1. So the 2w values
are not predicting the actual state of the system.  So this is wrong.

Well, 0:1 is wrong. That's not where it would balance. Ok, so I was
thinking incorrectly about that.

If we start with these same inputs pulse flows, and start with the w
values at 1:1, we would send far more pulses to X.  So we sould start
raising that value, and lowering the w to the other path.  And shit,
that would just force us to send more and more to X.  That would never
balance in any sense. Well, it would blance in the sense that the ratio
would become 1:0, but then it would fail to correctly represent the
outputs, which would be shoing 3:1 at that point.

Shit, no wrong again.  How could we get from 50:50 to 100:0 without
passing through 3:1, akd .75:.25 Ok the system will balance at some
point in this case at some level less than 100:0.

But the point is, we would be sorting pulses the wrong way to create
this balance.  We would be sending MORE to the X when we would want to
send less, if the goal was to balance the output levels of X and Y.

Ok, so trying to set the forward w values to match what the sort is doing,
would result in the sort being less balanced, not more. I think.  So,
when we adjust w to blance the outputs, we end up pushing the w values
away from what the forward sort is doing. I guess.  Maybe.

Wait, I'm not trying to set w to match what the sort is doing. I'm
trying to set w to predict what the outputs are!  I'm getting lost here.
I don't care what the actual sort ratio is. I want the w values to be
good predictors of what the outputs are every time a pulse shows up to
be sorted! I want the w values to be good predcitors of what STATE the
world is in, where the outputs reprsent the odds of the world being in
that state.

Ah, but when it's all set correctly, the output will be in that state
when pulses show up on average.  So the real output sort ratio should,
match both the w values and the true long term sort ratios.

Ok, but wait. Something is totally wrong with this idea.  The outputs
can't be in a 2:1 ratio, AND have the w values be 2:1, AND have the real
sort ratio be 2:1.  Pick two, out of three, ou can't have it all.

If the outputs are 2:1 and w is 2:1 then the sort ratio will be 4:1.

So what the fuck DO we want? Should we want?

We can train w to match the output level ratio.  But them we will end
up with a sort that's thrown way to the high side.  That seems all wrong
and pointless.

Ok, so, with no interfence from the side, we can set w to anthing we want,
and the sort ratio will NOT match the output!

Shit, I thought this was a self fulling prophecy but I totally forgot
about the self feedback of the *a term.

Ok, but what I was saying, is that the output 1:2, nd w is 1:1, then
the output should stay constant.  The sort will match the output and
keep the output the same (if ATs work as we want in theory).

So w values don't mean "this is the output I want or expect".  It means,
this is how much my evidnece wil bias the output???

So w values of 1:2 mean on average the pulse is evidence that 2 is lower
than it should be and 1 is higher than it should be?  So the pulse is
evidnece of how the world is changing vs what "state" the world is in?

This doens't seem consistent with what my intuition tells me about
probablity distributions. Something about my intuition and undersanding
of probablity distriutions seems off here.

So, a pulse B says 2:1 it means the odds of the world being in the 2:
state is twice as likey as being in the 1 state.  when pulse B happens.
So I take the current estimate of what state it's in, multiple by this
evidence, and renormalize. So a starting state of 1:1 turns
into 2:1.  If the evidence shows up again, then 2:1 becomes 4:1. So
it keeps pushing it father and farther towards 100:0. Butnever gets
there.  Ok, so this does accumulate.

So if the sort ratio is 2:1, and I send to two static nodes, it should
slowly throw all pulses to one node! Ok, I see that. And I think that
is what will happen due to the *a sorting term.

What needsto be tested, is of w is 1:1, will the nodes stay at their
same value or blow up or decay to 1:1?

Ok, but w values are not actually self fulfilling profecies I don't think.
Maybe I was all wrong about that.

----

Walk around and think. Snack. Answer and email or two.

----

So if a probablity distriution is telling us some truth about the world.
And we have evidence that allows us to update the distriution, what
exctly are we doing? How do we determine with the conditional probablity
of the evidence is?

So the ground is wet, that's the evidence, what is the odds that it has
rained? Or, their are dark clouds, what evidence does that give us about
the odds of it raning?

We caluate the odds of raning, by collecting data about clouds and rain,
and calculating the odds.

But we are now trying to set the w value, to define a property of the
uvmierse, we can't measure (the hidden variables).

There must be well establised math behind this already.

Certainly this is related to PCA ICA and Factor Anaysys.  But those
are offten a bit different becuaes they have as goals needs like data
reduction to reduce N dimaions of data down to <N.

The question I must answer, is what is my need here?  And knowing the
need, how to do get the netowrk to create the nodes I need?

So, one need, is that I have N nodes, and I must map my data, to that
N nodes.  Whether that's data reduction, or data expansion. And I think
the other need is that I must represent as much information about the
sensory data as I can -- I must compress as much inforation about the
temporal historic sensory data, into those N nodes, as I can. So when
I do data expansion, It's still really a form of data reduction, becuse
I'm taking information from some long past hsitory of data, and cramming
it all into these N nodes.

So it's always a data reduction problem -- at least when we are working
with real world data where the information content is in effect infinite.

So this infomax approach is most certainly headed in the rigth direction.

Ah, but here's and idea.  I don't just need to maximize how much data is
contained in one node -- which is what this back training of weights is
doing I think. I need to maximize how much information is contained in
the the set -- which means I must remove correlations between the nodes.
If one node is able to predict the value of another node, then there is
information redundancy and loss.

So, I need to maximie the information IN the node, from the data and
at the same time, prevent cross contamination of infomation.  I have to
make all the nodes independet.

So there's is probabaly two different traning rules to put to work here.
And the backwoard rule is maybe maximizing the content but the forward
training rule, will maximie the independence? 

So the forward w learning rule will be to adjust w values to factor
out any cross correlation. So what value of w, in the forward sense,
do we need, to do that?

In the forward direction, I want to make the pulse UNABLE to predict
which of the output nodes, is active! Ah, so in truth, the pulse does
provide evidence for what state the world is in, but in the forward
direction, I need to adjust w values, so that pulse*w factors out
the prediction?

So I think this might mean, I need to adjust w, so that after
adjusting by w, I end up with an EUAUL SORT!  That is, my pulse
is equally as valuable for suppoting X, as it is for Y, after
I adjust it by w.

So if output X is twice as likely to be true as output Y, I need
to make w seek the value of 1/2 for X, and 1/1 for Y!

This feels like it's sort of a normalizig rule.  That is, how much
support I give to X and Y, must be equlized so that X and Y
become equalized (normalized).

Ok, so fuck, this is simple if this is true. In the forward
direction, I train w to make the pulse flow equal.  NOT THE OUTPUT
value equal! ???

No, wait, if this is a normalizin effect, it is an output balancing
problem.  So damn, It needs to be exactly what it was.  When
a pulse shows up, I want the odds of X and Y being equally true.
Ah, no, that's wrong!  We never expect the incoming pule to NOT
be correlted to the outputs!

What we must do, is adust the w values, to REMOVE the correlation!

So output X/w and Y/y should be equal!

So, in the forward diredtion, we don't have to worry about which
way we sort it. But we must tain the w values, so they normalize
the odds of correlation.

So if w is 1:1, but due to outside influce, the first one is
higher in value, then we must reduce w.  But here's what I
have nOT beeen doing! We must adjust it so that At/w is
equal!  So if X/w is equal to Y/w, the w values are set
correctl, and we end up sorting equally.

But maybe, what I'm alrady doing is the same thing???

If I sort to X, I can just look at X and w leading to X.

No, I can't!

Becuase the goal we sort towards, is when B shows up, the outgoing sort
probablity is equal.  NO NO> WAIt.

When B shows up, the outoing sort policy, will on average make the
outputs equally likey to be true?

dman, I'm close, but I'm still lost.

Back to basics. We want X and Y to be statistically indepndent of each
o ther.  But we don't expect them to be statistically indepdent of input
pulse B.

So we adjust w to make them stistically indepdent of each other.s

Ok, maybe this means, WHEN I CHOOSE TO SORT ...

WAIT -- something I did just merged two xterms into one window!
How the fuck???  Ok, no clue, back to what I was doing..

When i sort to output X, the odds of it being true, is equally likely
to when I sort to output Y?

So, if X is twice as likely to be true, when B shows up, as Y, Then I need
to sort to X, twice as often.  But the w value will not be 2:1. It will
be whathever value is needed to make me sort in that ratio.  Maybe if
the desired ratio is 2:1 the w values needed to make that happen are
the sqrt(2):sqrt(1)???

Ok, maybe this path.... So when the outputs are workign correctly,
Y will be twice as large on average as X, when the pulse shows up.
And I want to sort to y twice as often as x when this happens.  Ah,
but then I want the w values to be 1:1 to make that happen.

Oh, eat me.  Could it be that simple?  That the training I need to do
is to try and push the values to 1:1?  Shit, I think it might be!

So if the input nodoe to the left is 1:2 towards me. And the one to the
right is 2:1 toward me. That makes me 2:2 or an equal sort.  And if the
corelations are all balanced, I want to train to keep it at 2:2 make w
seek equal values.

So, it's not flow I need to equalize. It's not output I need to
equalize. But it's fucking w values I need to equLize? That's what this
is telling me???

Well, that's so simple to code I feel the need to try it.  Calcuate an
average of the fan out w values, and push them all towards the average.

That at least seems to have the power to balance against the info max
which is trying to push things out of balance.

Shit, time to stop thinking and code to see what that does.

----

[4:28 PM]

Well SHIT. Found I coded info max wrong yesterday.  Had a bug in th code
comparing the link index with the link.  I was decrementing w towarze
zero but never incrementing it. It never has been working anything
like intended!

Shit. Testing to see what it actuall does now!

I have no idea what it was doing before!

----

I get the feeling I've got the right ideas at work here, but still got
the code totally wrong.  Both updates need to minimize correlations and
minimize mutual information.  I feel like both updates need to be doing
the same thing, but just backwards from each other or somethking.

The backwards learning is trying to adjust w to represent the actual
flows.  Is that correct?  Should the forwrad learning be doing to
the same?

Backwards learning needs to make correlated data suck together.
Or does it?

The *a term makes correlated data suck together on it's own already.

Forward learning needs to create some sort of balancing effect. But
how I'm not 100% sure yet.  And backwards learning needs to create
an unbalancing effect so the two balance each other.

if w values are all equal, we can look at this an understand what
is wrong.

If A and B inputs have strong correlation, then they will sort to
the common X output.  But they will sort TOO much.  So the
balancing learning needs to offset that.

So then waht does the backwards learning do?

----

Oh mother of God.

I had a bug in the new balancing code as well.  Instead of adjusting
all the links, it was only adjusting the first link.

----

[9:33 PM]

Went to movie.

So, all my code wasn't working.  How absurd.

I still really like this idea of what I was trying to do, but I'm
sure what I've done is just wrong.  I need to keep grasping at what is
correct here.

After fixing the code, the fan out 3 version sends most pulses to the
right and left in the first layer, where they max with the data from the
signal 2 to the right and 2 to the left.  Only a small fraction like 2%
gets sent straight, which mixes with nothing since there is no data on'
the node to the right and left.

But why is this?  If the receiving node is only getting pulses from one
source, shouldn't it keep increasing it towards 100%?  I guess it is
trying, but the balancing code is winning by keeping the w value small.
Oh, and the balancing code runs for every pulse, and 1% of them go
forward, so it's only pulled up 1% of the time.  Yeah, so of course
the balncing code is winning.  But it's not balancing the flow in any
sense with this pull to the same value code.  All three w valaues are
very close togerher, like 32 34 32, and the center one is the largest
$at yet only 1% of the data flows to the center.  That is becuase
the data sent to the other two side nodes from the other inputs
dominates.

So, the balancing code isn't balancing.  Or, the reverse fan out code
isn't trying hard enough. :)

So let me attack this again.  What should both be doing?

The reverse code, is balancing what flows to one node, so it controls
what the the defintion of the node is. It defines the formual for
the single output.

The forward code controls the realtive flow to the neighboring nodes.
It has the power to balance overall activity to all output nodes.
But it also has the power to try and regulate the correlation between
the output nodes.

-

If all signals are equal in volume, frequency distribution, but random
and 0 corelation, what shold the system do?  I think it should mix
them all equally and randomly.  There should be no prefer3ece for
flow to one direction vs another.  All weights should be equal..

If two signals are nearly prefectly correlated in volume changes (the
AT values are in sync, they shold be strongly combined together.  But
the *a feature will make that happen on it's own without w adjustments.

Which means oddly, that when signals are postively correlated, we need
to adjust w away from the correlation to compesate.  So if one output
path is getting too much activity, we must adjust way from it.  That
can just be done with the simple activity balancing we are already
doing.  High output is pushed down.  Or low output is pushed up.

May thse two adjustments need to push in opposit directions. So if
the forward adjust pushes the high down, the backward adjust needs
to push the low up?

Ah but the forward adjust is likey to be done before the sort. The
fact that it has received a pulse is the reason it's time to do
that adjust.

The backward adjust is always done after the sort has received the
pulse.  Well, which is really the same time for all the center
nodes.  So a center network node receives a pulse, and it
adjusts w values backwards, and w values forwards. :)

The forward adjust, which I think needs to balance (the backwards adjust
can't do it), can push the high down, or the low up.  So the inverse of
this, could push up or down.

Here's a thought.  If the forward sort is just adjustig to equalize output
volume, and we have a circular net, there will be many different network
configuations that would balance flow.  If I push more the the roght,
the node text tome could poush more to it's right to adjust, and so on,
until the net on my left pushes more to me an dit's all in balance again.

So if we had another backwards sorting balancing some other factor
the network could find a stable configuation even if it were circular,
or if it was a full cross connect layer.

The backwards sort realy does have to adjust w values to make the input
nodes relative values match the propability of assocation I think.
If ouputs were 1 and zero, instead of of this continus value this would
be easier.  If it's 1, we increase the input nodes that are active,
and our our output is zer, we decrease the input nodes that are active,
so that the w value becomes a probablity of the output being active,
when the input is active.

----

Back learning could pull the w up relative how strong the node is.
This gives a greater weight to the pulses that show up the most when
it's in it's highest state.  So back learning in that case would only
pulls ups the link that was used to feed it.

Forward learning would need to pull w valeus down.  It could pull
down, the link with the highest output.

Fuck, lets just try that and see what happens.

----

[11:49 PM]

Played a lot with w learning code. The above idea wasn't panning out.
The pull up for sort was too dominating, in most everything I tried.

I'm doing simple balance on forward now by pushing down highest output
activity and pusing up all others.  Trying to blance high activity of
nodes this way.

And on backlinks, I'm doing the infomax. I pull up the choosen sort,
and push down the others.  The idea is still to give priority to the
link that delivers the most pulses.  Which will be correlated wiht the
link that is most correlated to the output when it's high.

Switched inputs to only two signals, that cycle though a 12 21 22
configuration.  What I'm looking to try and make this sucker create is
output waveforms that look something like:

____--____--____--____--

__--____--____--____--__

--____--____--____--____

So pulsed waves that mark the input state.

Maybe I will quit trying to grasp info maximising in this sort of
network and just go for forcing this sort of pulse wave system.

---

Oh, turned off distanceBiase while testing these w learning
algorithms.

----

So that's tomorrow work.  Figure out how to make this network
create those state signals.  Can this design even do that?

how? I need temporal pattern detectors and I thought this
network had what would be needed.  So job one is fitgure
if the network can do it. Job two, is to figure out how
to make the w learning updates make it happen.

================================================================================

2016-03-17 8:18 AM Thursday

Yesterday

1) Discovered bug in previous days infomax code.  It never was working
at all! Would never increment any w values -- was only decrementing
everything all the time!  WTF!  Disovered bug in new balance code I
wrote as well.  All the testing was for nothing!

2) Tried another handful of varitions of matching a forward blancing with
backward infoMax ideas.  None really worked very well.  The bacward code
would just end up offseting what the balancing was trying to do causing
the system to always be out of balance.

Today

Thinking that everything I was trying to get the backward code to do, was
likely already being done by the system using the balancing code alone.
The *a term of the sort does all the work.  If a path is the prime
contibutor to an output, then the output is automatically high when
it's receiving pulses from that path -- which means it's value rises
which cretaes postivfe feedback to suck more of the same pulses to that
same output.

I'm thinking as of laste last night that my *A code did all the work and
by trying to add more postive feedbcak, I'm just breaking the ability
of the blancing code to work.

So I already had two forces balacing against each other.  But the *a was
the real force, and the only w learning I needed was to forward balance.

This also calls into question my later attempts to do different versions
of the balancing in an attemp to do input pulse "prediction" so as to
maximize information.

I think I might have had it perfect before I started trying to improve
it all!

I'm going to do the simple thing again. Which I think is the same as my
orgianl long ago w learning algoirthm.  Ignore the sort, and just balance
from the top.  Equlize the top of every signal by lowering w for the
highest output and raise w for all the others.  Lets see what sort of
"mud" problem this approach has.

I think all the non-mud effect are actually being addressed by the magical
*a term.  I wonder of other effects like force exploration might have
been muddying the waters?

So, need to test the simpler solution and see again what it really does.
But also, want to attack the probnlem from the idea that I want the
system to create independent state vairbals where each output takes a
turn at being high and the rest are low.   The *a with a large fan out
might already be doing this.

----

[9:37 PM]

Back from Starbucks run.

The balance rule I was using yesterday doesn't work.  Which was punish
high, reward all low.  The problem is that two high outputs would trade
off being high, with each one being punished, and then rewarded when the
other one was punished.  Alternating punish and reward stablizes at 50/50.
The week output would go to 1.0 with constant rewards but relative to 50
50 that was not enough to push flow from the two high outputs to the low.

But even without working in one layer, over the course of multiple layers,
it does in the end get flow blanced I think.  Maybe. Sort of?

I could switch to double punish high, and half reard load due to the 3
to 1 ratio but that would not fix it either.  It would set the outputs
to .33 instead of .5.  But .33 vs 1.0 is still not enough difference to
force flow to the weak output.  This style of update sets the w value to
represent how often the node is not high. But that value, is not always
enough to force flow to the low.  To force flow to the low, the system
must keep pushing the high down and the low up until it balances.

I've got a test running using punish high reward low.  Almost the same
I did in the past, but in the past, I only updated when a pulse was
sorted to high.  This new one learns with every pulse that shows up.
This I think is guranteed to make ever output blance as long as blancing
is possible (which is not always true due to flows from othr sides
being stronger).  So a weak input, say with average flow of 1, next to
a strong input, with average flow of 10, can't be balanced even if the
weak input pushs all output away from the strong.  But what will happen is
that in time, the signals spread out over the net until tey are blanced,
and then they do mix together.

Cool thing is that with larger fan outs, the net does naturally create
these nodes that represent different states of the input signal. So I
think this genral approach I've used all along, has in fact been working
as I desired.

But, I have to check what happens with full fan out.  There is the issue
that I don't want the net to just mix all the signals to mud, but I think
the *a term saves us.  The net will natrually fall into tendency for a
signal to dominate a given output becuaes that signal correlates with
itself.  So I have to test what happens with deeper nets and see if it
heads towards mud in the long run or if the system stays full of signals.

I think the force exploraion could be pushing the system towards mud in
the long term. Maybe.

And I have to experment with signals that fan out to many layers, then
reduce back down to a very few.  If I expand to 20 wied, and reduce back
down to two, what do the 2 signals end up looking like???

------

It looks like most of these are doing about the same things.  Though the
waveforms look a little different on all, the results are highly similar
in nature.

Looks like they all suffer from stablity issues however.  I think
this design suffer3es the problem that it has too many degrees of
freedom alowing the whole system to constantly but slowly drift from one
configuration to another.  We must stop that becuase when the lower layers
drift like that it makes it impossible for the higher layers to learn.

Distance biaes would give the system a starting bias, but I think in
time the w values just offset the distance bias. Wouldn't it? Well,
wait If a w value is always mlutipled by a .9 distance biase, then the
w value no longer ranges from 0 to 1. It ranges from 0 to .9. That means
all the learning updates are slower than the nodes closer to the output.
And the nodes further away always have less power.  But if the blance oint
needs to be .5 and .6, the a .9 bias isn't goint to stop the node from
getting the end result of .5 and .6 after multiplyig by the distance bias.
So it doesn't feel to me like distance bias reall solves the problem.

I need something more like the w^2 error term.

Flow effects are controlled by reltiave values of w.  So if we pull to
zero, that brings w values together trying to minimize the size of the
w value.

A simple way to add pull to zero I think, might be to make the push to
zero stronger than the push to one.

But also, I think I just turned on the old w learnig to make "punish
high reward low" balance. I didn't code true reward low punish high all
the time.  I need to check if it's true, and if it is, do another test
like that before I add the push to zero.

----

[10:54 AM]

Yeah, I had just turned on the old blance that only punished high and
rewarded low when we sorted to high. Not sure if that makes any difference
than doing it all the time. But I wrote new code to do it all the time
and I'm testing that.

Turned on forceExplore and turned off 1+ add to a values.  But I think
I probably have to turn back on the 1% add to a values becuase I think
that's needed for short halfLife problems when a values drop to zero
really fast.

Multiple tests running.

----

[6:56 PM]

Went to lunch with Pig and Jan for St Patrics day in fairfax city and
had corn beef and cabbage.

Came back and posted yet another long explanation about the illusion of
consciousness being a data binding error.  First time I really talked
about it i those terms. Saved it as a google drive document.

Got into stupid debates I should not have wasted my time with.

----

Just went out and tested the C5 setting to reduce max amps on my
controller per Lindsy? requres from Storm Owners group.  It does work
as advertised.  Posted photos in Storm Owners group.

----

The balance tests for the network seem to be working just fine.

The 7 layer network shows a lot more noise at the final output where
pulses waves are changing in volume from cycle to cycle of the inputs.
But the first few layers look fairly stable.  It's clear a little more
nose and instiblity is added with each layer. I don't know how much of
this is network instablity or just stostic noise from the random sort
picks building up?

I'm now thinking this damn network has worked fine as I created it a
few months back!  There's no need for this backwards traning at all.
The *a term along with the AT to create memory is where all the magic
happens of making signals that fire toggether "wire" together -- aka
sort together.  The w learning is only needed really to balance the
signals so as to create a maximal set of signal from the data.

As far as I can tell, there should be enough signals here to allow the
network to learn decoding!  Part of the problem before might have been
too short a half life or too slow a pulse rate for the problem I was
asking it to solve?

But, I'll have to do careful testing of RL with percpetion to see if
the sytems can play nice together to allow it to learn a decoding task!
But I want to plaly more with percpetion first. I want to see what
happens if it fans out a lot further.

And I want to see what happens as I reduce it down from a large number
to say a small number like two!

And I want to explore the idea of stablity more and see if I need to
do sorething to make the network stablize. I need to graph the w values
and seeif they stablize or if they might keep drifting!

And I need to turn w learning off after it's trained and see if the
network keeps functioning correctly or if it falls on it's side or
something.

AndI need to gest full fan out. I'm using an 8 wide network and have
mostly just tested with 3 and 5 fan out.

Oh, and small fan out like 3 clearly doesn't help the network form the
correct signals.  The system needs to identify correlations between
signals, and if we mix 5 tgogether, it can extract the correlation from
all 5 signals at once which can't happen correctly with small fan out
and tryig to mix and max signals.

Oh, and there's another thing that needs testing.  Using a more complex
and advnced try wiring format with a small fan out like 3 to see just
how it really works.

But as far as all these different training ideas for w learning, they
all suck except the simple obvious one -- lower the high value and raise
the low value.

Oh, and I added PTZ by making pull to zero training over a little faster
than pull to one. But I haven't done the ntesting needed to verify if
this helps or not.

Dman, so much to carefully test now.

----

Oh, and I'm not seeing any signs of signals turning to mud.  So whatever
was causing that in the past I think might be gone.  But I need to test
full fan out to see what that does.

----

[8:19 PM]

Ok, so a full fan out had an intersting effect.  It produced the same
signal to all outputs.  Basically, it just summed the inputs and merged
them to all the same outputs!  There was nothing to create a bias to
cause different signals to emerge!  I'll let it run longer to see if
something might emergre but there's no sign of drift let.

Tried turning distance bias on to see if it would have an effect.
Didn't see any effect at all!  Same result as with distance bias off.
guess I should double check that I actually turned it on based on my
track record.

I suspected that distance bias would actually have no effect as long
as the bias * w was still within the range of what the system wanted to
stablize to anyway.

So, limited fan out is like having a 0 distance bias.  That certaonly
creates bias that tseems useful.  Seems to me there should be a idffere
way to add biase that would work other than this failed attempt to use
distance bias.

================================================================================

2016-03-18 6:54 AM Friday

Yesterday

1) Went back to just a simple balance algorithm that punished high and
rewarded low for every pulse.  Cleaed up and deleted the code for the
other 200 ideas that all seem pointless.

2) Started doing more testing with this version of the code.

3) Added a pull to zero effect to try and stabalize the system by making
the punish high stronger (10% faster) than reward low.  Really don't
have a clue if it worked or not.

4) Tested full fan out and was surprised to find it created identical
output signals (for the most part).

5) Tested a 19 layer deep net to see what the final outputs would be.
Outputs are spiky signals that seem mostly sort of random, but yet
somewhat complex as well.  Seems to have lost all connection to the
intput signals, but yet not turned to mud or flatline either.  Fanout 6
in a 10 wide network.

Today

More testing and thinking and experimenting on percpetion.

The full fanout surprised me.  I thought this net would genreate
separate signals.  But it seems it only does that when the fanout is
less than full.  Letting it run overnight didn't make it switch to
different signals.

Now I'm wondering why. Why don't separate strong attractors develop?

----

Create a very deep 10x20 network for testing with a fanout of 6
(hum even number -- non symetical).  The final output is very spikey
with most data being sent to one somewhat random output at a time.
Not seeing any corrleation to the input data in this final outpout.
Though the shifting is sort of in the same frequency range I guess.

The energy measures per column keep incrasing for deeper and deeper
columns with it starting in the 1 ish range, and stepping up to like 10
by the end.  So that means each layer is getting a little more spikey
than the one before it.

----

I don't know waht to make of all this.

The full fan out surprises me that it doesn't naturally form separate
signals.  I thought by random chance outputs would become attractors
for different temporal/spatial patterns.

Has the pull to zero made the net so stable it can't form attractors?
I'll have to test that, but I'm gusing that's not it.

But I'm also wondering (still, as always) what the netowrk SHOULD do?
Do I want it to spread signal, or clump signal?  I can't have both can I?

Pulses need to spread to all outputs in order for the system to be able to
learn any routings for RL. But can information signals spread differently?
I think they can to some extent.  The *a allows signals to effect each
other and as a result, information in one set of pulses ends up controling
the routing of other pulses, which controls the spread of the information
in the network that is separate from the spread of pulses.

So questions about how information spreads really can be different from
issue about how pulses move?

Yeah, when we use electrical signals to encode information we don't care
which way the individual electons flow in the wire as long as the higher
level information signal acts as we want it to.  And in this recent
testing I've been doing I've cranked up the halfLife exactly for the
purpose of allowing me to see the high level information flows.

So I need to be careful about what I'm thinking of -- pluse flow issues
or information flow issues -- they aren't the same all the time and I
think I have been conflating the two at times.

If pulses couldn't interact, then information flow would be be limited to
pulse flow. But because my pulses do very much interact becuase of the
*a policy then information flows are very different than pulse flows.
The *a effect is amplification.  Something very important in signal
processing -- it's why transistors are so important in circuits.

And of course I added *a exactly becuase I was looking for ways to make
sure the system had the power to do amplification.

And amplification is information flow control. It's information shaping.

----

Oh, and on the full fan out I realizsed that the intput signal I'm using
no longer sums to zero. so when the full fan out balanced all data,
it didn't create flatline mud -- it created the signal that resulted
as the sum of all inputs.

So when I wrote yesterday there was no mud effects, it might have more
to do with the input signals not summing to zero, than with what the
network is doing?

I need to go back to a constant pulse volume input and see what that
does for these different networks.  Maybe some of these will turn
to mud with that othre than the full fan out?

----

Ok, thinking of this is a signal processing system with * as a keep took
I can think of the full fan out as a set of equations where if x and y
are the intputs, I'm asking the system to produce outputs of Axy + Bxy +
Cxy where each of these are equal.  And of ocurse, that solves to the
same signal.

But if I ask it to solve the equations ax^2y + bxy + cxy + dxy^2 I get a
different solution.  And this in effect is the sort of thing I'm asking
the network to do when I use different fan outs because the different
signals get mixed with each other in different ways baesd on the toplogy
of the network, and then all balanced out by the w learning.

So if intput is x and y, and output is ax^2y + bxy^2 then the pulse
routing means x+y = ax^2y + bxy^2 and the w learning means that E|ax^2y|
= E|bxy^2.|

I think this is sort of what is happening. Maybe.

So the system moves to a configaution that solves this sort of of set
of equations.

----

Oh, idea.  Balance median values instead of max or min! Since I calcuate
atAvg I could just train on that.  That would be intersting to see if it
was different than my blance max and min.  But gee, thinking about this,
when I punish max and reward min, it means I'm adjusting w to make the
signals have an equal amount of time in the max state and the min state
compared to the other signals.  Darn, that sounds perfect for controling
behavior where we want signals that each take their turn being the "lead"
and where over time, each signal is the "lead" and equal amount of time
with all others.

Wow, thinking about w learning in that terms makes me think I really do
not want to mess with it and do something different.

Oh, but the pull to zero -- that means that punish max has more power than
reward zero.  So I'm trying to train the signals so they are spend more
time as low, than as high.  But that's not possible. I pick one signal
to be high every pulse and one to be low.  So they have to be equal.
Ok, so I guess the difference in strength is only pushing the w values
down not shifting signals to be low more than high.

----

[8:13 AM]

Ok, switched to a singl that sums to 0 for all inputs -- that is contant
pulse flow input at all times.  And sure enough, this formta signal
tends to turn more mud like at deeper layers.

When I used signals that didn't have constant pulse signals, then the
outputs would remain signal like, exactly becuase they were forced to by
the fact that total pulse flows were going up and down over time.  Even if
some were made zero, the rest had to be even more non zero as a result!

Buit when the input pulse flow is constant, the outputs start to turn
to mud (flatline) far easier.

But is this ok? Or what?

The signals all sum to a constant pluse flow? And if there is a way,
is that a good thing to do or not?  This is what I was lookig at before
when I started to expore these other more complex learning rules.

Ah ha ha.  ICA trains seperation to maximize an odd measure of complexity.

Maybe that's just the point.  If you mix signals goether, you get the
central limit theory of it all turning to one normal noise distribution.
But if you mix just right so as to undo the random mixig, you get
increases signal complexity coming back out.

It seems to me that this sort of unmixing could be valuable.  Maybe even
inprtant.  But that it's certaoly a lot more complex than just the *a
correlation and blancing that I'm doing.

And, due the cetnral limit theory, I can expect the signals by default
to always tyurn to mud if I don't carefelly un-max to maximise complexity.

I tried information maximising, and it didn't seem to work.  But I might
not have been doing it correctly.

ICA maximizes kurtosis.

----

Just paused to read a bit about FastICA.  Don't fully understand it. But
it's basiclly using some tricky math using a non linear fuction and it's
derative to figure out the direction of the slopes which tells it which
way to adjust the weighs to increase contrast.

I guess maybe the issue is not simple gradiant decent but due to it
being a 4th order statistic trying to be maximized maybe it's more like
the gradiant of the gradiant and the system is subtracting the two to
figure out which way to adjust the weights to incrase the measure?

----

But, taking this sort of idea, and abstracting and simplifying.  This idea
comes to mind. If I want to avoid MUD (all outputs flatlined), then I
want to adjust the w values so that they maximize the spread of the output
node values over time.  I want to maximize the difference of the nodes.

I can't just force the spread by sending to the highest value. I need
to find what w value maximizes it over time. So this is a problem of
finding the slope of the spread function..

So, if we start with what we want to maximixe, I could say the spread of
the output nodes we sort to.  Which could be measures as their stadnard
deviation around the mean.  Ahy, and in this balanced pulse sorting
world, that would mean I want the w to be set, so as to try and force
all pulses to one output, then all to another, than all to a third.
That would maximize the spread while keeping them balanced.

So that is a tricky call since we want to maximize spread AND balance
outputs.  And this dual mandate is what has created problems in my random
attempts to do something similar already.

But the *a term is natrually helping to form this already.  Since it
tendsd to suck pulses to the highest output, which piles on more pulses
to whichever output dominates at the moment. And then by using that
constant *a term, that doesn't change, we can balance using w.

----

Shit. just had an insight thinking with my eyes closed and feet up
on the desk.

To maximie spread, I can't just look at the current outputs and
know which way to move w, It's impossible.  I may need to increase
or decrease w from the curent value. Then test, to see if the change
improved spread or hurt it.

FastICA is looking thte current values, and testing somehow to measure
which direction makes it better, by doing a full scan over the numbers
using the current w values, then pushing them numbres in the dirction
that make it beetter and repeating. Reresting from the new point.

My insight, is this is the same as RL. I don't know which way to shift
flow to maximixe rewards. I just have to shift a little, test and see.

So maximizing spread, is a problem akin to maximising RL.

So shit, could I use the RL system to solve the spread problem as well?

Make the measure of spread the reward signal?  And maximize the measure
of spread?

If the input layers need to do more perception, and the later layers
yess, then this would be an adjustment to which reward signal was
stronger.  The spread reward or the external reward?

But them there's the balance problem as well. That I can do by looking at
the current outputs like I am now doing.  But maybe it should or could
be mixed in the reward as well and so we just focuse on optimizing for
the one reward which is the sum of all these reward goals???

So, spread, I could culculate as the square of the variation from mean.
Maybe just calculate for the entire column?  Then use that as a reward
signal to maximise but onkly in THAT COLUMN???? That's a cool idea.

But, the key important concept, if I want to maximizse spread, I can't
calcuate it, or even calcuate the dirction of slope which would allow
me to use gradiant decent. I must do trial and error search, and track
whether spread is getting worse or better, when w moves one way vs
another.  And then drift in the direction of improvement.

----

[1:47 PM]

Went to see zootopia movie with Pig.

This idea of having to find the correct sorting by using trial and
error is fascinating.

But thinking about how I do trial and error learning, I can't help but
wonder if I'm doing it correctly. Or if I can do it better.  Most of
what I've been testing with so far is solutions that have either balanced
shared outputs or outputs turned off. The system also needs the power
to optimize the setting of the flow ratios to maximize rewarcs as well
so that the optimal ratio might be 4:6 or something.  I'm not 100%
postive that the way I'm doing it now could learn that correctly.

The node learns the value of the pulse..  Wait, I'm drawing a blank
It's been all of a week since I was last obsessing on the design
and I'm drawing a blank about the details of how it works!

But since I took the use of the node q value out of the update,
is the update actually calcuating it's relative values correctly?
I fear it can't be.

I'm thinking that each link needs a q values, and that our random changes
away from the eta, should be measured to see if increases or decreasses
are creating more reareds, and that the link b value should be drifting
up and down relative to which direction is producing more rewards.

The changes in rewards should directly be pushing sort ratios up and
down. And they don't do that now. I have this complex shit mapping
the fake q values, to target sort ratios, that slowly adjust the
actual sort ratios towards the avergae targets. I don't fully understand
why rewarsd aren't directly pushing the sort ratios up and down.

----

Great pause, sleep, snacks, facebook distractions, etc.

----

Ok, looked at code to make sure I was remebbering how RL was last working.

The Node tracks the Qvalue, and the error update we use to adjust links,
is derived from how the node is different from the truth.

This seems so round about.

But there were reasons I did this I think. :)

But in simple concepts, we should track a q value for a single link.
That q value represents the correlation between the use of a single
link, and the expected rewards.  As should track how our random behavior
differes from speivide behavior (at/eat), and then see how that correlates
with rewards. when a random increase in behavior causes a incredase in
reward, we adjust our w value upwards to incrase the use of our behavior.a

Right?  So qLink += (reward * inc + qLink disconted - qlink) * speed

Or: TD = (reward * inc + qLink disconted - qlink) * speed qLink += TD b +=
(1.0 - b) * TD * (1.0 - at/eat) * speedORUnitsAdjustFactor

Or something alone those lines.  We just track rewards and and track our
usage (at/eat) and we adjust our useage (b) as rewards are adjusted. When
rewards go up, we go up, when rewards go down, we go down. And the speed
of movement is relative to our usage at/eat, so rewards g up and own
faster the ruther we are from our expected usage.

I feel like we have this problem of mapping q values to b values, that
I've been fighting for months now and coming up with complex solutions,
but that we should have avoided it all form the begining, by just tracking
how rewards change with our behavior changes and adjusting our behavior
in parallel with reward changes.

It's maybe a little complex to do this but it seems possible. The slope
of how the link activeity deviates from policy, must be correlated to
how rewards deviate, and that must drive poicy up and down. So we would
still need link q values and link v values, but not link sd values.
But that assumes there's not a time delay issue.

But darn, i'm working on percpetion, and RL is trying to suck me back
for yet another big fucking rewrite.

Ok, so I got thikning about this as a way to do a trial and error serch
for w value optimality as well. so lets did a big into that..

----

Lets say we want to adjust w to optimize sum(at-atVvg^2)/b for the
entire column?

So, we sort.  We update the node output at values.  We calcuate a new
column varriance. We do a diff from the old to the new, and this is
the error update. We correlate this, to the the w value of the link we
just used???  1-at/est tells us how our actual behavior is different
from the w value.  so we update the w value, by the direction and speed
of the difference and the direction and speed of the change in varrance?
And I guess we have to track this measure per link so that the update is
relative to what has chaged since the last time the link was done?  Maybe?
It's complex. Not sure.

Sort of feel that when we see a change in the varrance measure we need to
apply it to all w values in the column, relative to how each has changed.
And with the AT values we apply it bvack across time. But how often do
we need to measure and calculate the varriance?  Wow, and I'm thinking
spatial varrance across AT values, but we could measure temporal varrance
for each AT as well.

Darn, getting to complex. Having to think about 10 things happening in
parallel at the same time that we are trying to cope with.

Not to mention, where does balancing fit in?  Minimize varrance of spatial
average but maximize teporal and spatial varriance of node at values?

Darn complex.  Messy. Is it needed???? Shit, I don't know.

My head hurts thinking about the complexity.

Break time.

================================================================================

2016-03-19 6:36 PM Saturday (yes very late in the day to start)

Yesterday

1) Try to grasp percpetion still.

2) Realized that information maximsing like ICA had to be done as trial
and error search.  It can't be done as a direct learning algorithm based
only on node or link AT values!  So there's really no point on thinking
a siple w value update formula could solve that!  It requries a more
coplex sytem that tracks change in the target I'm trying to maximixe,
and then correlates to random chagnes in behavior away from policy,
just like we do with RL.

3) Started to question how I'm doing RL again.  At a high abstract layer
I don't undrstand why we can't track expected rewarfds, and update the
policy cireclty from changes in rewards, as correlated to the random
exploration away from current policy (at/eat). I wonder if all this
sturggle to mpa q values ot policy has been becaue I never should have
mapped q values to policy to begin with -- I should have learned policy
in parallel with learning q values.

4) Sort of got depressed about feeling like I'm moving backwards instead
of fowards here.

Today

Correct dates for the last 3 days. How did I get it wrong that many days
in a row and not notice it???

Let myself get distracted all day on facebook and email lists instead
of doing work.  Well, it is the weekend after all.

It's late now. Need to go get some food probbaly. But just wanted to get
started here before the day was over and not have any work to show for it!

----

Brain storming about perception some more.

Though I had this insight about having to solve ICA like factor extraction
using trail and error serach, like we do with RL, I'm not so sure that
we need that to create AGI.  I think that might be overkill. Humans have
the power to issolate and "listen to" one voice in a crowd, but I don't
think that has anything to do with ICA like algorihtms. It's RL that
trains us to select what data we use to drive behavior and I see this
whole prolem of "attention" as being something solved by RL controlling
what data we move forward in the network.

And I see that working, through this sort of *a state selection system.
Once we start to focuse on on one voice, the internal state of our voice
recognition is represneting what that one person is talkig about and
that state, is what is causing the next state, to be filtered out from
the noise, not some complex ICA type algorithm.  That is, our currnent
state (current AI values), is what is predicting and controlling what
state comes next. So I thnk we are on the right track without the complex
ICA algorithms.  So I'm inclined to ingore that for now.

And for RL, being wrong, I'll ignore that for now as well. Jumping
back to dig into that will disract from trying to finish off a beter
percetion system and getting tot he issue of makig the current RL
better integrate with perception.

----

So, if *a and signal balancing is all we need, and punish high, reward
low, making signals blance how long they are the strongest signal in
the fan out, with how long they are the weakest signal, that sounds
fine to me.

The only thing we might improve on that, is rewarding for how lONG they
stay high so as to slice up time into the longest temporal slices
possible? But yet, I'm thinking that might emerge on it's own without
extra work. 

This whole isue of how important network topolgy might to control
waht sorts of signals get created needs to be payed with a lot more.

But, the one issue that bugs me currently is this problem with a full
fan out, creating N nearly identical signals.  Is this right?  Or broken?

If I have two signals, that have clear non-correlation effect, maybe
even negative corelation, why can't it create alternating outputs?

So like:

A __--__--__--__--__--

B --__--__--__--__--__

Why would these get mixed togeher, instead of each forming their own
output node to dominate? Two perfectly negatively correlated signals
should not mix, I guess is what I'm saying.

Or, maybe, any postive correlations sould be mixed, until we end up
with perfect negative correlations? Then they remain sperate forever?

A ____----____----____----____----____

B ___----____----____----____----___--

Mixes to become something like:

X ___---_____---_____---_____---____--

Y ______---_____---_____---_____---___

Where the postive high corelations are removed?

So maybe, the goal never should to create statistcially indepdent signals,
But rather, maximum negative correlations? We want pulse TRAINS to
cluster together in the same signal?

This same like a decent path forward. I need to craate a test of two
simple inverted signals, and see how they mix in the current system.

----

Set up two input inverted tests on 3 fan out to see what happens.

Actually, intead of what I said above, if the netowrk has 10
ouptuts, and the signal has two states, the states should get
slized up into 10 states each lasting for 1/10th of the cycle,
I'm thinking.  So the blance should foce the signals to
cycle though N states I would think. Somehow.  magically. :)

But maybe that's asking too much, and the asusmption is that
with complex data, it won't slie up one long imput state
into multiple buit will instead pile up smaller states
into larger output states?

How might that work?  A node, once on, will want to stay
on for a while, then burn out and develop a desire to turn itself
off and let other nodes come on?

Or that the longer a state is off, the more power it delopes to suck
nodes to it?

feels like the burn out appraoch might be more reasonable to implment?

Hey, with this burn out idea, makes me think of temporal balancing. That
is, from the perspective of a sngle node, it doesn't like to be active
for too long? Or inactive for too long?

The w balancing learning certainly sets the codition that it must blance
the high and low, but has no test for how low the low is, or how high
the high is, or how long it's been high or how long it's been low.

The mud problem is that nodes don't go low enough, not that they fail
to balance trading off high and low with other nodes. So a burn out sort
of effect might force lows to go low and "rest" to recover.

It feels like the burn out time and behavior would define a frequency
that the nodes like to respoind to, and that natural frequency would
sort of define the sparsense of the network and how long a node would
tend to stay active.

So, if we had a real world condition like "cat" that would stay atctive
for hours as the cat was haning out with us that "cat state" migth be
ofrced to be spread over many nodes that alternated. Oh, gee, that's
not right Then we would have to learn different behaviors for each cat
state. No it doesn't seem rrigh tto force a state to turn off when it's
correctly representing a feature of the envionment. Maybe this burnout
concept is not the right one.

The problem of course is to blance the states over the nodes over time.

But maybe with this mud problem of the full fan out, the issue is not
the balance, but too much blance? That the nodes don't naturally fight
to unblance themselves enough?  That one node, isn't already trying to
get all the pulses?  And that the two nodes, each trying to get all the
pulses, is what the balance system need to fight in the first place?

So the question would be, why doens't one node fight to take cotnrol
from the other, and end up winning?

----

[9:05 PM]

Back from dinner.

Ok, the outputs clearly didn't flip one way vs the other, they produced
equal signals on the overlap.  One node is not showing the power to
dominate the pulses from one node -- which means the forces that balance
the signals are somehow dominating over any tendency for the flow to
become lopsided.

Well, the balance training might be to blame. When one gets higher it gets
punished for being higher and if the other is thelow one it's rewarded.
This creates an active blancing force to keep them from getting out of
balance.  The *a force is pulling against that to cause it to fall to
one side vs the other. I guess the balance training is stronger than *a.

Ok, but if it DID fall over, why would the w values end up training
to support that? The high output, would be getting all the pulses, but
nothing is at work rewarding the high output.a  The only force as work
is the punish high, and reward low.  Ever pulse from the active input
NOT beng sent to low, is being rewarded for not being sent to low.
Making that active input a very strong motivator for the low input.
The weekly iput, that we want to train to send to the low, is being
tained but not as much as the weak input.

I don't see any training in the system to force to learn to support the
lopsided activity.

ok, is the active input, needs to be trained to balance, but there seems
to need to be some traning to encourge it to support the imbalance that
it sees when it's activity. so thogh we raise the lower input, and lower
the high output, we must ?? what?

Also adjust the inputs to produce the same output in the future?

The reverse training I was trying to do and then gave up on, sort of
feels like it's needed here again.  Ah, I can look backwards at the
strength of the downstream nodes, and adjust w values some how related
to that? I need to give the strong signals ???

Balance flow forwards, imprint patterns backwards?

Decrease strongest flow forward to balance, incease signals relative to
their downstream node strength backwards?

But every pulse sort would be relative to their downstream patterns.
So if we just increse every w when a link is used, that would do the
imprinting.

And then decrease the high path, every time?

BUT, we need something to increase unused paths more than to decreasee
high paths. The unused paths are not being used for if they are rewarded
for use, we have a catch 22 that they can't reward if they aren't
being used.

So a more workable solution would seem to be rewrading paths not used
But punish paths that are used??? But how does that imporint downstrtea
maptterns?

So, so every time a input node fires, we want to imprint the downstream
path?  yes! So if it's blanced, but starts to fall to one side, we
want it to imprint that fall, and create a postive feedback effect.
But do we imprint with negative punishment or postive rewards?

So, reward with strength relative to the downstream nodes, then push
the high? Punish the sort?

Or, for every pulse the downstream node receives, imprintbackwards,
relative to the strenth of the downstream nodes? Reward eacn w relative
to the stength of the downstream nodes? So if the downstream nodes
are 1:2:5 in strength we want the w values to take on a 1:2:5 ratio?
That's what I was trying before, and I didn't get anywhere with that.

Punish high forewrad?

Reward patern backwards?

Yeah, it's the looking backwards that we need to imprint the input node
pattern onto the w values for the output node.  isn't it?

Yes, the input patterns are where we are looking for patterns.  The
outputs are creating new patterns.  So each output node is learnng an
input pattern to respond to.  The forward routing then ha the power to
blance the nodes so eachoutput pattern detector is high an equal amount
of time.

But given an input pattern of nodes, like 1:2:5 relative value what is
the correct w values for that? How de we find them?

Yeah, I'm thinking we just need to increase the 5 input 5 times faster
than 1 input. then let the w values find their balance.

But if they are puinshed for being high, then every input will be punished
an equal number of times.

I'm lost. Just got to code something to look at how it acts.

----

I'm still lost.  This is so complex trying to understand what updates will
do when.

But just had another thought.  We can use currnet inputs and outputs levels
and do something like reverse calculate what the w values would have to
be to create these levels, and make w seek that value?

I was thinking if the inputs are 1:2:5 ratio the w should be that as well.
But that doesn't account for the outputs?  If ontput outwas 10, and set it's w
w intputs 1:2:5 and another output was 20 and set it's outputs for
1:2:5 the two would not work oterher.  The 20 would be sucking far more
than ... ah, wait, no?  The 20 would suck twice it's pulse volume which is
what we want?

Ah, but what if we don't want to maintin the imblance, but instead push it
to more equal?

----

[1:31 AM]

Still lost.  Still confused.  I do feel there must be a siple answer
here I just can't see it, or see how to figure out what it is!

There must be some simple way to make the output nodes lock on to
different input patterns so the ydon't all just get an equal feed of
data form all inputs.

Getting late. Time for bed.

Maybe I need to go back to the idea of postive and negative correlations
from above and find a way to leverage that into insight into how to do
the updates?

Ah, maybe when we look at the nodes on eithr side of a link, we can think
of the w value as a measure of the coreation betwween the two signals.
If w is large, the signals are corelated. When that input is high, the
output is high. When the signals are negatively correlated it means one
is high when the other is low. And when that happens, the w is low.

Maybe this idea can be leveraged into how w should be trained?

Tomorrow...

================================================================================

2016-03-20 10:03 AM Sunday

Yesterday

1) Dig into why two shared outputs don't form separate signals but just
sum together and balance both common inputs creating two copies of the
same signal.  Tried different ideas of forward and backwads training
again -- what I had given up on a few days ago.

2) No progress on 1).  Tried different w learning algoirthms but it's
soo confuing to try and grasp the dynamics of this complex system and
how a given w learning algoirthm will make things happen.

3) Ended with thinking about correlation again as a key factor on how to
set w values.  Correlated signals wire toether -- high w values negative
correlations mute connection.  No correlation balance?  But no progress
on how to code that.

Today

Continue forward on the percpetion mystery.

So mostly, *a and balancing learning works well and solves a lot of the
underlying problems needed to be solved.  But it leaves open this one
unsolved problem.  With larger fanout, two or more shared output nodes
shouldn't just sum.  They should automatically form unique unique signals.
But they don't now.

So it feels my w learning is still not correct.  If two nodes are
correlated the correlated signal component should be extracted to one
output node, and the others left out.  Ok, but blancing does that if we
don't use large fan out and if we only hvae one common connection.

----

[11:28 AM] back from Starbucks

Did a test with PTZ on and off using the old code. That's the 1.1 extra
strength of the punish. I see not difference in the graphs. I'm going
to conclude that it has no usefl effect and delete it. I think all it
could do, is shift the w values down a bit but the system will create
the same relative solutions.

----

Ok, moving on.  Back to the great percpeiton mystery.

So, here's a new idea.  Instead of expecting the system to create two new
signals when two output nodes share two common downstreem nodes, could
the same thing end up happeing, if we just wired the network differently?
And not allow to output nodes to share more than one downstream node at
any point?

So, if we want to extract the common correlation between two signals,
the way to to do it, it seems, using our simple *a and blance code,
is to build a W network, where the two inputs fan out to three outputs
but each node only has two fan outs.

This creates one common shared node, which picks up the shared correltaed
signal, and two wing nodes, that pick up the negative of the correated
signal.

Is it possible this is a wiring topology problem, and not a w learning
problem?

Tring to come up with a better wiring idea, I just re-invented my same
binary tree wiring I've already have coded as a way to best mix and
match all the signals.

It could be that the easiest solution to the issue is the correct toplogy
-- which has parallels I think with what a FFT does.

I think that might basically take a bumch of signals, and extrac them
down to the common non-correlated components..

----

The problem with every attempt I've done so far to figure out how to
w learning to make common signals extrac without the help of topoligy,
is that the two goals of the learning, are always in direct opposists
with each other. I need to increase w to blance, but decrease to make
it form signals.

----

Ok.  If toplogy is the solution, what is topology anyway? It's just
connections that are 100% active or 0% active. Why can't we make
conmnections 50% active? And get the same sort of result?

Why can't we use the distanceBias idea and make it work to solve this
shared node problem?

So the problem with distance bias, is all that it seemed to do, is cause
us to learn different w values, to offset the effect of the distance bias.
The network seeked the same solution in the end, it just had to offset
the distance bias to get there.

So, that just means we weren't doing it correctly.  But what is
correct? I'm now thinking what we got wrong, was the balance learning
was wrong. It would just keep adjusting w to whatever value was needed
to offset the effects of the distance bias and in the end, the system
settles on the same solution.

What if we bias the balance learning by the distance Bias values?

So instead of creating equal outputs, we create unqual outputs matching
the distance bias profile?

So, each time we do a sort, instead of looking at the outputs and making
them all equal, make them match the distance bias profile instead?
This would mean the two adjesnt nodes would be trying to shape their
output, to a different profile.  Would this result in a system that could
balance at some comprimise?  Or would it create a highly unstable system?

I guess I have to try it.

But should the distanceBias JUST be used in the blance code?  Or should
it be used in the policy as well?

It seems to me, using it in poicy has no effect. The balance code will
always just seek a solution it wants to seek, and factor the blance ot
of the policy. SO lets just try using it in the balance learning.

----

Coded adjusting a values by /distanceBias before doing min max test
for learning.  This makes each node try to create a bell curve output.

This is ALMOST WORKING! The three common nodes were picking up three
different signals!  But it seems unstable. It seems that that the
nodes are just in constant battle and not settling on a solution.

I'lve started a second run to let it run and let w speed drop down
to e-8 to see if it seems to stablize.

But the next idea to try is use distance bias to adjust learning speed
across the fan out instead of adjsuting target outputs.  It feels
like that would not actualy work since there are not two different
forces at work balancing each other between the two nodes competing
for control.

Oh, wait!  If the downward learnign speed was different from the
upward learning speed, that would crate the two competing forces!
So it will blance at the point where it was higher 2 times more
than it was low for example.  Ok, so it wouldn't train the output
value, it would train for the ratio of high vs low?  So we would
be saying we want the center output to be high more often, and
the wing outputs to be high less often?  That sort of sets
a blance of power in which nodes get most control over an output
I think. Not sure if this is correct, but it sounds possible.

Got to just test and see what it does.  All this crap iss too
hard to predict by thinking about it.

----

[1:38 PM]

Well fuck.  Both of these seem to produce about the same answer dispite
being so different. And both are working to produce different mixes
of signals! [EDIT LATER -- wrong the second that adjusted by speed of
learning didn't work, when I tried to test it, I left the first turned
on by mistake so it was just the same result again]

Trying a full 7 fan out for total overlap to see what it produces.

And both seem somewhat stable, once the w learning speed drops down
low enough.

And oddly, even the version that still trys to blance outputs, but
only uses different learning speeds, is not choosing to balance the two
outputs that are driven by ownly on input.  I don't get that.  How is
that possible?

I need to also test one signal driving many outputs, and see what this
will do.

I would be cool if one signal driving man outputs had the power to crate
different output signals, but to be honest, I don't  udnerstand how
that would work.  But, you know, it really needs to be able to work --
at least in a network with mutiple layers if not in one layer.

I don't think it can work in one layer, because the outputs don't
interact with each other -- there iw only a single w value, controlling
what the output does in that case. We need multiple w values to make it
do something different, which would happen in the next layer once the
input signal was first split up into N copies of itself.  BUt then how
would the network pick a solution? It would be symetical at that point.

----

Ok, the full fan out is procducing diferent signals. BUT, what we get
here is just different mixes of the two imputs. So we get sin waves of
different strngths on one side, and sin waves of different strenths on
the other, and they are perfect inversions of each other.  This is really
not headed in the right direction. I'm missing something important here.

What is needed, is sequential temporal outputs that cycle though the
outputs.  And we don't get that from what is in effect ,one input signal.

I need to check what's happening in multiple layers.

----

On the deeper network, the two inputs with full fan out, created 7
signals, where the middle common one was mustoly just an even sum of
the two -- which produced a flatline result, and the rest rest were
also just even mixes in different rations of the two inputs. But in the
next few leyars, the signals don't seem to be changing -- the just push
temselves through for the most part.

----

This is all a step forward. We have comomon signals that aren't all the
same now.  But all we have done, is created SPATIAL differences in the
signal, not temporal diferences. Temporal differneces is what we need!

I sense something important is missing in the percpetion system that
would create the power to create balanced TEMPORAL signals.  Signals that
pulsed in a repeating sequence.

I think some form of cross temporal feedback is needed somehow here.

----

Ah, yes.  I'm beginning to see the light I think. the *a term and the
fact we do pulse routing, instead of splitting, is creating node to node
SPATIAL communication, but we are lacking TEMPORAL communication, where
what a node is doing now, can change what a node is doing in the future.

We have temporal communication forward from layer to layer but I think
we need something commuicating backwards in the neowrk, ot at least,
node to node in the same layer.

Our feedfoward design is not creating the needed temoral communication
feedback.  Maybe.  The At nodes are a form of mapping temoral information
to the spatial, but something is missing here even if I can't accuratly
define it yet.

I'm thinking that even a single constant pulse input, should force the
nodes to cycle though a repeating pattern.   All it does now is blast
random noise out in it's pattern of sending pulses to the different
downstreem nodes.

What if we could replace the random scattering, with some mechanical
predictible scattering?  Even if all it did, was force the network to
scatter sprawy the otputs in a defined sequence that repeated over and
over? That might be cool.

But what I'm thinking, is that nodes in a single column, need the power
to effect each other.  Mutual inhibition?  We might be able to train the
w values to create something lke this, but I'm thinking the reason trying
to train w value has been so hard, is that we need some form of feedback,
or mutual inhibition as part of the policy to make this work correctly.
The use of *a most certaonly crates one form of this, since it sucks
pulses away from other nodes. But it feels like there is something else
important missing still.

----

So, if we fix the w values as equals, and I send a fixed pulse stream
to the nework, what happens?  I don't know.  If we turn w learning off,
I think the network would fall over.  One node would start to get more
pulses, that would bias the suck to that node, and then it would just
get locked into the state of pulses going to that node forever.

That seems broken.  W learning should not be an active balancing force.
The network, if we turned w learning off, should osscilate from one node
to the next in this sitation I'm thinking. Somehnow. On it's own.  But if
it did, what frequency is correct and why would it pick on frequency vs
another?  I guess maybe this is a halfLife issue vs pulse densetiy issue?

And I guess random ossilations would be fine.  Becues if the signals
had more iformation in them, the signals would drive which way it would
oscillate. But if feed it a conatnt pulse srteam with no variation,
it would need to oscilate randomly on it's own. Maybe.

----

Another idea.  Gloabl feedback would create this same effect I'm thinking.

Maybne it's time to experiment with global feedback?  Could be. But lets
think about local feedback options as a soution first.

----

Ok, just figured out my two tests from above were bullshit. The second
test intended to just bias w learing by speed, but not by voume, I
left the bias by volume turned on at the same time!  Retesting now.
That explains why the second test wasn't able to balance the outputs!

----

Ok, it's looking like the blance by speed isn't working. I think it's
having no effect except to really slow down learning. :)  Only adjusting
the output targets really had an effect it looks like.

----

So, I'm thinking we need feedback either node to node in a column or
node to previous layer.

The *a effect is already a form of feedback from one layer to the
previous layer.  I'm thinking it needs to be a policy effect and not a
learning effect.

But *a is the feedback now What would we change without breaking *a?
So far, I'll I'm coming up with is a delayed effect where the pull of
*a weakens over time.  So we could follow the node with a moving average
and the strength of *a was determined as the difference between the node
and the moving average.

What if we have *a strength be from the node-avg that we already ave
crating a negative *a effect?  What would that do? At first thought,
all that does is amplify the *a efect to a greater extend. If the pull
was zero when a drops to zero are we saying it pushes pulses away when
a-ang turns negative?

That does't seem to buy anything.  At least not if the average was a
slow long term average.  If it's fast moving short term avg, then the
system dynmatically tries to blanace itself because of it. That does
sound intersting.

What if we did a/avg as a way to normalizse the power of *a?

----

[6:26 PM]

Back from Pizza.

================================================================================

2016-03-21 4:07 PM Monday

Yesterday

1) Still trying to understand perception and if the current system is
working or still missing something improtant.

2) Was looking at the issue of full fan out, or multiple nodes, shared
by the same outputs, all becoming clones.

3) Noticed that simple toplogy didn't have this same problem. If two
inputs share only one common ouput, the sytem wouldn't create clone
signals.  So I explored the ideas of using distanceBias instead of on
off toplogy links to see what I could do.

4) distanceBias to define different output balance targets, actually
worked.  Surprising really becuaes it means each node was tryign to force
a different output blalance.  But the effect was to allow the netowrk to
create a range of different mixing effect. But it did stop the network
from creating blanced outputs (in a single layer -- multuiple layeers
still tended to be blanced).  create

5) Tried distanceBias to adjust speed of learning -- had no effect at
all except to slow down learning on more distant nodes.  Still seeked
the same solution as without distanceBias.

6) Testing led me to think that all this current system is doing, is
adjusting sptial mix of signals, and factoring out spatial correlations.
It feels like there is some important teporal correlation power missing
form the current design.  But no solution as to how to add it. Or if
it's really even missing.

7) tested 1.1 pull to zero on downward speed of w learning and could
not see that it was doing anytyhing. Like adjusting learning speed,
I think all it likely would do, is change the stable point of the
w values, but not the relative w values, so the system produces the
same result with the *1.1 speed factor as without. So I removed that
code.

Today

late start. (4:20 PM)

Had a 12 noon dermitology appointment. So I slept in, and did that in
the morning.

Then, after getting home, I called American Express, and cancled that
account for newsreader.  One of the little pieces left over to cleam
up form the shutdown.

Then called Vonage and shut down that phone line as well -- another
missing piece.

The only things left at this point is to remove the two servers from
Avi's racks (tango and feed7).  And maybe set up a web page for
newsreader.com elsewhere.  Well, and I guess I should also just
close that checking acount as well once I'm sure nothing is going
to get charged to it.

Percpetion

So, this is still a fucking sturggle to figure out what is needed.
It's been an ongoing struggle for 10 year4s now really.

The start of this recent push of work in Januray was really all about
decideing that percpetion didn't need to be complex, and that simple
removing of correlations was all that was needed. That is, I didn't need
something highly complex like ICA or PCA.

At the moment, I'm still learning to the belief that PCA or ICA like
algoirhtms are overkill.  But my thougths on this stuff change back and
forth by the hour.

But yesterday, I had this idea that I'm missing some important temporal
corelation processing with my current design. The beauty of the current
design is, in theory, the use of the AT to map temporal informaiton into
spatial node activity level values, and then do all the corelation work
in the spatial domain which happens with *a effect of the nodes along
with the pulse conservatrion which creates negtive signals for where
pulses are not routed.

So, is there something missing I must add?  Or is the missing part
something that is solved by a different toplogy aor larger network Or
more data?

So, if in the time domain, signal A goes active, and that's predictive
of signal B going active a short time later, how do we capture this
fact in our network?

If A is active, that fact is recroded in one or more nodes going high
somewhere in the network. Then, if it is followed by B going high,
B's pulses should get "sucked" into those nodes activited by A, creating
one "concept" for the A->B correlation.  That sounds correct.

If A and B are not correlated, then A activates something, then when
b randomly follows, B gets sucked into the same nodes.  But B will
also get sucked randomly into other nodes.

So *a is contantly wiring activity together. Any time A->B they get
sucked together.  But the balancing will make B reinforce different
nodes somewhat randomly.

This is all looking correct from this perspective.  At the high level.

But the issue I was seeing, was the problem of one node, in effect feeding
mutiple output nodes.  Ok, so that's a problem. We have nothing in this
network, to "temporal decode" one node with itself.  This design only
deals with corelations BETWEEN signals.  And I remember thinking about
this fact a few months back as well now that it's come up again.

So if I feed a single signal into the network, that is morse code
for exmaple, it has no ability to build temporal "undestanding" of
this signal.  But recurent networks are really good at doing this.
Which relates to why I was thiking baout the need for feedback yesterday.

Full network global feedback might be all that is needed to make
all this work.

But what if we really need layer to layer feedback. Or what if, layer
to layer feedback would be highly useful to helping the network produce
better state signals and make it strong enough to decode even a single
signal feed into it!

Yes, I have felt that internal feedback was likely going to be useful
or even important at some point for making the sytem more robust and
stronger.  I need to think about this for awhile.

If a node on the second level developes a singal, how can we feed back
that signal, to the previous level, to make it useful?  Other than with
the *a effect we are already using?

Or, how do we make a network, that can take a single pulse input, and
decode it to changing interntal states?

What if we fan out, to output nodes, all with different halfLife values?
But otherwise, use the same balancing algorithms?
Would that do the temporal signal splitting needed to start the netowrk
off with the needed split signals?

What if, different inputs to a node, ACTED as if it were different
halfLifes?  What if the distanceBias idea, turned into factor= bias
for the output nodes?  So the power of our pulse, would have
less effect on futher way nodes?  Which for a single pulse, would
force the syustem to send more ulses to the futher way node?  Unless
we inverted that factor to 1/distance?  So more to the close nodes
and less to the further way nodes?

Ok, that might be a better way of doing the splitting instead
of the differen output targets (or maybe just produces
the same effect),

Ok, but that really doesn't chagne the half life effect and doesn't
help the temporal issue I don't think.

----

[6:10 PM]

More distractions in email and facebook.

One argument I gave myself, is that if you want to use this system to
decode a single input, one must give the network extra reference pulses
as well.  So if you want to feed in morse code for the system to learn,
you have to have a rich changing enviornment from which to correlated
the morse code sequece against.

Without a rich changing enviroment to lay that signal against, one will
be blind to the changing history of the one signal by itself.

I guess when we do things like count to ourselves, we are doing just
that to create a ference for comparision -- however that's often to deal
with extending the short term lmits of our system, instead of with just
giving SOMETHING to compare against.

Or, just feed in what can be understood as clock signals for reference.
to create such an enviornment artifically.

Certaonkly, if we have global feedback, it feels to me like that feedback
loop will natrualky crate a changing clock refernce effect for the
whole system.

----

So the key problem I've looking at, is really one of taking a single
simple signal, and expecting it to be decoded into many different signals
such as clock like signals.  But maybe the real answer here, is that
these networks never need to do that. It's always the other problem of
taking massive complexity from the enviorment, and reducing it down to
"simple" behaviors.

And if you want to do that, then you must feed in temporal reference
signals at the same time to create the temporal framework to help
decode the temporal input signal.

But the idea of nodes with different halfLives in the same network is
also intriguing.

----

So, if this network is good enough, it still needs the power to solve
the decoding problem. And it seems to me, that with the right toplogy
it really should have this power already.

Maybe it needs more help than I realised in the form of background
noise data to work with as well?  So if we send in mulltple srterams
of highly random pulse noise, but also a few streams of strong
correlated binary counting patterns, that strong corleatd binary
counting data patterns will start to dominate the etwork into decoded
paths?

What if the random noise had the long slow frequency temporal data
mixed into it as well? Or had it randomly scattered across the network?
woudl it pick it out and sort it all together bcause it was low
frequency state?

I guess that's a key poinjt I wasn't thinkinf of about what this
network would do. The output signals, should be the strongnest and
lowest frequency signals the network can pick up in the data?

So if it's got 100 output signals in the last node, those 100 sinnals
should the strongest, lowest frequency data patterns it could find
in all the data?

That seems reaonsale.

----

Full fan out. Ok, I think there's still an issue here. If there's enough
input information in the input signals, full fan out shouldn't merge
all the signals together. It should create individual signals.

I don't think I should be forced to create a binary tree network.

But hey, what if a binary tree network works better than these sorts of
distanceBias tricks?

================================================================================

2016-03-22 8:27 AM Tuesday

Yesterday

1) Not much real work.  No new code or testing.  Just working on
percpetion theories.

2) Current conclsion.  I relearned what I had aleady decided months
ago, but sort of forgot.  This network design, does not have the power
to decode a single temporal signal like morse code, sent in to it. It
only has the power to decode signals relative to each other.  To make
it decode a temporal signal like morse code one must sent in clock or
reference signals in parallel to the network to give the network something
to establish similarty and difference gainst in the morse code signal.
This is why, the nework by default is not able to decode temporal features
of a singe signal.  I knew this once, then sort of forgot I knew it in
my recent thoughts on perception.

3) The net mostly does what it is supposed to do. But, there is the
remaining issue that a full fan out cross connet from layer to layer
should have the power to create separate signals, if the input signals
have the needed information to key on. But does the network do that?

Today

Keep pushing forward on these ideas.

So, the plan is to assume the nework design is headed in the right
direction and so we need to test it's pwoer without making the error of
assuming it's broken for not doing what it was not expected to do. :)

BUT, full fan out seems an issue. If we feed it three signals, with
various temporal properies relative to each other, it should be able to
turn that into a full fan out of 3 hidden nodes i nthe next layer that
have 3 uniuq signals, and mnot merge all these it the same 3 output
signals.  So, what does the current network do here? Is it creating 3
clone outputs if I don't use the distance biase trick?

I need to confirm what it does first.  And if it merges them all together,
(which I think it does), we do need to fix this.

Likewise, two signals in, two singals out, should not just sum the two
signals. But to do this, there is total symetry, so there is no reaosn
for one output to take on one signal vs the other.  So if the neowrk
does have the power to crate two differnet signals, then they would be
arbatray as to which output took on which role.

Clearly, the network needs a bias if it's going to be stable and produce
the same answer every time.  So something like distanceBias is a must
I guess.

Need to code a two in two out test, and start playing with ideas
of what defines "correct" in how the outputs are generated.

Starbucks run first.

----

[10:01 AM]

Back from starbucks.  Modified code to create 3 input 3 output full
fanout test.  Using distanceBias to define diferent balancing targets for
each node and the system is stablaizing now -- creating three different
output signals.

Ok, but what are the the "Right" thres signals and why is one max right
and another mix (like all equal) wrong?

I think there's probably an anser here I'm failing to see.  And I don't
suspect that this current use of distanceBias is produceing the right
answer. All it's doing is forcing an answer that is different from all
three being equal.

Though *a is helping do it's job of picking what the answer is, I don't
think the distance bias is producing the right mix. Maybe it's a usefl
mix, but probably not the right mix.

----

So, posting on Facbook Model Dependent Ontology group I came to the
conclusion that the reason Manuel's argument was wrong, was becuase there
is only one way to model a stream of observations -- and that's through
temporal correlations -- that operation at T1 predicts observation at T2.

So, if this is correct, than that is what is correct here.  The outputs
should model the temporal history of the inputs, with as much inforomation
as possible.  That is, the outputs, should be statistically indepdent.

BUT WAIT.  NO!  If the unierse repeasts a pattern in a cycle, then we
want the outputs to specify the state of the eivnment, ni the same cycle,
so that our brahivor, driven from the outputs, repeats in the same cycle.
We don't want the state to be random like blinking numbers that can't
produce the right behvior at the right time!  If going to bed is the
right beahvior when the sun goes down, then we want the internal state to
reflect the same state, every time the sun goes down so that we produce
the same behavior, every time the sun goes down.

It's not the temporal dependencies we need to remove from the data. It's
the spatial dependencies! We don't want nodeA to be predictive of Node B,
for any one instant in time.  Which is why summing the outputs together
and making all signal prefect predictors of all other is maximally wrong!

Ok, cool, this feels like we are headed somewhere useful.

But we want the nodes to also include as much temporal history as is
possible.  So if we have 100 binary nodes (ok they aren't binary, but
lets ignore that), we want to remove all spatial dependency from them.
So if this discreete binary values each sample of the values would be
statstically independent in the sample -- not bit of the sample could
be predicted from another.  But from sample to sample we want maximum
predictiablity -- we want bits to stay on as long as is possible,
assuming they are derived from the sensory data.

Or, more accuratly, we want to represent as many sensory input bits as
possible with each output bit! (Ochams razor)  We want the output to
compress as much of the input data as is possible into the outputs.

So, what an output node should come to represent, is a temporal model
of the intputs, that reprsent as much information about thie temporal
history of the observations as is possible?  That sounds correct.

So, each sample of the current state of the nodes, at any instant in time,
should represent maximal information by the pulse history. And if all the
outputs are the same all the time, clealry they have failed to do that!

----

So it feels like the forward view, when an input pulse arives and we are
looking the output nodes, we should maximal randomness.  The input pulse,
should have no ability to predict which output is on?  No, that's not
right right. If the input pulses are temporal predictive, then we expect
them to be predictive the current state in some way.  If this is a sream
of cat pulses they shold be preditie of teh cat output being true.

----

Ok, side node -- instead of thinking of the nodes as binary variables we
think of our nodes as probablist variables that represnet the probability
that the state is true when normalized to sum to one across all the nodes.
But the same truth needs to apply -- that at any instant in time, we
want to minimize any cross correlations in the signals.

----

It feels that what is correct, is that when input nodes show up, if
they are not preditive of an output, we should not sort our node to that
output. If they are predictive of an output, we should sort are node to
that output.  So the weights should be adjusted to measure how predictive
our node is, of the different output nodes.  So when a node shows up, we
can look at the outputs, and track what the estimated output value is.
Say, we calcuate an running average of the outputs based every time we
look at them.  If the output's running average (say sampled at a fixed
frequence, or sampled at every pulse that shows up, is different from
the single inputs view, then we determine how random that output is with
respect to a given input.

If the node outputs were normialzied to 0 to 1 values with a mean of .5,
and we averaged what we saw for every input at a downstreem node, and
the average value was .5, that would mean there is no corrlation, and
that we should never send pulses to it.  But if the averfage was 1, we
should always send pulses to it?  And if the average was 0?  Never also?
And average of zero, would indicate a negatve correlation.

Hum, there's this an issue here with negative correlation vs 0
corrleation.

Is it ok for two outputs to be negatively correlated?

It seems in this behavior driving system, where only pulses drive
behavior, negative corelation is not bad.  But, negative correlation
implies a lack of information. What would be better, and more information
would be no correlations. So instead of three inputs that cycled as ine
001, 010, 100, with negative correlation, that's only 3 states total
the bits represent.  If they encoded as 00, 01, 10, 11, that's 4 states.
BUT, since we operate in an environmet there no bits is no information,
this is all somewhat different.  00 is not a different state than 01. 00
doesn't exist in effect.

So this is odd in a way that thikning of it as binary values dosen't
help us fully understand.

Ok, so I think the *a term tells us something important here. The amount
of suck all the downstream nodes have for my input pulse is the current
pulses correlation to all those nodes in the state they are in, relatlve
to when my pulse shows up.  If we average that over time, it tells us
the long term correlations to all the hidden signals.  So, if we know
that correlations, what does it tell us, that we should be sorting the
pulses relative to that correlation?  We could map the output values to
their relative suck power, and then train the w values to that reltive
value. So if the outputs are 1:2:5, we could train the w values to seek
to a 1:2:5 value.  But would we do it to what relative average?  .5?
1/fanout?

But, this is the not the forward blancing. This is forward prediction.
And clearly, it creates deadly postive feedback that ends with all pulses
going to the same output.  So it must be offset with something that will
balance it.

So, from the output node looking backwards, what do we need to see?
The same thing but in reverse maybe?  The correlation between the value
of the inputs, and the output node receive a pulse, should be matched
to the the output receivng a pulse?

This is the same idea I was going for before, but never got to balance
when I tried to code it. So the output node, looks at the inputs, with
the exact same algorithm in reverse, and tries to match the state of the
inputs, to the sorting probablities defined by the w values, even though,
the backward w values aren't used to sort.  That seems reonable. So
if in the forward direction, A tries to sort all it's output to B,
making the Wab alrger and larger, the backwards direction won't let it
get larger and larger, relative to the other values if it diesn't match
the true correation!

This is feeling like it could work.

But it feels like there needs to be some sort of standard for how the
node values, map to w values! oh, and if the fan out of all the nodes
and fan in is the same everywhere, we could make the w values always
sum to 1. But, we must support cases where fan in and fan out do not
match! So we can't assume that!  So all w values could be 0 to 1
with an expected average value of .5 maybe.  So 3 inputs, sum
to 1.5?   Maybe, is this even the right way think about this?

The issue, translates back to pulse volume. In the forward direction,
we know whatever relative values we set w to, will define the pulse
voulme. But in the reverse direction, we are trying to blance the
flow from one fanout to the flow from another fanout.  So if I set
a value of x in one w value and 2x in another w value, I need to
know that this will cause a 2x flow volume. I think.

----

Maybe this all translates to something much simpler.

If we want the entire network to have equal average balances everywhere,
which is sort of what we do want, when we sort A->B, maybe we can just
look at the input node value, and the output node value and determine
which is greater?  And adjust the w values based on that.  If B is
greater than A, we decrease the w values and if A is greater than B,
we increase it?

This feels like it fits into the spirt of what the previous discusion
was getting at. And it feels somehow very right becuse it feels like it's
making the relationship between the two nodes maxiamlly prediticve of each
other. Or, really, just the opposit, minimually predictive of each other.

So, we set w values, to make it true, that when the ysten DOES decide to
sort A to B, it's qually likey that that one is stronger than the other.

Ok, but what about like a 2 to 5 fan out where the otuputs will always
be less than the intputs?

Maybe, with them all fighting for the same equality, it will just
correcftly blance?

I have to code this sucker and see what happens.

----

That was a cool idea. But so far, it seems worthless.  The outputs are
basically balancing, but also seem to just be randomly difting up and
down and not settling on any solution.  But I feel the need to let it
run longer and let the w values get smaller and see if it converges
on something.

----

While that runs.

So back to the previous idea. It might still reduce to something cool
and simple like that, but lets attack it from the not so simple apporach
and figure out what to try to code.  So, in the forward direction, take
a snapshot of the output values, define an input mapping that matches
the same ratios, and map into a result that sums to .5*fanout, and then
push all w values to those targets.  And, then do the same in reverse.

Ah, but two dead inputs, and one live input, will push the outputs to 0
0 1.5.  But, ok, in the reverse direction, it will push to .5 .5 .5 as
each of the outptus try to suck up all the one inputs data.

Maybe it should sum to 1, and the issue of different sized fan outs is
not inportant?  Not sure, but lets sum to 1, and then later we can test
in a mpre complex network wtih different fan outs and see what happens --
assuming it even works in a network with equal fan outs everywhere.

----

BTW -- the push away from greater, seems to just be stabalzing on a .33
.33 .33 sort everywhere.  Well, worse than that, not realaly stabalzing
at all, just drifting up and down NEAR a 1:1:1 sort.

----

[12:59 PM]

ok, coding the idea of of adjusting w values to match forward and
backwards node levels didn't work. Almost all output ended up flowing
to one node.

So the nodes not getting output, were clearly trying to push w values
the other way -- to make all output flow to them. But they were losing
the battle for some reason. The forward facing learning was dominating
over the backwards learning.

Fuck.  What if forward larning should not be matching the output anyhow?
What if should be balancing. Pushing the w values back to equal?
Maybe that's it.  If they balance by pushing to equal that's not the
same thing as balancing by rewaring high or punishing low. Of course,
this just once again the same thing we have tried 100 different way
without luck so far.

Well, I guess it's worth a try. :)

----

Still didn't work.  End result is that one output dominated and sucked
most the pulses to it.  It was slow to fall over, the the w values never
got huge, but were like 30 30 40 but that was enough to push most output
the the large w value.  The balancing power wasn't as strong as the
sucking power yet again.

And this is the problem I keep running into over and over.

Whever I try to add some learning push to make it learn a pattern, that
has dominated over the balacing force and made the system just send all
pulses to one output or most pulses to one output.

Ok, so pushing to equal is not strong enough.

Pushing to output pattern creates self feedback problem that cuases
all output to go to one output.  What if we try to factor out the *a
postive feedback loop maybe and define the output pattern as the sqrt(o)
or something that we train for?

But wait, if we tried pushign to equal, and that wasn't strong enough
to balance the outputs, pushing to sqrt() of output is not going to
fix anything.  That's less balancing power than pushing to equal.

So, forward learning has to push opposit the output, if it has any chance
of balancing.

----

If the inputs were the right negative correlation and balanced (sort
of perfect signals) -- then the solution I think will be put to push 100%
to one output, and 0% to the rest. Backwere looking, from the output
every time it got a on, the onputs would be 001. And forward looking
we see the same thing 001.

But such a view would look unblanced from the perspective of each input.
Ah, but they would be perfect inverse predictors!

That's an idea.  What if the sum of the input node values time the weights
need to predict the output value?  Wait, fuck, This is not a new idea,
I've been here before. I am so just going in cicrcles here.

But it could still be valid even if I'm going in circles. There's a
problem of course that the AT is not a pulse count, and that this isn't
linear.  So even if we sorted 30% of the pulses, the AT would not be 30%
of the input value. So we can't directl compute what to expect like that.

----

[6:54 PM]

Went to movie at AMC with Pig. Back now.

To mimic sort of what deep learning is doing, we would have to stocasticly
reverse calcualte a pulse sort -- or all pulse sorts.  Then forward
calculate. Then diff on the first forward and the second?

But Dl doesn't have this *a effect in play which makes my appraoch a
whole nother ball game.  Not to mention the pulse sorting.

----

Ok, so we have the idea of negatively correlated signals that should
get passed through all the way to one output.  How would a learing rule
detect that and do that?  With the time delay of the activity trace,
not so well unless there were large gaps between the pulse streams?

Certaonly the back looking view would see LLH -- so for the back looking
reverse calcuation, to predict the input, the w values would have to be
LLH as well. So that's conceptually in the right ball park of what we
are trying to do.

I've tried to make the forward view balance.  And I guess I'm sure that
flow blance is not correct as the extgreem case of the LLH negative
correaition points out. It should be OK if we have to sort all pulses
to one output.  And the idea there, is that from the forward direction,
all outputs would still be balanced.

But all atempts to make the back view predictive end up in a disater
where it overpowers the ability of the system to balance. Or if I make
the blance strong enough, it just goes back to mixing signals.

With my current back view logic pushign w values to a valid probablity
distributions, maybe the forward balance logic should limit it's power
to only lowering the high output? Or only raising the low?  If an input
is high, but not sorting to a giaven output, then any pulse sent to that
output would train the w to send more to the weak output.

Seem like only pushing high down could be a valid idea?  But just not
nicely reverse semetical in any way.

Set, this reverse matching is new. I really just need to try it out with
the ovious options instad of trying to guess what might or might not work.

----

Ok, changing code.  New attack.

If the max negative corelation should do the max sort to one output, why
not test with that!  So I've created a three input where each input is a
frequence of 5 when high and .1 with low, and only one input is active
at a time in the 5 state.  So lets see what it takes to make this sort
to three outputs that aren't just the sum of all three!

================================================================================

2016-03-23 12:41 PM Wednesday

Dropped off Pig at Vienna Metro this morning so she could go into DC to
see Cherry Blossems.  Got distratged with facbook and email aftr getting
back.  Posting about the prediction issue on the NuPIC email list (arguing
that compression is the key to what the percpeiton system is doing).

Yesterday

1) Still struggling to (re)define the percpetion problem and code it.

2) created test with input data that is highly negative correlated as
an extreem example of what the input has to deal with.  Three iputs,
pulsing input 1, then 2, then 3 in a contant looping cycle where only
one input is highly active a time.  In theory, the optimal coding of
this, is NOT to mix all data together, but rather to map each input,
to it's own output, with no mixing at all.

3) Found that the reverse learning to match w values in the reverse
direction, to input node level ratios, almost solves the poroblemn on
it's own!  With no extgra forward learning.

4) But it tended to combine two inputs to one output at times leaving
one output mostly unused.  So the outputs were not blanced correctly.
Decided to add a simple learning rule that tried in the forward dirction
to push the w values to a valid probablity distribution. The reverse
larning was training to a valid distribution, like .2 .2. .6.  But it
would end up with non-valid distributions in the forward direction such
as .4 .4 .6. (summing to 1.2 instead of 1).  So I just took the numbers,
and produced a normlilized version of the same, and then trained TOWARDS
the normoralizsed version.  Not really trying to blance at all, just
trying to make the weights up up to normlaized values in both forward
and reverse directions!  This helped the system work better, but didn't
solve it.

Today

So, I got something sort of working for this special case. But I certaonly
wasn't left with the feeling it was corect.

With the help of the input normalizing learning, the system would learn
the correct patterns, for 3 layers, and then the 4th layer, was unable to
learn, and would create a solution that was an odd unblanced sum of the
inputs.  This tendency to get worse and worse was I think the result of 1%
force explore, combined with the time delay effects of the activity trace
causing the inputs that could be thought of as square waives, turning
into overlapped rounded waves as it goes through more an more layers.

I don't think the effect of it evoling thogh layers is an issue.  I thin
kthe fact that once it was too overlaped, it was no longer able to
learn, was the issue. The last layer wasn't just "more rounded" then the
previou it was a suden depature in the fact taht we had three signals,
pulsing at different times. The last layer, had two signals in sync,
and one signal weaker, as an invert of the the other two.  All of the
sequential pusing was suddenly lost beuaes it mixed the signals together
when it shouldn't have.

So I don't feel this is correct yet.

I also don't like the feel of how this is logicaly working.

The goal must be to have all hidden layers, as a set, making the most
accurate prediction of the input, as possible.  And once the signals
mixed, it lost it's ability in a big way to predict the sequential
behavior of the inputs.

So, wondering.  If the current hidden layes of my network, are a good
predictor of the current input layer node VALUES, is that all that
will be needed to make the system a good tempporal network?  Does this
network have what it needs with the basic w values and *a beahvior and
AT nodes, to be what it needs to be?  A good sptial temporal compression
to state system?

It seems to me, the answer could be yes.  We are compressing the past
history of pulses, down to an AT node value, which is a predetermined
choice about what data to keep and what to throw out.  But withint that
framework, it feels like if we can make predictions with that compressed
information, we will have done what we needed to do.

But, here's a point to consider.  The current hidden node AT values,
must be used, to make the prediction with. The current pulse, isn't
enough data.  Knowing I just sorted a pulse to node X, doesn't give us
much data to make predictions with alone.  But the current node output
level, gives us more to work with. SO I thnk we need to focuse on how
the hiden node AT level, is working as a predictor.

BUT, we can think of it as trying to predict the input layers AT values.
OR, we can ignore that I think, and use it to just try and predict
the next input PULSE.  I have attacked from this angle with previous
thoughts. But lets dig deeper here.

What if we think of the all the hidden nodes, in the next layer as
attempting to predict, which node will be the next input PULSE!  So we
train the system, to make it good pulse predictor. I liked the idea of
that concept in the past, and I still like it. But maybe we just filed
to think about how it was doing the prediction before.

Actually, no, what I was doing, was trying to predict which way the pulse
would be SORTED.  I think that's missing the point!  What the hidden
layers hsould predict, is which Input NODE, will receive the next pulse!
So nodes at one layer, should predict not be predicing which link will
be used, but which NODE at the previous layer will get the pulse --
that's different than what I was thinking before.

So, if we look at the entire column as a predictor, how does that work?
Output values, times w values, sumed backwards? Then that is a reverse
probablity distribution of the systems prediction of the likelyhood of
a pulse showing up at each node! That sounds cool.  So, as each pulse
shows up, we train the system to be a beter preditor of the probabiity
distribution.

Ok, so that sounds correct as well.  From a 10,000 ft view.

How do we do that?  Well, just a bigger version of how you would predict
the odds of one pulse shosing up at one node.  Increase when it shows up,
decrase when it doesn't sort of thing.

If we keep it local to the pulse, then we are talking about increasing
the w values of the forward fan out, and decrasing the w values of the
reverse fan out to the nodes that didn't get a pulse.

If we think of the entire network, we want to increase the value of the
distribution for the node that got the pulse, and decrease the value for
all the nodes that didn't get the pulse.

So reward all forward fan out values.  Punish all reverse, for all outputs
not just the one that gets sorted to?

Can we get away without training the entire column, for each pulse?

So, each pulse that shows up is in effect screaming to the outputs,
PAY ATTENTION TO ME! :)  And the outputs, will get this screems
from all directions, relative to how many pulses show up in each
direction.

But it won't reflect what the output value is???  will it? We may
need to adjust training based on output level or seomthing to make
this work correctly?

Well, if the training causes each w to increase by the same .0001,
then the backwards value contibuted to the prediciton will increase by
the product of that incrase times the node value. So we don't have to
incrase w faster in that case.

And if we look at signle w value, ever time a pulse shows up at it's
iput, it gets rewarded, and every time a pulse shows up at another input,
for it's shared output, it gets punished.

That will train the w value to reflect the probablity that it was a
POENTIAL input path for it's outout.

Ok, so from the output looking backwarfds, the w values will then take
on values, that accuratley reflect the odds that a pulse will show up
from different directions.

Ok, but it will have nothing to do with output value, or whether pulses
were beign sorted.

And if the inputs are long term balanced, all that will do, is cause
all thew values to seek 33/33/33 values.

Ok, so this is close, but not correct. It needs to be biased by the
output values of the node, or by the sort selection.

So w * output value is the odds that we will sort to that node.

We we don't just need to track the odds of the pulse showing up, we need
to track the odds that node value times w, is the correct predictor.

reward forward, punish backwards?  Reward all routes forward, to register
the incrase in prediction?  Punish all backwards, to pusnigh the falure
to predict?  But don't change the link used, because that's just a net
reward/punish?  Or do we need to reward it as well, becuase reward toward
q, and pusnish to zero is not a net nothing unless it stated at 50/50?

That would base the learning by the output value since we punish backwards
every time it's used.

It's so simple, I have to code it just to see what it does.  I love
simple. :)

----

Ok this last idea.  A w is rewarded every time it's input node "fires"
in effect, and is punished every time it's output node "fires". That's
another way of understanidng what I'm about to code. I don't know
if this is correct but it relly feels like it's headed in the right
direction. So the more the input fires but the output doesn't, w is
increased!  Pushing flow to the output. The mote the output fires,
but the intput didn't, it's decrased pushing flow away from that node.

But 0 to 1 learning might not be correct for this. I might need 0 to
infinity learning so that when the odds of the input firing matches the
odds of the output firing, the w value is at it's correct value.

Ah, but 0 to infinity is risky since the network only uses relative value
to determine policy.  It would be too many degrees of fredom allowing
all the w values in the net to drift upwards.  Maybe o to inf learning
with a ptz effect vs the far stronger limits of 0 to 1 learning?

lets start wtih 0 to 1 and see what happens.

----

Tested both 0 to 1, and 0 to infinity learning and nether seemed to work.
The netowrk stays around a 33/33/33 blance and just produced a blanced mix
output.  Even if it's not working, I sense it's close to being correct!

----

The runs from yesterday, that formed correct signals for the first 3
layers, but not for the 4th, are both inching towads valid signals for
the 4th layer!  I think it might be working better than I thought. But
the slowness of this learning is an issue.

16 hours of runnig to learn 3 layers of easy solution is not what is
needed here. :)

----

So, why isn't the other idea working?  Lets did into that. And if we
look at thes two algoirhtms, how are they similar and different?

The one from yesterday, is using in put node values -- which means a
history of past inputs. It's training the w values to be a good predictor
of input node values.  And since we train ever time the output node gets
a pulse, we are training the output to be a good predictor based on when
it's active. Which is indirectly training the relationship between iput
value and output node values.

With the new idea, we are trainign to predic the odds of an pulse showing
up -- which should be a close parallel to training the nodes value.

Ok, so a stream of pulses are showing up a input A, and it's driving
the w values up for all outputs.  But the paths chosen drive outputs down.

----

[3:40 PM]

coffee, facebook distractions. Hard to keep focused on this complex
problem.  My procrastination behaviors keep kicking in.  Will be going
to Star Wars this afternoon with Pig. We want to catch it one last
time before it leaves the theaters. Would like to make better progress
before then!

----

So, if the training is making w values better preditors of future pulses,
do those same values, also work to make the output behave as I want it
to? :)  Do I have a disconnect between what I'm trying to train the w
values to be in this way, and what the system does?

It sort of feels broken, that I train forward, to increase w values,
then on the path I take, I then undo that training.  If A is the intput,
and X the output, A want X to predict A. But if I don't train H->A when
I used X->A isn't something a bit broken?

Hard to grasp why it would be right or wrong.  So, I'm going to just
code that and see how it chagnes.  I'll not punish the path back to the
node that just received the pulse!

----

[4:39 PM]  Well fuck.  I got it to work.  Not reverese training the link
used was not the answer (from above).  But what I did instead, was only
use back traning.  Reward the link used, punish the ones not used!

Using 0 to 1 learning as well BTW.

No forward training required. The same test from last night wa able to
train all 4 layers to do this pulse sequence pattern in 26 ish minutes.
Vs the 16 hours of yesterdays algorithm!

Much simpler than what I was messing with yesterday. NO forward training,
no complex normalizing, just back train from the pulse to reward the
path taken and push the ones not taken.

It seems odd at first that only doing a reward for the link used could
work.  Links not being used, would have to be used, before they could
be rewarded to have them used more!

But, as long as they don't start off at zero, or we limit the low use
with force explore, they will always be used!  And if no other pulses
are flowing to that link from other directions, it will just keep being
rewarded every time it's used!

I have no clue if this works for other problems but I've have a strong
feeling it will!

I aldo don't know how good it is at balancing signals, that weren't
balanced to start with.  But again, I bet it does.  Or if it doesn't it
will liekly only be a small tweak to make it work!  I hope.

So, by only training links backwards, from when it receives a pulse we
are factoring in the strength of the node! Because the strength of the
node, is determined by when it receives pulses!  So we are training the
system to make the w values fededing a node, seek the odds of the links
being used.

I'm going to have to dig deeper into why this works. But first, I have
to figure out if it works for all cases, and not just one special test.

The one thing I worry about hwover, is stability.  This test had three
inputs, and three outputs with full cross connect (fan out of 3) but
the signals formed in the outputs, could be assigned to any of the three
nodes, so there wre 6 different possible configuations for the network
to lock itself into.  Locking into random configurations seems risksy
for behavior learning.  I can't have the signals in the lower layers of
the nework playing a never ending game of musical chairs with each other.
while the higher levers are being trained to produce different behaviors.
So I might have to add in a distanceBias trick again to force where the
output signals end up stabalziing -- which will just bias the straight
path signal to be the strongest one.

But first, I have to test more to see if this actualy works!

I'm almost scareed to test, because I fear finding out it doesn't work
at all on other problems! :)

----

Ok, started a random test.  Larger network, binary counting input.

So far, it's just suming signals. But that seems to be becuaes with
default weights, all the signals pile up on the common overlaping node.
And though it's training to sort to the weak signals, that training is
very slow beuaes so few pulses are headed to the other paths.  But I
think given time, they will rise up in value.

I might need a +1 reward and an -1/2 (1/(numLinks-1)) punishement to
make the syustem blance at lower w values?  But maybe that's not true
or correct.

Will have to wait a while for the weak signals toget enough reards to
offset the strong ones.

----

[10:06 PM]

Just got home from Star Wars!  Very fun to see it one more time before
it leaves the theaters.  I think that might have been time #5 for us.

Left a bunch of random tests running with this new algorithm.

It's not wowing me.  It's very slow to learn and adpat.  Not sure what
it's doing to be honest with a lot of these tests.

It's going to take more testing to see what's up.

I think this is very close to what's needed, but not quite there yet. More
adjusting is going to be called for I think.

If an input feeds an output, and no other inptu shares it, then it will
always have it's w value increased, and never decreased!

That should push all data to that node!  But even with this fact, my
test has been running for 5 hours. the w value feeding the lone node
is only up to .45 (from .33 at start), and only receiving a 36 ratio
reltive to the other outputs, and only receiving 2% of the outptu flow!
So it's getting very few pulses, and counting up very very slowely.
I'm going to leave these tests running overnight and see what develops.

Other tests are doing odd things like this as well.  One of the three
pulse tests from before, that I ran again, with a deeper netwrk, failed
to form the correct three pulses!  Much like the algoirthm from last
night was doing!  Will it learn given a day or two to run?

Also, the default beahvior of deep outptus, is just random noise with
no real signal at all!  Doesn't look good?  But maybe it is???

I think I see an inherent problem here.  These systems try to converge
on a solution to maximixze all the outptus, and they are all fighting
each other to find a good confiratuion.  That fighting and converging
I think can be very slow.

I'll have to test with really fast w learning to see what happens.
And i'll hlave to do more testing with full fan out and the like to see
how it all evovles!

Maybe, there's some need to force some balance in the system?  Sort of
the ptz type stuff?  That along with biased outputs might force the
system to covnerge faster as well???  Even if it's sort of a "sloppy"
coverge due to the other paramaters?

Lots to explore.  But I'll dig into that tomorrow.

================================================================================

2016-03-24 9:09 AM Thursday

Yesterday

1) Discovered a very simple and cool w learning rule that might be the
key I've been looking for.  For 10 fucking years!  Reward link used.
Punish all other back links from the node it was sorted to!  Simple
as that.  This is all based on the idea of making nodes good reverse
predictgors of where pulses will show up at the inputs? It makes the
reverse sum(w*o) a probablity distribution that predicts the odds of what
input nodes will get pulses.  I think this could be the key to perception
I've been looking for.  But tests done yesterday showed odds behaviors.
Lots more testing and experimenting needed to understand what this can
do or not do and what needs to be fixed or adjusted.

Today

Need to test more.

The 3 input 3 wide full connect net that hung up yesterday at column 2
of 9, has finally "fixed" column 2 after 10 hours.  That's a good sign.

A second verstion of the same test I started laste last night, but with
faster e-4 learnin (that still slowly deceaded down to e-7) learned all
9 layers very quickly and correctly!

The 3 node 3 fan out, but 6 wide network that has been so painfully
slow to send pulses to the unused output is still moving at a snails
pace after all night!  It should at some point, increase w enough to
force a switch to sending all data out the unused.

I'm thinking this network can work with much faster learning.  So more
testing with fast learning is on the list of things to do today.

I'm think it might be good to add a little forced balancing as well.
Either punish high reward low, or just ptz maybe?  But make it small and
slow compared to the main backwards learning so the backwards training
dominates.

Force balance learning could bias to the middle path as well, which might
be the way to control how the network assigns signals to outgoing nodes.

I need to better understand this backwards training for how it works for
data expansion where 3 signals expand to fill 10 nodes.  Does it work
at all or does it just leave it as 3 signals beause that's the best
prdictgor of inputs?  Expanding 1 to 2 doesn't reduce the prediction
power if you don't mix the signals together.  So we should be able to
expand that way.  Some indication so far is that it tends to want to keep
it as thre signals!  But if I mix in a small amount of forced blancing
that should change.

----

Just started a new version of the 6x8 3 input 3 fan out that has been
hung with very very slow pushing data to the lone output for 12 hours now
but this time with much faster learning to see what happens.  e-4 to e-5.
I tihnk that's what I set the low at, vs e-7 of the other.

Will run to Starbucks while that cooks.

Actually, the lone link's w value was only up to .49 in the slow test.
The one I just started is all the way to .68 already.  Fuck the slow one.
I'll just let the fast one run.

----

[10:18 AM]

Back from Starbucks and got Bagle BEC which Molly came down to share.

The restarted fast learning version of the network with single feed nodes
has pushed the w value for the single output to .99999something but it's
only geting around 50% of the output becuase the other w values don't
sum to zero!  I don't know if this this cool, or broken!  Good or bad?

If I change to w values that do 0 to inf, that would just cause the
w values for the single ouptut to march to infinity and never stop.
That can't be good.  But, this single output can make better predictions
for the input node, if all data is sent to that one.  So this network
has not confgured itself into the state of most accurate prediction.
Wait, or has it?

A signal can't predict itself, unless there is strong temporal
correlations in the signal -- as with my my pulse cycling signals.
The strongest predictor of the next pulse, is the last pulse in that
signal.  If there are strong spatial correlatoins, then those need to
be factored in by combing signals.  So though the one route wants 100%
of the signal, if the other neighbors are correlated, they want and
deserve a piece of the action as well.  We have to balance the need of
precting signal C, with also predicting signal B and A!  and if C helps
make better predictions for B, we can't route all the pulses for C to
where it can't be used to predict B!

And maybe this is what this split is really telling us?

But is it telling us the correct answer how I have it coded now?

Or do I need to normalize the forward values as well, as I was doing in
the other code so as to force the system to find an "honest" configuration
that correctly balances the use of signals to make predictions with?

Or, do we have a more fundamental problem, that nodes feed by only one
input aren't balancing anything so their vote shouldn't count anyway!

Lets reproduce this same test, with full fan out, so there is no doubt the
system is balancing the use of all inputs in all outputs, and see what
sort of solution the system produces! And see how that compares to what
the system is doing with the limited fan out and the hanging nodes, etc.

----

[12:09 PM]

Well, learning some stuff.  With fast learning, the networks converge
faster and I get a better feel for what's happening.

The three pulse pattern, that has strong temporal self prediction in
each signal, and strong negative correation between signals, does a
fine job of converging on straight through wiring for deep networks,
if the learning is fast. When slow, the test from yesterday is still
struggling to converge on straight through wiring.  But given the time,
I'm sure it will converge on the same answer.

But, when I fan out that three pulse pattern to 6 wide full fan out
network, things are bit different.  The first layer, split inot 6 equal
sized signals -- so the same 3, just split into 2 copies of each.

The third layer, was 5 signals, and one signal mostly unused.  So two
were split, one was all on one input mostly.  The 7th column is back
down to three sigbnals, and 3 outptus mostly unused (very small weak
summed signals).

Now, maybe in time, all layers of that 6 layer will be like the second
with 2 copies of each signal. But the netowrk seems to struggle to
blance, when the best solution, is basically just to keep the three
signals separate.

So, if I want fan out to work, I think I do need to add this touch of
forced blancing.  And for this application, where the goal is to route
signals everywhere, we need more balancing.

In fact, now that I think of it, keeping the sane signal going to the
same place is a problem.  It ca't learn RL if all signals don't en dup
going to lal aoutputs!

----

So here's an interesting and important question!  Even though the output
at layer 7 is a copy of the one input signal, what is the actual pulse
mix of that signal! How much it is a copy of the pulses, vs just a copy
of the signal?

To sovle the RL problems, all pulses have to go to all outputs I think.

Got to test this.

----

Now, with the binary cycling pulse, we have something different happening!

With 3 fan out on a 6 wide network, starting with 3 input signals
counting in binary, we get a mix of intesting different wave forms
after a few layers and a network that seems sort of basically balanced.
But we don't get get "counting" behavior.  But more of a problem(?),
by layer 7 there's lots of instiblity with the siganls drifting up and
down in volume over short periods.

Could this set of waveforms help the sytem learn the edocig probnlem or
not? It's hard for me to tell.  frequency input as it's dominate factor
and the high frequency counter has turned to mud mostly by the last level.

More important however, is that this same input, when used with full
fan out, turns to nothing other than just the sum of the three inputs
spread across all 6 outputs!

This doesn't feel so useful.  What's up with this?

I thought that's what I was trying to stop with this new code! :)

Gee, this problem just keeps getting more and more complex.

I bet it's something to do with the binary counting problem.  Each input
is high and low for equal amounts of time.  So the avergae expected
value is constant at the middle.

This form of learning I'm doing is adjusting w value to factor out bias
at each level but a binary signal has no bias to factor out?

Maybe that's sort of what is at work here?

This learning is predicting the odds of a pulse showing up, but mnot
predicting the input node VALUE -- which would be predicting it's
temporal FREQUENCY!

Hah, maybe that's the issue. I'm predicing pulses, but not frequencies?

But, when the frequency increases, the odds of a pulse showig up at that
input increases.  So my prediction system, if it wree doing a good job,
should catch that should it not?

And with the sequence pulse input, it does most certaonly create solutions
that predict the incrase in frequency!

So maybe that's not the issue.

Is it possible that what I'm expecting this system to learn, is just
beyond what is possile for this system to learn!!!  That no combination
of w values will solve the problem in the way I'm thinking!

--

Well, I need to figure out how to add bias to help force the system to
pick straight through paths for the "answer" with the strongest use of
each signal, so let me work on adding that and see how it changes the
picture for these probelms.

So, how to add bias????

----

Used distanceBias to weaken the punishment for nodes further away.  Shit,
wait, isn't that backwards? I was trying to strengthing the reward for the
straight though node, but ended up coding it to weaken the punishment.
But I think I should have increased the punishment for nodes further
away. Or weaken the reward?

Well, lets see what this does..

----

Attempts to add bias aren't working well.  Added learning bias to both
reward and punishment to reward the straight path more and the distant
once less, and pusigh the straight less, and distant paths more.

Also started w vales to be higher for the striaght paths. But the net
still didn't settle by default to 1->1 wirring for the three pulse test!
The first layer got two switched, but all other layers got them all
correct.

Is is due to the system starting with a long run of one signal, The
messes up the other signals right at the start.  Wait, but why would
that be the case???

----

Ok, just force system to start with straight through wiring of w=1.0 and
0.1 for non straight though, and that sure the fuck forced the bias to
stay that way!  Along with learning bias favoring straight wiring!

Now. To test other inputs than the one that results in straight
though and see how it evloves over time!

Maybe just starting with the w bias is all that really needs to happen?
As long as the enviromet is stable, the problem of drifting assignments
of nodes might be a non problem????

I guess it depedns on whether we need large fan outs or not. If large
fan outs imporve learning large fan outs will likely be more senstive
to assignment drift. If we build huge networks out of small fan outs
then it seems tobe the drift is less likely.  Got to keep testing
to understand more about what this w learning does.

----

[9:52 PM]  Went to see Eye in the Sky movie with Pig.  Just got back.

Also tried the DC Buck converter on my bike today for the first time.
Increases the voltage from 36V (41 ish) up to 55V which gives higher
top end speed.  Rode modified bike to Pizzia.  Got it working, but
there are issues.  Current limits, bike dies on start from stop, have
to get it rolling first.  So not as much power at start in trade
for a little extra high end speed.

Anyway, not a lot of work on percepttion today. To much distractions.

Made system start in straight wiring, to force bias of straight though.
That worked, but I don't like it.

Added learning biase for straight node.  That seems to help as well.
I think. But wasn't enough without w values set to wire straight though
by default.

Set up a 5 wide network with fan out of 3 on the 3 pulse input.  Solved it
just fine, but it took a long time to wire those non-straight paths.
Ended up with two of the signals split in half, and on signal not split.
The only way it can fit 3 signals, into 5 outputs! (where the signals
don't want to balance.

Genreally I like what this is doing. It's ability to maximize
representation of the inputs seems cool, and correct.

But I need to test more to see if this is really waht is needed and what
is correct.

There's this odd problem that since the point of this is to create state
to drive behvor, what we need may not be what I think it is.

I need to test this same config on full fan out to see how that works
with the new biased learning features. The old version ended just merging
all the signals together. Didn't like that so much.

I'm sort of bummed that I'm not able to get really excited about figurout
out this simple learning rule.  I feel like maybe it's what I was looking
for, but that maybe, I was looking for the wrong thing!

I'm hoping that's not the case. And I've learned some new cool stuff
about how to do this processing even if there's more work ahead.

----

I'm thinking, these bais tricks to try and force stability are all
wrong.  I'm think the best way, is to make it happen with topology.
If the network though random chance stablizes on a solution that
is fighting the bias, then we end up with a "weak" confiation that
might "break" at some oint in the future.  I would rather see
a "strong" configure due to it not beinga falsly biased so that
whever it setles on, it stays biased becuase it's seteled on that
solution.

Just a thought. :)

================================================================================

2016-03-25 9:02 AM Friday

Yesterday

1) Studying the new simple w learning update that works well for prediction

2) Added tricks to force bias to try and increase stibility.  Learning speed
bias baesd on distance, and pre-wiring net as mostly straight though
with w values of 1.0 and 0.1.

Today

Batman vs Superman movie this morning.  Leaving soon.

The issue with this new learning rule is that it isn't all that stable. It
doesn't converge stronly onto only one solution.  That seems inherent
in what we are trying to do here since all the nodes are fighthing each
other for balance the who system tends to be wobble easilly depending
on what the balance is.

The tricks I tried yesterday seemed too agressive. They worked for the
three pulse problem but for others that made learning very very slow as
the starting 0.1 w values had to be slowly increased to higher values.
And I don't know if I like the feel of the learning distance bias I
have added.

Maybe I should try a bit of flow balancing bias and see what that does.

----

[1:02 PM]  Back from movie

The 1/10 the speed output balance left the last few layes just summed
together.  They had not yet created blanced outputs.  Might never?

The 1/100 was fairly balanced.  the 3 pulses had split into 5 for
all layers. But some were stronger than others, some were more stable
than others.

----

Thinking about balancing  The last test code was punish the strongest and
reward the weakest. But thinking about this I realize that motivates the
system to kill signals with high peaks and or low vallies. In information
maximising, we might need some signals that peak very high or drop very
low at times.  We don't want to motvate the sytem to create a certain
shape of curve.  What we really need is to train for total flow balance
-- but not link flow blanace.  We need to train the outputs to make the
area under the output level curves equal. (which might not be exatly the
same as pulse count blancing but it should be close enough to be fine).

So, I need to reward or pusnish based on output level I think.  But how
to do that?  If I use an average signal I could make it relatve to the
average. But it would nice if we could skip that.

Ah, but I now reward every sort to an output. That's a postitve effect
we can offset.  Just bias it very slightly relative to the output level.
So when sorting to a high output it's not rewarded as strongly as when
sorting a low output.

We could do the same for the reverse punishments as well.  punish high
outputs a little stronger than low outputs.

Of course doing the extra reward separately really makes little
difference.  than if we do it as one.

But if we only punish sorts to a node, then that's equal to puinishing
high.  Odd, so wait If I just reduce the size of the postive punishment
that makes it relative to the output?  Nah.  That can't be right.

So, looking at one input node forward, what do we see now?  We see
positive reewards for all sorts to nodes.  Which means the balance is
really just controlled dby the pusnishments?

So, think of a binary sort, where the two ouptuts, rae shared with two
nodes to the side, and no overlap.

I positive reward every time I sort.  But my w  values are being
negatively rewarded ever time THEY sort.  The odds of me or they sorting,
is all amplified by the *a effect for correlations..

Ah, so no matter where the pulse is coming from ths system balances
the correlation -- whether it's correlated with my own pulse or from
their pulse..

If I (speakign as an input) am more correlated with an output than
my neighbor, more of my pulses get sucked, and his gets punished.
Which sends more of his pulses down that other path.  So the w values
really show the back correlations of the input signal with the output.
Really, like 5% from X, 20% from Y, and 75% from Z.

So if one output is really weak, it's rations can still be 100/0/0 but
be small flow volumns.  Then from the other side, the guy feeding it,
can be 30/100. So the flow volume is really just 23 and 77.

It does seem to me, that the forward numbers should sum to zero as well
because if they did that, it would force the system to be at least as
blanced as the inputs were blanced to start with.

I was doing this trick in the old traning.  But is it right to do this
and was I doing it correctly?  What if we train the forward numbers to
match the outgloing flow?  Wait, didn't we already try that?

No?  Reward the path taken, punish the others? That seems exactly]
backwards from flow balancing.  Shit, but it's wonderfully symetic with
the reverse training and it seems as if it will attampt to force the
forwad view of the numbers to seek a true measure of the forward value?

Ok, is seems wrong enough that it might work. So I have got to code it
and test just to see what it does.

which means we double reward the sort path taken.  Odd.

----

No, that does't work at all.  It just forces all pulses to flow to the
same place.  No blancing at all.  Not surprising since the update we
just added is inherently an un-blancing learning.

let's try what I did before, which is to train forward to push w values
to sum to 1 to make them accurate probablities of both the forward and
back sorts.

just to see what it does. I'm tried of tring to figure out what's
right. Time for so more good old random trail and error serach.

----

Pushing to sum to 1 doesn't work. Again, tends to cause all output to
go to one or two outputs.

----

Found that the call to Link() had fanOut hard coded for 3!  For what
years now? :)  Fix that.

----

Try pushing the w to even balance instead! .33/.33/.33 or .2/.2/.2/.2/.2
for the fanOout 5.  That will push towards output balance.

----

Fuck.  Just had an idea.  Why am I trying to balance this?

The input I'm using only has 3 states for the environment.  Why would
I expect a 6 wide state system to "balance" when there are only three
states input really?  I shouldn't expect it to balance.  A more complex
input will have lots of "states" to pick from and might automaticalkly
balance between them all just fine.

There is this idea that a state input which is constant, could, in theory,
by divded up into n pulses.  But I think maybe that's a mistake on my
part to think the system should do that on it's own without more data
to help it.

Yeah, right.  It can't know when to switch from one state to the next
if there are no imputs that define "switch"!

And on the issue of motivating the system to assign states in some fixed
order, doesn't that fight against any RL learning that tries to force
states to different places?  Why put in a bias that the RL system would
have to fight agains? We want the states to move to different places
easilly if the RL system indicates it should move by even a small amount.

The idea of trying to tie them down is a perception concept.  But the
whole point of the entir enetwork is to map states to actions and it seems
like the whole system should be free to drift with even the slightest
push to allow that to happen!

So maybe, what I really need to test her, is more types of complex inputs.

What happens, when I add random data flow to the 3 pulse test for exmaple?
Does the randome mix everywhere or get issolated to the empty outputs
or what?

This sounds like the path to test.  Give up on blancing, just use the
clean straight forward w learning I've uncovered, and see what it does.

The good thing about thi slearning, is even with 7 layers and 5 outputs
the full fuan out doesn't sum together!!!  Which was an honestly bad
porlem. The data from the inputs makes it all the way to the outputs if
they are strongly temporally correlated.

----

But, the binary counting Input didn't seem to want to decode.

So, is this yet another type of percetion issue?  Shouldn't a decoded
version of the input be a very strong predictor of the binary inputs?

Is making it decode signals a different probnlem than what I solved by
making maximising prediction with this cool learning algorithm?

Ok, so, need to add more signals, and need to test the decode question.

Also, I wonder if my decode input was running at too high a frequency
for the system to really decode it?

----

Ok, so the 1/100 push to flat is creating more than three outputs more
easilly than the 1/10 push to flat.

It seems the push to flat, which was intented to try and blance is
actually making it lock onto the strong signals and forcing the weak
signals to stay dead it seems!  So it's doing exactly the opposit if
what I was trying to make it do.

But, new thinking, is that's ok that the signals drift and change and
we don't need a "stable" solution for percpeiotn but rather one that
can drift easily as pushed by RL!

I might change my mind on that later, but for now, that's the game
plane. Driving is not bad, it's good!

----

Wrote RandomPulseGenerator

----

Added Random noise input to 3 pulse test to see how it changes Forgot
to turn off 1/100 pull the even. :)

Created three pulse test with one pulse overlaping the other two to see
how that works!  Running tests with and without extra random input.

----

[6:39 PM] waiting for Pig to do the cats across the street so we can go
get food.

----

Idea on distance bias.  Adjust the 1.0 value on learning rules by the
distance bias to limit how much strength nodes have on distant nodes.
That might be a better way to apply a distance bias to the network.

----

[12:25 AM]

Funny.  4 tests running.  With two levels of balancing, and one with
distanceBias activated as well, all to test how they work to stablize
the system -- and the one that was the most stable, was the one with no
distanceBias and the lowest level of blance in the set.

Tomorrow, test with no balance at all on the same test, to compare!
Or, really I have to start it up tonight don't I?

================================================================================

2016-03-26 11:37 AM Saturday

Yesterday

1) Continue to play with ideas for making the new learning info max
learning alrogirhm more stable.

2) Fixed call to Link() that hard coded fanOut to 3.  Been like that
since forever I guess.

3) Decided maybe I was wrong about the system needing to be stable
and maybe it shouldn't be stable?  Maybe only RL should stabaliizing
the chaos?

4) Still thinking of stablizing ideas anyway.

Today

Work up at 3am.  Still awake at 5am.  Runny nose starting to devlop.
Too little sleep? Polen? Took sleep medication.  Didn't wake up until
11 am.

Left 5 runs running yesterday. Thought they were overlap pulse tests. But
they were actually all sequnce pulse tests.

Can't really say that any of the stabalzing techniques make the system
better balanced than having none of them.

----

Yesterday had the idea of adjusting 1.0 target of learning rule
using distancBias.  That that was a new idea.  I realized later,
but didn't write it down, that that shouild be no different than just
using distanceBias as a policy factor as it was orignally implemented.
With current learning rule w value just seeks to document pulse ratio of
incomeing pulses to output node.  Setting 1.0 to .5 just cuts the value
in half.  But multipying by .5 does the same things.  So not a new idea.
Didn't test to confirm.

----

Here's yet another balancing idea.  Stop trying to do it with the same
w weight, and add a new blancing weight, and apply the flow blance
learning to the blancing weight which is then multiplied with the w
value in the policy.

Not sure if that will be differemt but I think it might well be.
Will have to try that today.

The flow blancing learing could for exmaple be faster.  Ah, but no, then
that would probably just make it overpower what the info max learning
was trying to learn. Ah, but no, maybe not!  Maybe we don't care if it's
forced to blance, and info max will then just shift the flow to create
a better signal?

Yeah, I need to test that.

And/or, the active balancing code could balance to the target defined by
RL as well..  So maybe we think of it as part of the RL system instead
of prepetion alone? :)

Off to starbucks first however.

----

[1:11 PM]

Looking at the test runs again, I would say theone with the *1.0 balance
and the distancBias in use has rechaed one of the more stable states,
if not a well balacned state (one signal is much stronger than some
others, some very weak).  But it's more stable than the same run with
no balancing features.

I stil don't know if these balacing texchniques are bad or good. But
none of them strongly stabalize the system. But, I have to question
where strongly stable is good.

I guess on a larger scale, what should strongly stablize the system
is a large network to fan out ratio, so that most the path of data is
controlled by toplogy, and not by learning.  Forcing visual data into
one part of the net, auditor in another, etc but also limiting where
different signals form. I won't be able to get a better undrsanding of
that until I have larger data sources to play with.  Not there yet.

----

Going to code the balance using separate variable idea and see what
that does.

----

Testing the flow balancing idea with a few f variable as part of
the policy.  Runs 0 to 1 like w, but is trained separatily from w.
Trying it with wLarningSpeed as well as *0.1 for 1/10th as strong.
The full speed is activitly offseting w learning and turning active nodes
off by punishing them.  I'm just doing punish low reward high learning
at the moment.

So it feels like f learning is far too fast since it's just activlty
sutting down signals.  But I fee I need to give the system time to
stablize to see what happens.

----

On the idea of percpetion needing to be stable. If the goal of perception
is to just create a foundaion to learn from, where all the "real" leanring
is RL, then we can think of percption as a soruce of chaos that learning
is built on.  That works when we think of RL as learning to just connect
A input to B output. But when w need to comine signals in a careful
balance way, and keep them balanced in that way, then it feels like we
need a stablze perception system to do that balancing not RL.  That is,
when the solution requires 2A + 1/3B + 6A as the "answer", that formula
needs to be created by Perception breaking down to a raw simple state
and RL combining the right pieces back together.  The pieces we combine
back together, must be stable. I would think.  So it feels like there's
some real need for some level of strong stablity here.

But maybe, most of that will come from the toplogy instead of from
the learning????

That's what I need to figure out.

----

Changed f to seek the value which measures the imblance.  So a 2:1 flow
emblance sets f to 1:2.  So it won't keep adjusting until blacned, it
will instead find a blance point where the amount of imbalce is equal
to the adjustment.  That sounds better than an arppach that attempts to
erase what w learning is doing.

The fast erase version is shutting down nodes in the middle of the log
poulse and forcing the system to switch a second output to carry the
value.  Producing an intersting, one of N pulse outputs for the 8 outputs.
But it's not stable and consistent, it's highly  unstable and random
as to which pulse is dominating an input signal next. The low
learning is doing the same sort of thing, but it's slow enough to allow
one output to carry the bulk of the signal for an entire cycle, but
then causeing the it later to swtich to another output path sort of
randomly as well.  Will have to let it run longer to see what it stabalizes
on.

And compare tha to this new run which is not trying to ersae the
imblance but just adjust it some towards being more balanced.

----

yeah, I'm liking the new version of f learning that doens't force a given
level of balance but rather just measures the imbalance and creates an
adjustment equal to the imblance which allows the system to stablaize
on a given solution evevn though it's not blanced.

What the 3 pulse input to 8 node output seems to be stabalzing on is a
2:3:3 assignment of input pulse states to outputs. So the f learning
will keep this dynamically balanced that that level I'm thinking but
not try to keep fighting it to make the 3 outputs (what are weaker)
strong er or the 2 outputs, that are stronger, weaker.

have to see if it's able to stablize in that configuration.

The other runs, are just constantly fighting each other, and trading
off from time to time, who is "winning" making the system unstable.

When we have to confilcting learning forces at work, the must work in a
way that they can find a stable point for balance.  But does the current
f and w leanring fit this or not?

What happens, when the amount of imblance, like in this exmaple,
doesn't actually match the imblance crated by this very much discreet
assigmentof 2:3:3?

I guess some extra signal will end up "leaking out" into the other paths?
So some of the 2 signal will get forced to leak out into the 3:3:
paths? Maybe that's fine if that is whta happens, as long as the leak
levels are all stable and good.

----

The one good run is doing something very odd.  It's locking on the 2:3:3
pattern on the 3nd and 3rd layers, bu the col 7 output is stlil very
messey with sin wave pule sort of shapes.  BUT, it's forming 7 distinctly
termporally shifted pulse outputs!  A 1 of 8 decoding.  Roughly.
What will that converge on????  But even more strange, it's sorted
perfectly from top to bottom!  How the fuck could that even happen???

The flow blance, if it's causing data to "leak out" a little at each
level, might in fact end up creating this sort of even spread of activity
over time, creating blanced sparse distributed encodig?  in this case
1 of N, but in more complex case, maybe n of N?   Thatwould be fucking
cool if that is what happes over many levels of leaked temporal data!!!

----

Yeah, the first two apptemps using punish high reward low just produced
chotic messes.  Nothing is stablizing.

The last one, that adjusts by measuring imboance is far more stable,
but the signals are certaonly drifting and not locking onto a simple
stable solution.  The 3 outputs are trying to eat into the 2 outputs
sort of. I think. I'm not sure. have to let it run.

There seems to just be an innate issue with the ifo max, with this strong
temporal pulse signal, not being able to balance. There is no solution
that fits the info max, and the idea of a flow blance when we have the
three strong inputs, and 8 outputs, where 3 doesn't divide into 8 in
any sense.

I should set up a 3 to 9, which could balance and see what happens.

And a 3 into 8, with 3 fan out instead of 5, to see if the forced toplogy
helps to stabalize the produced answer?

----

Started a bunch more tests.

Here's an idea.  To allow it to balance, maybe I shouldn't worry about
trying to balance outputs, but rather, just apply this same technique to
balance pulse flow out one node? Measure imblance and just to the point
of flow matching imblance and not worry about total network balance?
Let it work it's on flow out by pushing each flow closer to balance?

Certainly RL will only care about balancing a givin fan-out.  So must
attempt to blance the fan out?

The goal I'm seeking is a design where there always IS one answer to
converge on so the system and alway converge on that ONE answer and
stablize!

So instead of perfect inof max or perfect balance, just be happey with
sort of info max sort of balanced.

Sounds like it might be able to work.

I think having one answer to converge on is key here as opposed to a
system that is "never happy" with whatever solution it currently has.

================================================================================

2016-03-27 1:28 PM Sunday

Yesterday

1) Added f learning.  Separate varible in policy to do flow balancing.
Trined it with the exact mirror opposit of w learning with info max. 
Forward learning, punish path taken, reward all paths not taken. 
Policy is f*w (and *a of coarse)

2) Started more long running tests with this new form of flow balancing.

Today

Well, it's Easter.  Sort of taking a day off as I'm letting my tests
keep running longer to see how they converge.

The new balancing learning with the extra f parameter doesn't seem to
be any new perfect solution that stabalizes the w learning.  It still
drifts some with larger and weaker flows slowly over time.  But it does
already return back to a sort of balanced configuration.

Running a test without balance, made it clear that it would tend to create
some outputs off, and some very strong, though it might drift over time,
with blance on it cleary remains more blanced in the long run even if
it's still not stable.

I think the w learning for information max is just inherenly unstable and
you can't make it work, and balanced, at the same time.  Either we have
the information maximizing that work so well to keep signals passing all
the way though, but we sufer instiblity, or we use a blance algoirthm
that makes it stable, and turns the signals to mud.

Having a sepasrate varible for flow balancing seems to work. NOt sure
if it's better than mixing it with w but I like the feel much better so
I'm sticking with it for now.

Also, this flow balance could be set up to implement RL by making the
blance target defined by the q values or by RL.  Might try that later.

Letting a 9x8 3 pulse test run for a very log time.  With 9 outputs,
the 3 divide evenly and stay very stable.  But it's taking FOREVER
to train all the way though the netwokr.  Took overnight for the last
outputs to turn form flat, to start showing signal. And the signal is
slowly growing larger and larger. But it's still slow wavy lines instead
of square wave pulses.  Want to let this run say for another day to see
how squrae it gets. With the forced 1% 3exploraiton effect of the network
it will never be as square as the itputs, but it will be intersting to
see how strong the outputs may get. 

I need to test this without blance to see 1) is it as dead stable
as this is, and 2) does it train faster without the blancing trying
to flaten it out?

----

Just to be clear.  3 pulse signal.  9 output, but 5 fan out, is not
stable beause 3 doesn't divide into 5.  Or 1 doesn't spit into 5.

3 pulse signal to 9 with 9 fan out, is stable with flow blance.
Testing now without blance to verify what happens but I think I alrady
tested and determined it was balanced.

----

[11;47 PM]

Not much work today.  Lots of catcing up on Mythbusters.  Only 6 shows
left now from the Chirstmas Marithon.

The no-balance test of 9 output 9 fan out certaonly did not balance.
A few outs were ver high, a handful for near zero, and it never balanced
the outputs.  The version with f balance turned on certaonky stained
blanced -- but it took a long time for the signal to make it though 8
levels (all night), where as the version with w learning only, had the
(unbalanced) signal work it's way though all levels in less than a hour
(or something in that range -- didn't time it).

So balance certaonly works well when it's not fighting the w learning
trying to unblance due to strong pull by a limited number of strong
signals into a small number of outputs.

But with enough complexity in the signals, thebalance works fine, eve
if the outputs tend to drift over time due to the pull of w learning.

At the moment, my belief is that it's not possible to botgh do the w
learning for info max correctly and make it stable at the same time.

So, I have f learning for balance, and info max w learning now that we
can use as tools in the network alone wtih RL. Both I really like, but
I'm still finding out how well they work and how well they fit together
with RL etc that I'm trying to make work!

A few long running tests are still running for the night.

----

A point about blancing.  It occured to me, if I want to turn down the
power of balancing, the way to do it, is in how the policy mixes f and
w values together.  Same issue with b values for RL.  If the values are
normalized to be 0 to 1 values, then we can just map 0->1 to .50->.75
(for exmaple) to weaken it by 50%.  Or do it relative the max() of the
numbes, or to the avg() of the numbers.

So, it should be easy, to experiment with different levels of power and
control between RL (b learning), Flow blance (f learning), and info max (w
learning) using this sort of hide in how they are combined in the policy.
Oh, and *a as well can be tuned as well.

================================================================================

2016-03-28 12:31 PM Monday

Yesterday

1) Nothing really. Just letting tests run longer to better understand
how the f learning balancing works with the infor max w learning.

Today

Off to another slow start.  Got up early, but was posting about Basic
Income and AI on facebook instead of working on AI.

So what have I learned.

Without balance, the system with only w learning will often not balance
throwing lots of data to only a few outputs beczause the nature of the
data demands that -- low informaiton content data.

Without balancing, node assignments become more random and can drift
a lot.

With balancing, it will do a better jobs of balancing, but can do such
a good job, that in deep network, the information from the inputs never
(seem to) make it to the deeper layers and outputs.  They just stay
flatlined and balanced instead.

For RL, which is the whole point here, it's unclear how important
balancing and this info max type of learning is to making it all work.

But it seems the info max is a key compoment I have been missing to
allow the nework to do solve the "decode" problem.

I need to go back to testing tha tnow with the binary inputs to better
undrstand how the info max solve that. And to verify it can solve that
and that we don't still have something else important missing.

----

Moving slowly I guess because I'm sort of lost?  Or maybe just
procrastinating due to the idea that there's a lot of hard work ahead
to verify what works and doesn't and to fix it?

I am getting this feeling like this isn't going to be easy.  That just
getting the low level stuff right, and then BAM all the high level stuff
just falls out.  Every time I get the low level a little better, it only
seems to create more problems for the next step, and to make the next
step of debuging what is wrong, harder becuase the system gets bigger
and more complex.

But I must keep pushing forward or else I won't make progress.  Wheneer
I feel like this, the best thing is to just keep pushing forward, and
then I find something that turns into the next big step forward.

Don't look, you won't find it, and you won't get to the end.

----

Just started up 4 tests for different binary counting configurations
to test. All wtih both w and f learning turned on.

----

Question -- given many signals to pick from, what does this system pick
out?  Is it the strongest signals mixedin the data that is assigened
to individual ouptuts?  Or the ones with the longest frequency slowest
change?

I could test by mixing together some different signals, and see how good
this system is at extracting the data into outputs.  And if there are more
signals than outputs, what ends up happening?  What signal is extracted,
and what is not extracted?

Seems to me, the one with the strongest correlations will have the
hignest priority and "win" the battle for control of an output node.

I should mix like 5 random noise signals together with two non noise
sin wave signals of different frequencies, scattered across the intputs
and see what happnes!  What power does this system have at finding and
extracing the common sin wave signal?

I guess the mixing has to be some linear combination for it to be able
to find and extract????

yes, I need to reserach this.  But need to let thse binary tests run
and see what emerges first.

----

Down to only 6 mythbuster shows left to watch from the Christmas
Marithon. 3 from old shows, 3 for this year.  Will be sad to have them
all finishsed!

================================================================================

2016-03-29 1:03 PM Tuesday

Yesterday

1) Finished watching all the Mythbuster shows!  So sad to be done.

2) Started test runs on binary counting/decoding experiment.

3) Not much else.

Today

Pig has taken off to go pick up Julie at school.  Julie will be home
for another week. :)

----

The test runs are sort of intersting.  I had a couple running with 3 input
binary counting, one with a 4th balancing input.  And then a couple more,
running at half the frequency to slow it all down.

The slower one, with a larger 10 wide nework, produced a very nice lookig
10 output signal in sync with the lowest frequency of the counting cycle.
But all phase shifted, to create 10 diferent signals with peak activity
a different points in the cycle -- exactly the sort of thing I was hoping
this network could do.

But the networks with the faster intpurs, failed to do this. The outputs
are highly noisy and chaotic and driftig.  Though the lower level
network layers produce an intersting and seeminly useful selection of
different signals.

It feels as if there could be an inherent frequency limit of the system
so that when the signals chagne too quickly relative to the halflife,
it cna't learn to "decode" the signals, but instead just ends up mixing
them all together after enough layers.

So I need to repeat these tests with different half lifes to see if the
faster signal can decode as well, if I just lower the half life.

And I need to understand what happens with a faster half life, on the
slower signal -- does it still decode? Does high frequency signals start
to dominate or what???

----

Another possible idea -- use nodes with different half lives in the
same layer to create a form of FFT type frequency senstive effect?
Might be a useful or edven required approach?

----

Some posted an interesting article on "Bayesian machine learning" aka
FastML? I need to stop and read that now...  There seems to be great
parallels in how my nework is operating and in Basiasn approach. I need
to beetter under this.

http://fastml.com/bayesian-machine-learning/

----

Ok, that paper above wasn't so good.  Overview was a little too high
level and the other paper I chased down as a little too detailed.  No good
simple ideas to extract without more hard work learning a lot more...

----

Started a new test of one of the high frequency tests.  Set half life
to 500 vs 1000 to see how that would do.  However, that version wasn't
counting in binary correctly. The lowest frequency was shifted in time
so the count wasn't true binary.  I would thikn this network would not
care about that and deal with it, but my test I've started is counting
in binary correctly. I idn't go back to the same counting style used in
that run. I'll see how the binary works first when the half life is half
of what it was and see what that produces!

----

So, change freq is called every 3000 seconds. The high speed version of
this pattern has 8 cycles, so the full cycle time is 24,000 seconds
or 6.6666 simultion hours.

And with a half life of 1000, the signal, that cycles at 24,000 is lost!

Odd.  But at 500 half life it's showing up in the last output.

Odd mapping. 

To better understand the behavior I'll need to change to something that
is easier to grasp time wise vs 6.66 hour cycle time.

But I have multple tests running at this frequence to see what happens
for 1000 half life, 500, and 250 seconds.

So far, the 1000 half life is turning to flat line output (well very
weak signals).  The 500 is showing strong sin wave like signals as the
lowest frequency.

At 250 the output signals at the last level are even stronger, but the
signals at all levels are more noisy with low level high frequency noise
making he lines jagaed enough of smooth signals.  I think at this lower
half life, we are able to see more of the stoatic noise of random pulse
routing showing up in the lines.

To get smooth curves, we must use a half life lone enough to reduce that
noise down to the one pixel level in the graph data.

So this is a trade off between swtiching noise and being able to see
the long term signal noise.

Which brings up th equestion -- who well can this stocastic system process
signal noise that is at the frequency of the stocastic noise? Or does
it end being swamped by the random stocastic pulse routing noise?

----

Certaonly with the lower half life of 250 the signals all start to
look like square waves vs sawtooth when we run the same signal at 1000
halfLife.

================================================================================

2016-03-31 12:36 PM Thursday

Yesterday

1) Nothing Really.  Let the tests keep running.  Didn't even add any
notes to the notes file for the first time since starting this big push!

Day Before Yesterday

1) Tested changing half life from 1000, to 500, 250, and 100 for this
three pulse test to see how w learning reacts.  Conclusion, halfLife acts
like a high frequency signal filter.  To the extent that when it's too
high, the w learning is unable to pass the information in the inputs on
to the outputs!  The outputs just flat line.  At 1000 sec halfLife and
the higher speed version of this counting pattern input, the outputs
flatline -- then turn into random noise -- amplifying internal random
noise I think.  But at 500 and below, the major cycle of the input
couting gets passed to the outputs.

Today

Really loosing my motivaiton to keep working for some reason. Can't
quite figure out why.  Letting facebook and other things distract me from
the work.  Maybe just a bit of burnout from working too hard for too long?

Sort of fear the rest of life is chatcing up?  Running out of money.
(have enough for about another year, then I'm mostly dead broke) and
will be forced to make hard decisions.

This new w learning algorightm feels like it could be the missing
piece of the puzzle I've struggled over for 10 years now.  But yet I'm
not excited about it. Maybe, the fact that the output ends up being so
noisy and random has me concerned that it won't as well as it needs to?
Or that this whole approach has issues?

Or do I have some fear that it might work?  And then I have to face the
harder question of what to do next?  I mean, even i fthis is the perfect
algorithm, I have to plug it into a much larger and complex environment
and test to see how it works.  Is the shear amount of work that is going
to take scaring me away from going down that path?

In the recent past, small advances here got me so excited I couldn't
sleep.  But now, instead of insane excitement, I'm just feeling burned
out.

Finishing watching Mythbusters left me oddly sad and empty feeling. Then
Pig left to go pick up Julie, but didn't even bother to wake me up and
tell me she was leaving for two days.  That put me in an odd mode as well.
Not that I deserve better treatment from her, considering how I've been so
distant myself.  But it did affect me.  And having to do this work alone,
doesn't help. I like to share, and need constant ego gratification to
keep me motivated.  And I get none of that doing this alone.

Oh, and there is this crap of having to debate Michale about Hillary
vs Bernie on facebook.  It really bugs me to have to debate a friend
about something like this.  A friend, who is very stong headed about his
beliefs as well.  And one that has been left me depressed about having
to talk to him about my life and my relationship with Pig.

I just so much want to escape this life, and be free of all this crap.
Not being able to escape is certainly burning me out.

----

So, enough of that.  What to do next with the software.  I could dig
deeper into understaning the power and limits of this new w learning
algorithm.

Lets try a quick test of decoding learning and see if the system can
actully learn to sort states to outputs?

----

[3:07 PM]

Set up overlap test to with cycle of 8 count, to try and learning to
control the first two outputs of an 8x8 net, to mark the first and second
halfs of this 8 count cycle.  Not doing so well, but interesting watching
it try.

The out 0 was doing well at first creating a wave mostly in sync with
that it should be.  But the output 1, that we we trying to train to be
the invert of output 0, was starting to mimic output 0, and as it did the
rewards got worse and worse and then output 0 started to turn to crap.
Still watching as it progresses to see what happens.  This is the first
time the w learning and RL have been working together so I have no clue
how the two of them will interact.

================================================================================

2016-03-31 8:20 AM Friday

Yesterday

1) Set up first test of RL with new w learning to try and train a simple
version of the decoding problem.  Didn't work.  But produced intesting
results.  Left running until this morning.

2) Nothing else really.

Today

The test yesterday produced interesting results.  The rewards would
start heading up, as if the yste was learning, then drift down, then
jump up, and suddenly dive way down.  Clearly there was a constant
complex struggle in play between RL and w learning and the two never
learned to "work together".  In end end, the system seemed to give up,
with a reward value bouncing around zero.

Took a screen shot of the graphs to save the chaos that resulted.

Should try to run the same test with w learning turned off to see how
RL can do on it's own.  Better I suspect!

The test was trying to train only two ouptuts of 8, where one was to
be a square wave with the frequency of the count pattern, and the other
was to be the inverted version of the same square wave.  Meaning, output
pulses on the first otuput, durring the first half of the count sequence,
and on the second half for the second half of the count sequence.

This wasn't really even a decoding problem becuase one of the inputs
(the highest order bit of the count) was the answer to the problem.
If the net could learn to send that pattern out the first path, and
produce an inverse of that patten for the second path, it would have
solved the problem.  And it was producing a semi-valid outout for the
first much of the time, but never got the second right, then in time,
the first got killed and the outputs looked like crap.

Totally failed to produce a good answer.

So RL alone, could produce half the answer just by routing that one input
to the one output!  Producing the inverted version of the signal is much
harder -- and something we can look at in general instead of thinking
of this as a decoding problem.  Being able to take complex signals as
inputs and mix and match as required to produce different toutputs is
really the issue that has to be solved now.

So we can test RL alone, just to see what it does for this problem
as a baseline.  If turning on w leanring makes it worse, clearly
we are not on the right path!

---

Making it work better. In the past, my thought was that the way this sort
of network would work, would be to create balanced behavior that would
act as a percpeiton network when there was no corelation with rewards,
and once the signals produced by the correlation network started to show
correlation with rewards, it would turn into a signal routing problem.
So w learning would create fundamental signals by which to build with,
and RL would mix and match them into the correct results.  But the
fundamental signals would be a balanced.

However, with this stronger version of w elarning, I understand to a
better degree what's happening. Though they may be somewhat "blanced"
in the long run, they aren't so well balanced at any one point in time.
They are complex unbalanced signals. And RL seems to be destryong them
in this one long test I've done -- turning them from intresting signals
to flat lines, and other crap as RL domiantes over w learning.

So, I'm thinking I need to explore the idea of making w learning domainte
the process in the early stages of the network, to allow it to form
the needed signals to work with, and then make RL stronger in the later
stages to do the routing and combbing work.

So either a grandulat transfer of power from percpetion to rl, as we
get deeper in the network, or maybe just 100% percpeiton for some early
levels, and 100% RL for the last with some mix of the two in the middle?

Maybe just 0% RL for the first half, and both percpeiotn and RL together
in the second half?  (RL seems to have the power to overide the others?)

So that's what needs to be tested today.

----

[1:22 PM]

Worked no better than before.  Rewards swing up to about .8 level and
then down jump around -.6 randomly and setle in ar arount 0.  All a
very complex pattern of jumping around with no end sign of progress.
End result is a systme that basicalky just urns off the output of the two
nodes I'm trying to train and sends most data to the other outputs. Sort
of a "I give up" type of answer. :)

The net that used percpetion in the first half and RL in the second
did only slightly better.  It swung around in a very similar pattern,
rising higher to around 1.7 and dropping to about -.4 but is currently
stuck below zero in a slowly rising dirction -- but like the other tests,
to current solution has both outputs mostly just turned off.

I need to test a deeper net to make sure perception has enough layers
to form a valid set of signals for RL to work with.

And I need to experiment a little with what I'm asking the nework
to learn.  My postive rewards for sorting to the right output and
negative for the wrong might be too much for the network to cope with.
The negative for mistakes might be dirving it to give up and do nothing.
I should try only postive for correct sorts and see how that chagnes
things.

----

If I assume this new percpetion is producing good useable signals,
I need to go back and verify what my RL system is able to do.

Mostly, what I have been testing with RL is simple sort input X to
output Y.

I need to verify it's power to learn to OR signals together to create
outputs.

And I need to verify the system's ability to set balance to set points
vs all off and all on.  I fear, due to how I've been testing, that I've
created a system that can't really do that!

But maybe, it's not so imporant anyhow!  Maybe, with the right percpeiotn
system creating lots of good small signals to work with, that RL will
only need the power to learn to OR little signals together?

But, in working with perception, I had this insight that maybe I should be
able to train b values directly with RL measuring the difference between
paths to push flow one way or another, such that when the alternatives
are equal, the balance stays in the same place (well it will drift a
little up and down -- but the idea is that as it drifts it will learn
that both directions make things worse).

Now that I've speent a few weeks working on percpeiton, I can't even
fucking remember how RL works!

Ok, but before digging back into RL and thinking about how once again it
might beed to be done all differently (shit I'm getting tried of flopping
back and forth between RL and percpetion and constantly reoding each),
I need to do these few simple tests on the current test.
d9 1
a9 44
2016-03-31 3:15 AM Tuesday

Days Ago...

1) Changed rewards a bit to only reward correct sorts to the first or second
outputs.  That seemed to help a little.

2) tried larger net with only rewards in second half and w and f learning
in first half.  Maybe, it did a little better.

But all for this test had rewards drifing from like 1 to 4 up and down
randomly for 3 days.

Today

Well, taking Julie to the train with Pig soon. wasting tiem on Faceobook.
Don't have much time to write here.

My work on this has come to a grinding halt it seems.  My enthusism is
gone at the moment.  Not sure why.  But I think it's me pancing over
the fact that we are appacohikng Spring Fling, and I have to do the lunch
on Sunday. And Taxes are due.  And other stuff in life keeps trying to
force me to do things I don't want to work on. But the AI work is not
easy to work on when I'm anxous about not doing the other work, so I turn
instead ot tasks that are better for procrastination.
RL doesn't seem to be able to solve it. It's complex so I'm not sure
what the issue is. I can't tell if the output of the perception system
in the middle layers is not stable enough, and drifts. Or if RL
just can't pick out and mix together the right signals to solve the RL
problem?

As I wrote above, I really need to dig into this a little to see what
is happening, or just goback to looking at RL, and see what it's able
to do beyond rounting pulse x to output y. Can it mix 2 of 4 signals
to create a solution?  Can it learn in ths system to create the
invert of a signal that is an input?  What can it learn and not learn
and is that what is needed?

But, opening this box and digging it ont seems hard, with this other stuff
hanging over my yhead. I think I have to clear up the rest of my life
and them come back to this.

================================================================================
================================================================================
d223 171
@
